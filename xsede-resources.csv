,access_description,current_statuses,info_resourceid,info_siteid,latest_status,latest_status_begin,latest_status_end,other_attributes,parent_resource,project_affiliation,provider_level,rdr_resource_id,rdr_type,recommended_use,resource_description,resource_descriptive_name,resource_status,updated_at
0,,decommissioned,nstg.ornl.teragrid.org,ornl.teragrid.org,decommissioned,2011-07-31,,"{'organizations': [{'organization_abbreviation': 'ORNL', 'organization_name': 'Oak Ridge National Laboratory', 'organization_code': '4006060'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,8,resource,,"The ORNL NSTG cluster has 28 nodes, 16 of which are dedicated to running compute jobs. Each compute node has two 3.06 GHz Intel Pentium4 Xeon CPUs, 2.5 GB memory, and 26 GB of local scratch. 800 GB of shared scratch is provided across the private gigabit interconnect. Four additional nodes are dedicated to running GridFTP servers, and each is configured with 4 GB of memory.",ORNL Intel Xeon Cluster (NSTG),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
1,,decommissioned,tungsten.iu.teragrid.org,iu.teragrid.org,decommissioned,2008-01-01,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,15,resource,,"The IU AVIDD IA-32 cluster has 192 compute nodes, each with two 2.4 GHz Intel Pentium4 Xeon CPUs, 2.5 GB memory, 10.4 GB of local scratch, a Myrinet-2000 interconnect and access to the 1.6TB GPFS scratch space. NOTE: 20% of this resource was available for TeraGrid usage.

<strong class=      ""attention      "">IMPORTANT: </strong>There will be no more future allocations on this resource.",NCSA Xeon Linux Supercluster (Tungsten),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
2,,decommissioned,ranger.tacc.teragrid.org,tacc.teragrid.org,decommissioned,2013-02-04,,"{'latitude': None, 'node_count': 3936.0, 'manufacturer': 'Sun', 'rmax': None, 'interconnect': 'InfiniBand', 'is_publicly_available': True, 'peak_teraflops': 579.3, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530113, 'conversion_factors': [{'begin_date': '2008-06-16', 'factor': 1.533, 'conversion': 'Teragrid', 'end_date': None}, {'begin_date': '2008-01-01', 'factor': 1.256, 'conversion': 'Teragrid', 'end_date': '2008-06-15'}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 200000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-Ranger', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'TACC Sun Constellation Cluster (Ranger)', 'allocations_description': None, 'allocable_id': 126270}, 'job_manager': 'Default/Interactive: jobmanager-fork  or  Batch: jobmanager-sge', 'batch_system': 'SGE', 'platform_name': 'Constellation System', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '1.73 PB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'tg-support@tacc.utexas.edu', 'login_hostname': 'tg-login.ranger.tacc.teragrid.org', 'storage_network': 'InfiniBand', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Opteron, four-socket, quad-core', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Ranger', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'Lustre Parallel File System', 'user_guide_url': 'https://www.xsede.org/web/guest/tacc-ranger', 'disk_size_tb': 1730.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.3, 'cpu_count_per_node': 16, 'operating_system': 'Linux (CentOS)', 'community_software_area': True}",50.0,XSEDE,XSEDE Level 1,126270,compute,"Ranger is intended for users with codes scalable to thousands of cores (1024 and above).  A batch queue is available to assist users develop, test, and scale codes up to 1024 compute cores.  Four separate login nodes will provide interactive connectivity to the system for compiling and interfacing with the batch queuing system.<br />
","The Ranger Sun Constellation Cluster is configured with 3,936 four-socket, quad-core AMD Opteron nodes (62,976 compute cores) and 125 TB of distributed memory.  With each core clocked at 2.3 GHz and capable of four flops/clock cycle, Ranger provides the user community access to a resource with a theoretical peak performance of 579.3 TFLOPS.  Multiple shared file systems (HOME, WORK, and PROJECTS) are configured from 1.7 PB of raw storage.  All file systems are managed via the Lustre Parallel File System.  Nodes are interconnected with InfiniBand technology with two non-blocking Sun Magnum switches acting as the core of the fabric.",TACC Sun Constellation Cluster (Ranger),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-02-04', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2013-02-03', 'friendly_end_date': None, 'production_begin_date': '2008-02-04', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
3,,production,Abe-QB-Grid.teragrid.org,teragrid.org,production,2008-10-10,,"{'is_accounted': False, 'is_authenticated': True, 'user_guide_url': '', 'is_authorized': True, 'is_publicly_available': False, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'GridResource', 'allocable_resource_id': 530130, 'conversion_factors': [], 'allocation_type_specifics': [], 'display_resource_name': 'NCSA/LSU Dell PowerEdge Linux Clusters (Abe/QueenBee)', 'allocations_description': None, 'allocable_id': 458}, 'resources_in_grid': []}",104.0,XSEDE,XSEDE Level 1,458,grid,,NCSA/LONI Dell PowerEdge Linux Clusters (Abe/Queenbee),NCSA/LONI Dell PowerEdge Linux Clusters (Abe/Queenbee),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2008-10-10', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
4,,production,jetstream-storage.tacc.xsede.org,tacc.xsede.org,production,2016-01-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': False, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'GigaBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530192, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2015-11-01', 'required_resource_display_name': 'IU/TACC (Jetstream)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530188, 'allocable_id': 144545}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-09-01', 'required_resource_display_name': 'IU/TACC (Jetstream)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530188, 'allocable_id': 144545}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-11-01', 'required_resource_display_name': 'IU/TACC (Jetstream)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530188, 'allocable_id': 144545}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-11-01', 'required_resource_display_name': 'IU/TACC (Jetstream)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530188, 'allocable_id': 144545}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}], 'display_resource_name': 'IU/TACC Storage (Jetstream Storage)', 'allocations_description': None, 'allocable_id': 144549}, 'is_accounted': True, 'user_guide_url': '', 'databases': '', 'supports_sensitive_data': None, 'file_space_tb': 960.0, 'xsedenet_participant': None}",94.0,XSEDE,XSEDE Level 1,144549,storage,Storage for Jetstream Computing,,IU/TACC Storage (Jetstream Storage),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2016-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-06-06 18:53:16.866
5,,decommissioned,dtf.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2010-03-31,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,43,resource,,"NCSA       's IA-64 TeraGrid Linux Cluster consists of 887 IBM nodes: 256 nodes with dual 1.3 GHz Intel Itanium 2 processors (half with 4 GB of memory per node, and the other half with 12 GB of memory per node), and 631 nodes with dual 1.5 GHz Intel Itanium 2 processors (4 GB of memory per node). The cluster is running SuSE Linux and is using Myricom       's Myrinet cluster interconnect network, and the GPFS parallel filesystem.",NCSA DTF,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-02-28', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
6,,pre-production,stampede2.tacc.xsede.org,tacc.xsede.org,pre-production,,,"{'latitude': None, 'node_count': 4204.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'Intel Omni-Path Architecture, Fat-tree topology', 'is_publicly_available': True, 'peak_teraflops': 12.8, 'advance_max_reservable_su': None, 'is_accounted': True, 'allocations_info': {'unit_type': 'Node Hours', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530199, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2017-03-14', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'Maximum request(SU) limit of 10M Node Hours', 'allocation_state': 'New and Renewal Only', 'maximum_amount': 10000000.0, 'allocation_type': 'Research', 'dollar_value': None}], 'display_resource_name': 'TACC Dell/Intel Knights Landing System (Stampede2 Phase 1)', 'allocations_description': 'Stampede 2 is allocated in service units (SU)s. An SU is defined as 1 wall-clock node hour.', 'allocable_id': 144584}, 'job_manager': '', 'batch_system': 'SLURM', 'platform_name': ""TACC Dell/Intel Knight's Landing System (Stampede2 - Phase 1)"", 'nfs_network': '', 'sensitive_data_support_description': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '28000000', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': 'xsede-support@tacc.utexas.edu', 'login_hostname': 'stampede2.tacc.xsede.org', 'storage_network': 'Intel Omni-Path Architecture', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 1.4100000000000001, 'cpu_type': 'Intel Xeon Phi Knights Landing', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Stampede2', 'supports_sensitive_data': False, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': 'https://portal.tacc.utexas.edu/user-guides/stampede#knl', 'disk_size_tb': 28000.0, 'local_storage_per_node_gb': 200.0, 'cpu_speed_ghz': 1.4, 'cpu_count_per_node': 68, 'operating_system': 'Linux Centos 7', 'community_software_area': True}",126.0,XSEDE,XSEDE Level 1,144584,compute,"Stampede2 is intended primarily for parallel applications scalable to tens of thousands of cores, as well as general purpose and throughput computing. Normal batch queues will enable users to run simulations up to 48 hours. Jobs requiring run times and more cores than allowed by the normal queues will be run in a special queue after approval of TACC staff. normal, serial and development queues are configured as well as special purpose queues.","The new Stampede2 Dell Knights Landing (KNL) Cluster is configured with 4200 Dell KNL compute nodes, each with a new stand alone Intel Xeon Phi Knights Landing bootable processor.  Each KNL node will include 68 cores, 16GB MCDRAM, 96GB DDR-4 memory and a 200GB SSD drive per node.  Stampede2 will deliver an estimated 13PF of peak performance. Compute nodes have access to dedicated Lustre Parallel file systems totaling 28PB raw, provided by Seagate. An Intel Omni-Path Architecture switch fabric connects the nodes and storage through a fat-tree topology with a point to point bandwidth of 100 Gb/s (unidirectional speed). 16 additional login and management servers complete the system.  Later in 2017, Stampede2 Phase 2 consisting of next generation Xeon servers and additional management nodes will be deployed.",TACC Dell/Intel Knight's Landing System (Stampede2 - Phase 1),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2021-09-30', 'friendly_end_date': None, 'production_begin_date': '2017-07-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2016-05-09', 'pre_production_end_date': '2017-06-30'}",2017-03-13 14:33:33.854
7,,decommissioned,teradre.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2010-07-01,,"{'latitude': None, 'node_count': 1750.0, 'manufacturer': '', 'rmax': None, 'interconnect': 'Gigabit Ethernet', 'is_publicly_available': True, 'peak_teraflops': 60.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530124, 'conversion_factors': [{'begin_date': '2008-01-01', 'factor': 0.849, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 200000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}], 'display_resource_name': 'Purdue Distributed Rendering Environment (TeraDRE)', 'allocations_description': None, 'allocable_id': 126168}, 'job_manager': 'jobmanager-condor', 'batch_system': 'Condor', 'platform_name': 'TeraDRE', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': 'TBA', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'teradre.purdue.teragrid.org', 'storage_network': 'TBA', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'TBA', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'TeraDRE', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': 'TBA', 'parallel_file_system': 'TBA', 'user_guide_url': 'http://teradre.rcac.purdue.edu/tutorial.html', 'disk_size_tb': 170.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.3, 'cpu_count_per_node': 8, 'operating_system': 'Linux Debian Etch, RHEL4', 'community_software_area': False}",13.0,XSEDE,XSEDE Level 1,126168,compute,"The TeraDRE allows TeraGrid users to render graphics with a number of rendering packages: Maya, POV-ray, and Blender, among others.","The Purdue TeraDRE is a high-throughput visualization resource built on the Purdue Condor Pools. A 48-node subcluster featuring Nvidia GeForce 6600 GT GPUs are available for GPU-accelerated programs and hardware-accelerated rendering using 
Gelato.",Purdue Distributed Rendering Environment (TeraDRE),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-06-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
8,,production,gordon.sdsc.xsede.org,sdsc.xsede.org,production,2012-02-01,2017-03-31,"{'latitude': None, 'node_count': 1024.0, 'manufacturer': 'Appro', 'rmax': None, 'interconnect': 'Dual-rail 3-D torus, Quad Data Rate (QDR) InfiniBand technology with a 40-Gb/s point-to-point bandwidth. ', 'is_publicly_available': True, 'peak_teraflops': 341.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530155, 'conversion_factors': [{'begin_date': '2012-02-01', 'factor': 4.932, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'SDSC Medium-term disk storage (Data Oasis)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530165, 'allocable_id': 142302}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': 50000.0, 'allocation_type': 'Startup', 'dollar_value': 0.06672760500000001}, {'relative_order': None, 'required_resources': [{'begin_date': '2016-09-01', 'required_resource_display_name': 'SDSC Medium-term disk storage (Data Oasis)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530165, 'allocable_id': 142302}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.06672760500000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 0.06672760500000001}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'SDSC Medium-term disk storage (Data Oasis)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530165, 'allocable_id': 142302}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.06672760500000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.06672760500000001}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'SDSC Medium-term disk storage (Data Oasis)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530165, 'allocable_id': 142302}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.06672760500000001}], 'display_resource_name': 'SDSC Appro with Intel Sandy Bridge Cluster (Gordon Compute Cluster)', 'allocations_description': None, 'allocable_id': 142281}, 'job_manager': '', 'batch_system': 'Torque', 'platform_name': '', 'nfs_network': '1 GigE', 'sensitive_data_support_description': None, 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': ""SDSC's Lustre Data Oasis, /oasis/scratch: 1.6 PB, /oasis/projects/nsf: 1.36 PB;4 NFS servers, ~70 TB"", 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'gordon.sdsc.edu', 'storage_network': '64 QDR InfiniBand to I/O nodes with 128 10 GigE', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 4.0, 'cpu_type': '8-core, 2.6-GHz Intel Sandy Bridge processors', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Gordon Compute Cluster', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://portal.xsede.org/sdsc-gordon', 'disk_size_tb': 1628.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.6, 'cpu_count_per_node': 16, 'operating_system': 'Linux/Rocks', 'community_software_area': True}",61.0,XSEDE,XSEDE Level 1,142281,compute,"The Gordon Compute Cluster is designed for data-intensive applications spanning domains such as genomics, graph problems, geophysics, and data mining. The large-memory supernodes are ideal for users with serial or threaded applications that require significantly more memory than is available on a single node of most systems, while the flash-based I/O nodes may offer significant performance improvement for applications that exhibit random access data patterns or require fast access to significant amounts of scratch space.

Allocations of the Gordon compute cluster provide access to the flash memory roughly in proportion to the number of compute nodes requested by a given user job. However, users may also apply separately for long-term dedicated use of the I/O nodes. Please see the Gordon ION resource entry for a more thorough description of this unique resource and how to apply for dedicated access.

Users interested in the large memory nodes should request SUs in proportion to the amount of memory required. For example, 1TB corresponds to the memory on 16 compute nodes (64 GB per node) and the request should account for the resulting number of SUs (with the exact processor counts to be published prior to the proposal deadline for computing SUs).

Allocation requests must describe in detail how you will make use of the distinctive features of Gordon. If you are new to high-performance computing and do not yet have benchmarking data to support the use of Gordon, we encourage you to apply for a startup allocation.

If you require special help with using the Gordon Compute Cluster, we encourage you to request XSEDE Extended Collaborative Support Services in conjunction with your allocation request.","The Gordon Compute Cluster is an Appro-integrated, data-intensive computing resource. It is composed of 1,024 dual-socket compute nodes and 64 I/O nodes connected by a dual-rail, QDR InfiniBand, 3D torus network. Each compute node has two eight-core Intel EM64T Xeon E5 2.6 GHz (Sandy Bridge) processors and 64 GB of DRAM. The Sandy Bridge processor uses Intel’s Advanced Vector Extensions (AVX) to achieve 8 floating-point operations/clock cycle. The full 1,024 node cluster has a peak speed of 341 Tflop/s. Gordon has three distinct features that make it ideal for data intensive computing: 1) 300 TB of high performance Intel flash memory served via 64 I/O nodes, each of which is capable of over 560k IOPS, or 35M IOPS for the full system; 2) large-memory super-nodes that provide up to 1 TB of cache-coherent memory via ScaleMP’s vSMP Foundation software; and 3) 3 PB of disk in two Lustre-based file systems, the larger one of which is capable of sustained rates of 50 GB/s.",SDSC Appro with Intel Sandy Bridge Cluster (Gordon Compute Cluster),"{'coming_soon_begin_date': '2011-11-01', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2012-01-31', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2017-03-31', 'friendly_end_date': None, 'production_begin_date': '2012-02-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-02-15 01:43:48.352
9,,decommissioned,dash.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2012-02-01,,"{'latitude': None, 'node_count': 64.0, 'manufacturer': 'Appro', 'rmax': None, 'interconnect': 'DDR InfiniBand', 'is_publicly_available': True, 'peak_teraflops': 4.9, 'advance_max_reservable_su': 128.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530145, 'conversion_factors': [{'begin_date': None, 'factor': 1.873, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'SDSC Appro Flash Disk, vSMP Cluster (Dash)', 'allocations_description': None, 'allocable_id': 142265}, 'job_manager': 'Moab', 'batch_system': 'Torque', 'platform_name': '', 'nfs_network': '1 Gb/s Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': ""GPFS-WAN, IU's Lustre-Wan DC-WAN, TG's Distribued Lustre-Wan Albedo, SDSC's Lustre Data Oasis, Dash-NFS"", 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'mahidhar@sdsc.edu', 'login_hostname': 'dash-login.sdsc.edu', 'storage_network': '1 Gb/s Login, 10 Gb/s shared for Batch', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 6.0, 'cpu_type': 'Intel Nehalem quad-core processors', 'alternate_login_hostname': 'dash.sdsc.teragrid.org, dash-login.sdsc.teragrid.org, dash.sdsc.edu, dash-login2.sdsc.edu, dash-login2.sdsc.teragrid.org', 'xsedenet_participant': None, 'max_reservable_su': 128.0, 'nickname': 'Dash', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '192.67.81.8', 'parallel_file_system': 'GPFS, Lustre', 'user_guide_url': 'https://www.xsede.org/web/guest/sdsc-dash', 'disk_size_tb': 75.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.4, 'cpu_count_per_node': 8, 'operating_system': 'Linux (CentOS)', 'community_software_area': False}",27.0,XSEDE,XSEDE Level 1,142265,compute,"Like its Gordon follow-on, Dash was designed for data-intensive computing. Allocations will preferentially be made to users whose applications can benefit from Dash 's large memory per node as well as its two novel features -- flash memory and vSMP software. Such users are encouraged to request startup allocations to implement and tune their applications in advance of the Gordon deployment. Requests should describe how the applications might benefit from running on Dash and what programming changes might be needed. ","Dash is a cluster from Appro with 64 compute nodes connected by DDR InfiniBand. Each node has two Intel Nehalem quad-core processors running at 2.4 GHz and contains 48 GB of DRAM. Thus the entire 512-core cluster has 4.9 Tflop/s of peak performance and 3 TB of DRAM.

Dash is a prototype of Gordon, a much larger cluster targeted at data-intensive computing that will be installed in 2011. Hence Dash has two novel features, which will be scaled up in Gordon.

1. A large amount of flash memory contained in 64 Intel solid state drives (SSDs). Each 64-GB SSD will initially be attached to a separate compute node.

2. vSMP, ScaleMP 's virtual shared memory that currently allows aggregation of DRAM and flash memory across up to 16 nodes.

Dash hosts several file systems. NFS is provided for home and work space, GPFS-WAN is available for parallel I/O, and a scratch file system supports the SSDs.

The initial deployment is two 16-node partitions, one with vSMP and one without to allow for comparative benchmarking. Each partition has 768 GB of DRAM and 1 TB of flash memory. The remaining 32 compute nodes will be made available in late 2010 following acceptance testing of the 32-node vSMP functionality. ","SDSC Appro Flash Disk, vSMP Cluster (Dash)","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2012-02-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2012-01-31', 'friendly_end_date': None, 'production_begin_date': '2010-04-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
10,,decommissioned,lemieux.psc.teragrid.org,psc.teragrid.org,decommissioned,2007-01-01,,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,4,resource,,"Lemieux comprises 610 Compaq Alphaserver ES45 nodes and two separate front end nodes. Each computational node is a 4 processor SMP, with 1-GHz Alpha EV68 processors and 4 Gbytes of memory.  They run the Tru64 Unix operating system. A dual-rail Quadrics interconnect links  the nodes.

IMPORTANT: No further allocations to this resource.",PSC TCS1,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2007-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
11,,decommissioned,kraken.nics.teragrid.org,nics.teragrid.org,decommissioned,2014-05-01,,"{'organizations': [{'organization_abbreviation': 'NICS', 'organization_name': 'National Institute for Computational Sciences', 'organization_code': 'T103349'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,29,resource,,"The Kraken system is a Cray XT5 with 9,408
compute nodes interconnected with SeaStar, a 3D torus. Each compute node has two hex-core AMD Opterons for a total of 112,896 cores. All nodes have 16 Gbytes of memory: 4/3 Gbyte of memory per core.",NICS Cray XT5 (Kraken),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2014-05-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2014-04-30', 'friendly_end_date': None, 'production_begin_date': '2009-10-05', 'decommissioned_end_date': None, 'pre_production_begin_date': '2009-01-01', 'pre_production_end_date': '2009-10-04'}",
12,,decommissioned,dtf.anl.teragrid.org,anl.teragrid.org,decommissioned,2009-07-01,,"{'organizations': [{'organization_abbreviation': 'ANL', 'organization_name': 'Argonne National Laboratory', 'organization_code': '9000126'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,32,resource,,"The IA-64 TeraGrid Linux Cluster at UC/ANL consists of 62 nodes with dual Intel Itanium 2 processors, with 4 GB of memory per node. The cluster is running Red Hat Enterprise Linux and is using the Myricom Myrinet cluster interconnect network. There is a 16 TB local high-performance GPFS, and access to the TeraGrid-wide GPFS-WAN file-system.",UChicago/Argonne compute cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-07-01', 'friendly_begin_date': None, 'post_production_end_date': '2009-06-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2009-06-01', 'retired_end_date': None, 'production_end_date': '2009-05-31', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
13,,,schooner.ou.xsede.org,ou.xsede.org,,,,"{'organizations': [{'organization_abbreviation': 'U Oklahoma', 'organization_name': 'University of Oklahoma', 'organization_code': '0088070'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 3,121,resource,,University of Oklahoma HPC cluster schooner.oscer.ou.edu.,Schooner,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-10-30 17:46:12.328
14,,decommissioned,lear.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2008-01-01,,"{'organizations': [{'organization_abbreviation': 'Purdue U', 'organization_name': 'Purdue University', 'organization_code': '0018259'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,11,resource,,"The Lear Cluster consists of 512 dual-CPU Dell PowerEdge 1425 compute nodes, running Red Hat Enterprise Linux, version 4. Each node has two 64-bit EM64T 3.2 GHz Xeon CPUs and 4 GB of RAM. The cluster is interconnected with Gigabit Ethernet, and offers 8.8 TB of NFS scratch storage available to all Lear users plus a 4.8 TB NFS scratch area dedicated to TeraGrid users. Lear users may also access a 1.3 PB DXUL archive system. Lear   's peak performance is rated at 6.6 TFLOPS.",Purdue EM64T Linux Cluster (Lear),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
15,,decommissioned,bigben.psc.teragrid.org,psc.teragrid.org,decommissioned,2010-03-31,,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,16,resource,,"BigBen is a Cray XT3 MPP system with 2068 2.6-GHz dual-core AMD Opteron compute nodes linked by a custom-designed interconnect. Twenty-two dedicated I/O processors are also connected to this network.  Each compute node has 2 Gbytes of memory shared by its two cores, and runs the Catamount operating system. The front end processors run SuSE Linux.",PSC XT3 (Big Ben),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-03-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
16,,decommissioned,copper.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2007-09-30,,"{'latitude': None, 'node_count': 12.0, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': None, 'is_publicly_available': True, 'peak_teraflops': 2.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530101, 'conversion_factors': [{'begin_date': '2006-02-01', 'factor': 0.447, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-NCSA', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}], 'display_resource_name': 'NCSA IBM p690', 'allocations_description': None, 'allocable_id': 125678}, 'job_manager': 'jobmanager (default)\n\njobmanager-ll', 'batch_system': None, 'platform_name': 'pSeries 690', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': None, 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': None, 'community_software_area_email': None, 'login_hostname': 'login-cu.ncsa.teragrid.org', 'storage_network': None, 'graphics_card': None, 'is_visualization': False, 'memory_per_cpu_gb': 8.0, 'cpu_type': 'IBM Power4', 'alternate_login_hostname': None, 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Copper', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': None, 'parallel_file_system': 'GPFS: 20 TB', 'user_guide_url': 'http://www.ncsa.uiuc.edu/UserInfo/Resources/Hardware/IBMp690/', 'disk_size_tb': 30.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.3, 'cpu_count_per_node': 32, 'operating_system': 'AIX 5.3', 'community_software_area': False}",7.0,XSEDE,XSEDE Level 1,125678,compute,"[DECOMM]<br />
<strong>Status</strong>
This resource was retired from service on September 30, 2007.","The pSeries 690 is a symmetric multiprocessor (SMP) system based on the POWER4 processor. The NCSA system consists of 12 32-processor hosts running AIX, eight of which have 64 GB of memory, and the other four have 256 GB of memory. It uses the GPFS parallel file system from IBM.",NCSA IBM p690 (Copper),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2007-09-30', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
17,,decommissioned,brutus.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2010-03-31,,"{'organizations': [{'organization_abbreviation': 'Purdue U', 'organization_name': 'Purdue University', 'organization_code': '0018259'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,14,resource,,"These resources consist of an SGI 450 (brutus.rcac.purdue.edu) with two RC100 FPGA blades, totaling 4 available FPGAs. Also available is a Sun Fire X2200 M2 (portia.rcac.purdue.edu) which serves both as a place & route node for preparing FPGA code for use on Brutus and as an entry point for GSI-SSH and job submission to Brutus by TeraGrid users. <br> 

<b>NOTE:</b> All references to CPU below should be interpreted as referring to be FPGAs.",Purdue FPGA Prototyping Environment (Brutus),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-02-28', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
18,,decommissioned,frost.ncar.teragrid.org,ncar.teragrid.org,decommissioned,2011-07-31,,"{'latitude': None, 'node_count': 4096.0, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': '', 'is_publicly_available': True, 'peak_teraflops': 22.936, 'advance_max_reservable_su': 2048.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530115, 'conversion_factors': [{'begin_date': '2007-07-23', 'factor': 0.558, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [], 'display_resource_name': 'NCAR IBM Blue Gene (Frost)', 'allocations_description': None, 'allocable_id': 126030}, 'job_manager': 'jobmanager-cobalt', 'batch_system': 'Cobalt (http://trac.mcs.anl.gov/projects/cobalt/wiki)', 'platform_name': 'Blue Gene/L', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'MPP', 'gpu_description': None, 'community_software_area_email': 'tg-support@ucar.edu', 'login_hostname': 'tg-login.frost.ncar.teragrid.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 0.256, 'cpu_type': 'PowerPC-440', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 2048.0, 'nickname': 'Frost', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '6 TB GPFS (/ptmp)\r\n534 TB GPFS-WAN (/mnt/gpfs-wan, currently only available for data transfer and storage, not accessible from the Blue Gene/L rack)', 'user_guide_url': 'http://www2.cisl.ucar.edu/docs/frost/frost-user-guide', 'disk_size_tb': 110.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 0.7000000000000001, 'cpu_count_per_node': 2, 'operating_system': 'SuSE Linux Enterprise Server 9', 'community_software_area': True}",31.0,XSEDE,XSEDE Level 1,126030,compute,"The Frost Blue Gene/L system is a highly scalable platform for developing, testing and running parallel MPI applications up to 8192 processors, and providing efficient computing for smaller job sizes.
<br />
Frost became available for Startup/Education allocations in July, 2007. Requests for Research allocations were first accepted on Sept. 17, 2007, for projects beginning Jan. 1, 2008.","Frost is a four-rack, Blue Gene/L system with 4096 compute nodes. There is one I/O node for every 32 compute nodes (pset size of 32) for a total of 32 I/O nodes per rack. Each compute node and I/O node is a dual-core chip, containing two 700MHz PowerPC-440 CPUs, 512MB of memory, and two floating-point units (FPUs) per core. Thus frost has a total of 2048 processors capable of sustaining a peak performance of 5.734 trillion floating-point operations per second (TFLOPs). By default, the compute nodes run in coprocessor mode (one processor handles computation and the other handles communication), but virtual node mode is also available, where both processors share the computation and communication load.",NCAR IBM Blue Gene (Frost),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
19,,decommissioned,test1.xes.xsede.org,xes.xsede.org,decommissioned,2017-03-01,,"{'organizations': [{'organization_abbreviation': 'TG', 'organization_name': 'TeraGrid', 'organization_code': '00000TG'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,130,resource,,Resource Used for Testing XSEDE Capabilities,XSEDE Test Resource,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2017-03-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-22 21:10:58.782
20,,decommissioned,tape.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2013-09-01,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,76,resource,,,NCSA Tape Storage,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-09-01', 'friendly_begin_date': None, 'post_production_end_date': '2013-08-31', 'coming_soon_end_date': None, 'post_production_begin_date': '2012-09-30', 'retired_end_date': None, 'production_end_date': '2012-09-29', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
21,,decommissioned,condor.iu.teragrid.org,iu.teragrid.org,decommissioned,2010-03-30,,"{'latitude': None, 'node_count': None, 'manufacturer': '', 'rmax': None, 'interconnect': '', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': False, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530127, 'conversion_factors': [{'begin_date': None, 'factor': 0.0, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}], 'display_resource_name': 'Indiana University Windows Condor Pool', 'allocations_description': None, 'allocable_id': 142308}, 'job_manager': '', 'batch_system': '', 'platform_name': '', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': None, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Condor IU', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '', 'user_guide_url': '', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': None, 'cpu_count_per_node': None, 'operating_system': '', 'community_software_area': False}",52.0,XSEDE,XSEDE Level 1,142308,compute,,Condor IU,Indiana University RenderPortal/Windows Condor Pool,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-30', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
22,,decommissioned,dtf.caltech.teragrid.org,caltech.teragrid.org,decommissioned,1005-06-17,,"{'latitude': None, 'node_count': None, 'manufacturer': '', 'rmax': None, 'interconnect': '', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': False, 'sensitive_data_support_description': None, 'job_manager': '', 'batch_system': '', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': None, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'dtf.caltech', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '', 'user_guide_url': '', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': None, 'cpu_count_per_node': None, 'operating_system': '', 'community_software_area': False}",53.0,XSEDE,XSEDE Level 1,142310,compute,,DTF Cal Tech,Cal Tech DTF,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '1005-06-17', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
23,,pre-production,bridges.gpu.psc.xsede.org,gpu.psc.xsede.org,pre-production,,,"{'latitude': None, 'node_count': 48.0, 'manufacturer': 'HP/NVIDIA', 'rmax': None, 'interconnect': '', 'is_publicly_available': True, 'peak_teraflops': 894.6, 'advance_max_reservable_su': None, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530200, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2015-09-01', 'required_resource_display_name': 'PSC Storage (Bridges Pylon)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530191, 'allocable_id': 144548}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'New and Renewal Only', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}], 'display_resource_name': 'PSC GPU (Bridges GPU)', 'allocations_description': 'K80 nodes: 1 GPU-hour = 1 SU\r\nP100 nodes: 1 GPU-hour = 2.5 SUs', 'allocable_id': 144585}, 'job_manager': 'SLURM', 'batch_system': 'SLURM', 'platform_name': 'HPE Apollo 2000s', 'nfs_network': '', 'sensitive_data_support_description': '', 'advance_reservation_support': False, 'model': 'HPE Apollo 2000s', 'primary_storage_shared_gb': 'Pylon', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': 'Bridges contains two kinds of GPU nodes: NVIDIA Tesla K80s and NVIDIA Tesla P100s.  Because of the difference in the performance of the nodes, the charges will be different for the two types of nodes.\r\nK80 nodes \r\n\r\nThe K80 nodes hold 4 GPU units each, each of which can be allocated separately.  Service units (SUs) are defined in terms of GPU-hours:\r\n\r\n1 GPU-hour = 1 SU\r\n\r\nNote that the use of an entire K80 GPU node for one hour would be charged 4 SUs.\r\nP100 nodes\r\n\r\nThe P100 nodes hold 2 GPU units each, which can be allocated separately.  Service units (SUs) are defined in terms of GPU-hours:\r\n\r\n1 GPU-hour = 2.5 SUs\r\n\r\nNote that the use of an entire P100 node for one hour would be charged 5 SUs.', 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': 'NVIDIA Tesla K80 GPU / NVIDIA Tesla P100 GPU ', 'is_visualization': False, 'memory_per_cpu_gb': 128.0, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Bridges GPU', 'supports_sensitive_data': False, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': 'https://portal.xsede.org/psc-bridges', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': 8.0, 'cpu_speed_ghz': 2.3, 'cpu_count_per_node': 28, 'operating_system': 'CentOS', 'community_software_area': False}",127.0,XSEDE,XSEDE Level 1,144585,compute,,"Bridges GPU - 16 GPU nodes, each with 2 NVIDIA Tesla K80 GPU cards, 2 Intel Xeon CPUs (14 cores each), and 128GB of RAM and 32 GPU nodes, each with 2 NVIDIA Tesla P100 GPU cards, 2 Intel Xeon CPUs (16 cores each), and 128GB of RAM.  GPU nodes are allocated in GPU-hours",PSC Bridges GPU (Bridges GPU),"{'coming_soon_begin_date': '2017-03-13', 'retired_begin_date': None, 'decommissioned_begin_date': '2019-11-30', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2017-03-14', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-11-30', 'friendly_end_date': None, 'production_begin_date': '2017-07-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2017-03-15', 'pre_production_end_date': '2017-06-30'}",2017-03-16 13:30:24.321
24,,decommissioned,rachel.psc.teragrid.org,psc.teragrid.org,decommissioned,2008-01-01,,"{'latitude': None, 'node_count': 2.0, 'manufacturer': 'HP', 'rmax': None, 'interconnect': 'Quadrics', 'is_publicly_available': True, 'peak_teraflops': 0.31, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 520076, 'conversion_factors': [{'begin_date': '2004-03-15', 'factor': 0.463, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'AAB', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'NRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-PSC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}], 'display_resource_name': 'PSC HP Marvel (Rachel)', 'allocations_description': None, 'allocable_id': 125588}, 'job_manager': 'jobmanager-rachel-pbs', 'batch_system': 'PBS', 'platform_name': 'Alpha EV7', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'tg-login.rachel.psc.teragrid.org', 'storage_network': 'SCSI2', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 4.0, 'cpu_type': 'Alpha EV7', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Rachel', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '', 'user_guide_url': 'http://www.psc.edu/machines/marvel/rachel.html', 'disk_size_tb': 6.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.15, 'cpu_count_per_node': 64, 'operating_system': 'Tru64 Unix', 'community_software_area': False}",22.0,XSEDE,XSEDE Level 1,125588,compute,"[DECOMM]<!-- please use this space for Recommended Use and Status -->

Rachel is primarily intended to run applications with low to moderate parallelism that require large memory bandwidth and/or large shared memory. The queuing policies favor jobs that request more shared memory.
<br />
<strong>Status</strong>
<ul>
  <li>In production but no longer accepting allocation requests</li>
  <li>Rachel will no longer be available after 7/1/2008</li>
</ul>",Rachel is a loosely-coupled pair of SMP machines.  Each  system has 64 1.15 GHz EV7 processors with 256 Gbytes of shared memory.  Logins are to a front end node with 2 EV67  processors.  Both the front end node and  the SMP machines run the Tru64 Unix operating system.,PSC HP Marvel (Rachel),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
25,,decommissioned,project-work-space.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2009-01-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'user_guide_url': None, 'is_accounted': True, 'xsedenet_participant': None, 'databases': '', 'file_space_tb': 0.0, 'supports_sensitive_data': None}",69.0,XSEDE,XSEDE Level 1,139848,storage,,,NCSA Project Work Space Per Host,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
26,,production,wrangler.tacc.xsede.org,tacc.xsede.org,production,2015-02-28,2019-01-30,"{'organizations': [{'organization_abbreviation': 'UT Austin', 'organization_name': 'University of Texas at Austin', 'organization_code': '0036582'}, {'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,67,resource,,"The Wrangler Data Analytics system is designed to satisfy the needs for many in Data Computing who find their I/O patterns are not well suited for classic HPC systems.  Wrangler features 0.5PB of usable flash-based storage accessible directly via the PCI bus to all 96 compute nodes at TACC.  Unlike SSD solutions, this configuration gives all of the compute nodes direct PCI level access to all of the storage providing I/O rates of 1 TB/s and 250 million IOPS.  Wrangler features 10PB of replicated storage for both input and result data hosted at TACC and at Indiana University.  ",TACC Data Analytics System (Wrangler),"{'coming_soon_begin_date': '2014-09-12', 'retired_begin_date': None, 'decommissioned_begin_date': '2019-01-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2014-09-14', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-01-30', 'friendly_end_date': None, 'production_begin_date': '2015-02-28', 'decommissioned_end_date': None, 'pre_production_begin_date': '2014-09-15', 'pre_production_end_date': '2015-01-30'}",2016-04-18 20:36:58.133
27,,decommissioned,database.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2009-01-01,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,77,resource,,,NCSA Database,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
28,,friendly,usdrcg.usd.xsede.org,usd.xsede.org,friendly,2016-10-01,,"{'organizations': [{'organization_abbreviation': 'U South Dakota', 'organization_name': 'University of South Dakota', 'organization_code': 'T101687'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 3,125,resource,,,University of South Dakota Research Computing Group,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': '2016-10-01', 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-01-27 17:22:32.268
29,MSS will be available in read-only mode until September 30 2013. Access is via GridFTP.,decommissioned,tape.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2013-09-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'TeraBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530122, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-NCSA', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}], 'display_resource_name': 'NCSA Tape Storage', 'allocations_description': None, 'allocable_id': 141850}, 'is_accounted': True, 'user_guide_url': None, 'databases': 'N/A', 'supports_sensitive_data': None, 'file_space_tb': 10000.0, 'xsedenet_participant': None}",76.0,XSEDE,XSEDE Level 1,141850,storage,"Archival storage managed by EMC's DiskXtender software, is stored on various sizes of tape and disk depending on its size and length of time on the archive. NCSA continues to keep 2 copies of all data",,NCSA Tape Storage,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-09-01', 'friendly_begin_date': None, 'post_production_end_date': '2013-08-31', 'coming_soon_end_date': None, 'post_production_begin_date': '2012-09-30', 'retired_end_date': None, 'production_end_date': '2012-09-29', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
30,https://wiki.hpc.wvu.edu/hpc_wiki/index.php/Getting_Access,production,spruceknob.wvu.xsede.org,wvu.xsede.org,production,2013-11-11,2021-08-01,"{'latitude': 39.63363, 'node_count': 163.0, 'manufacturer': 'Dell/HPE', 'rmax': None, 'interconnect': 'FDR', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': 0.0, 'is_accounted': False, 'sensitive_data_support_description': None, 'job_manager': 'MOAB', 'batch_system': 'TORQUE', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '400 TB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '17 NVIDIA K20X 1 NVIDIA K40', 'community_software_area_email': '', 'login_hostname': 'spruce.hpc.wvu.edu', 'storage_network': 'GPFS', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 0.0, 'cpu_type': 'Sandy Bridge, Ivy Bridge, Haswell', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Spruce', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': -79.953441, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': 'https://wiki.hpc.wvu.edu/hpc_wiki/index.php/Main_Page', 'disk_size_tb': 1000.0, 'local_storage_per_node_gb': 500.0, 'cpu_speed_ghz': 2.2, 'cpu_count_per_node': 16, 'operating_system': 'RHEL 6.7', 'community_software_area': False}",115.0,XSEDE,XSEDE Level 3,144572,compute,"High Performance Computing
High Throughput Computing
","Heterogeneous HPC compute cluster with over 3,000 cores. System is available to anyone in higher education throughout West Virginia.",WVU Spruce KNOB HPC Cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2021-08-01', 'friendly_end_date': None, 'production_begin_date': '2013-11-11', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-09-13 13:50:39.494
31,,decommissioned,frost.ncar.teragrid.org,ncar.teragrid.org,decommissioned,2011-07-31,,"{'organizations': [{'organization_abbreviation': 'NCAR', 'organization_name': 'National Center for Atmospheric Research', 'organization_code': '4007944'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,31,resource,,"Frost is a four-rack, Blue Gene/L system with 4096 compute nodes. There is one I/O node for every 32 compute nodes (pset size of 32) for a total of 32 I/O nodes per rack. Each compute node and I/O node is a dual-core chip, containing two 700MHz PowerPC-440 CPUs, 512MB of memory, and two floating-point units (FPUs) per core. Thus frost has a total of 2048 processors capable of sustaining a peak performance of 5.734 trillion floating-point operations per second (TFLOPs). By default, the compute nodes run in coprocessor mode (one processor handles computation and the other handles communication), but virtual node mode is also available, where both processors share the computation and communication load.",NCAR IBM Blue Gene (Frost),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
32,,production,jetstream.tacc.xsede.org,tacc.xsede.org,production,2016-01-31,2019-11-30,"{'latitude': None, 'node_count': 640.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'Ethernet', 'is_publicly_available': False, 'peak_teraflops': 516.1, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530188, 'conversion_factors': [{'begin_date': None, 'factor': 6.856, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.1489}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.1489}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.1489}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': 50000.0, 'allocation_type': 'Startup', 'dollar_value': 0.1489}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.1489}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 0.1489}], 'display_resource_name': 'IU/TACC (Jetstream)', 'allocations_description': '1 Service Unit = 1 Virtual CPU Hour', 'allocable_id': 144545}, 'job_manager': '', 'batch_system': '', 'platform_name': 'PowerEdge', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': 516.1, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'jetstream.tacc.xsede.org', 'storage_network': '', 'graphics_card': 'No', 'is_visualization': False, 'memory_per_cpu_gb': 5.33, 'cpu_type': 'Intel E5-2680v3 Haswell', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Jetstream', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Ceph for VM block storage', 'user_guide_url': 'https://portal.xsede.org/jetstream', 'disk_size_tb': 1920.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.5, 'cpu_count_per_node': 24, 'operating_system': 'User defined per VM', 'community_software_area': False}",68.0,XSEDE,XSEDE Level 1,144545,compute,For the researcher needing a handful of cores on demand as well as for software creators and researchers needing to create their own customized virtual machines environments.  Jetstream is accessible ONLY via web interface (https://use.jetstream-cloud.org/) or via API using XSEDE credentials via Globus Auth.,Jetstream is a user-friendly cloud environment designed to give researchers and students access to computing and data analysis resources on demand.,IU/TACC (Jetstream),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-11-30', 'friendly_end_date': None, 'production_begin_date': '2016-01-31', 'decommissioned_end_date': None, 'pre_production_begin_date': '2015-11-01', 'pre_production_end_date': '2016-01-30'}",2016-06-22 15:27:04.326
33,,,xwfs.tacc.xsede.org,tacc.xsede.org,,,,"{'organizations': [{'organization_abbreviation': 'UT Austin', 'organization_name': 'University of Texas at Austin', 'organization_code': '0036582'}, {'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,91,resource,,,XSEDE-Wide File System (XWFS),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2016-06-30', 'friendly_end_date': None, 'production_begin_date': '2013-07-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-08-15 22:57:05.824
34,,decommissioned,hpss.ncar.teragrid.org,ncar.teragrid.org,decommissioned,2011-07-01,,"{'organizations': [{'organization_abbreviation': 'NCAR', 'organization_name': 'National Center for Atmospheric Research', 'organization_code': '4007944'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,72,resource,,,NCAR HPSS Storage System,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-06-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
35,,production,beacon.nics.xsede.org,nics.xsede.org,production,2016-09-01,,"{'organizations': [{'organization_abbreviation': 'NICS', 'organization_name': 'National Institute for Computational Sciences', 'organization_code': 'T103349'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 2,117,resource,,"Beacon is an energy efficient cluster that utilizes Intel® Xeon Phi™ coprocessors. Beacon provides 768 conventional cores and 11,520 accelerator cores that provide over 210 TFLOP/s of combined computational performance, 12 TB of system memory, 1.5 TB of coprocessor memory, and over 73 TB of SSD storage, in aggregate.",Beacon,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2016-09-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-10-19 17:37:51.986
36,,production,gordon.sdsc.xsede.org,sdsc.xsede.org,production,2012-02-01,2017-03-31,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,61,resource,,"The Gordon Compute Cluster is an Appro-integrated, data-intensive computing resource. It is composed of 1,024 dual-socket compute nodes and 64 I/O nodes connected by a dual-rail, QDR InfiniBand, 3D torus network. Each compute node has two eight-core Intel EM64T Xeon E5 2.6 GHz (Sandy Bridge) processors and 64 GB of DRAM. The Sandy Bridge processor uses Intel’s Advanced Vector Extensions (AVX) to achieve 8 floating-point operations/clock cycle. The full 1,024 node cluster has a peak speed of 341 Tflop/s. Gordon has three distinct features that make it ideal for data intensive computing: 1) 300 TB of high performance Intel flash memory served via 64 I/O nodes, each of which is capable of over 560k IOPS, or 35M IOPS for the full system; 2) large-memory super-nodes that provide up to 1 TB of cache-coherent memory via ScaleMP’s vSMP Foundation software; and 3) 3 PB of disk in two Lustre-based file systems, the larger one of which is capable of sustained rates of 50 GB/s.",SDSC Appro with Intel Sandy Bridge Cluster (Gordon Compute Cluster),"{'coming_soon_begin_date': '2011-11-01', 'retired_begin_date': None, 'decommissioned_begin_date': '2017-04-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2012-01-31', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2017-03-31', 'friendly_end_date': None, 'production_begin_date': '2012-02-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-02-15 01:43:47.745
37,,decommissioned,cobalt.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2010-03-31,,"{'latitude': None, 'node_count': 2.0, 'manufacturer': 'SGI', 'rmax': None, 'interconnect': 'SGI NUMAlink', 'is_publicly_available': True, 'peak_teraflops': 8.2, 'advance_max_reservable_su': 1024.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530094, 'conversion_factors': [{'begin_date': '2005-04-01', 'factor': 1.45, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-NCSA', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'NCSA SGI Altix (Cobalt)', 'allocations_description': None, 'allocable_id': 125640}, 'job_manager': 'jobmanager (default)\n\njobmanager-pbs', 'batch_system': None, 'platform_name': 'Altix', 'nfs_network': None, 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': None, 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': None, 'community_software_area_email': 'mikep@ncsa.uiuc.edu', 'login_hostname': 'login-co.ncsa.teragrid.org', 'storage_network': None, 'graphics_card': None, 'is_visualization': False, 'memory_per_cpu_gb': 4.0, 'cpu_type': 'Intel Itanium2', 'alternate_login_hostname': None, 'xsedenet_participant': None, 'max_reservable_su': 1024.0, 'nickname': 'Cobalt', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': None, 'parallel_file_system': None, 'user_guide_url': 'http://www.ncsa.uiuc.edu/UserInfo/Resources/Hardware/SGIAltix/', 'disk_size_tb': 100.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.6, 'cpu_count_per_node': 512, 'operating_system': 'SGI ProPack 5', 'community_software_area': True}",5.0,XSEDE,XSEDE Level 1,125640,compute,The NCSA SGI Altix (cobalt) is intended primarily for running large shared-memory applications.,The NCSA SGI Altix consists of several Intel Itanium 2 processor shared-memory systems. The Altix  uses the CXFS shared parallel filesystem from SGI.,NCSA SGI Altix (Cobalt),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-02-28', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
38,,decommissioned,athena.nics.teragrid.org,nics.teragrid.org,decommissioned,2011-07-31,,"{'organizations': [{'organization_abbreviation': 'NICS', 'organization_name': 'National Institute for Computational Sciences', 'organization_code': 'T103349'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,30,resource,,"The Athena system is a Cray XT4 with 4512 compute nodes interconnected with SeaStar, a 3D torus. Each compute node has one four-core AMD Opteron for a total of 18048 cores. All nodes have 4 Gbytes of memory: 1 Gbyte of memory per core.

To use Athena, users must apply for time on Kraken and request a transfer to Athena. Based on job requirements, NICS staff will determine whether Athena is a good match.",NICS Cray XT4 (Athena),"{'coming_soon_begin_date': '2009-01-01', 'retired_begin_date': '2010-08-01', 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': '2011-07-30', 'coming_soon_end_date': '2010-07-25', 'post_production_begin_date': '2011-07-01', 'retired_end_date': '2010-09-30', 'production_end_date': '2011-06-30', 'friendly_end_date': None, 'production_begin_date': '2010-10-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2010-07-26', 'pre_production_end_date': '2010-09-30'}",
39,,decommissioned,tiger.iu.teragrid.org,iu.teragrid.org,decommissioned,2008-01-01,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,10,resource,,"The IU AVIDD IA-64 cluster has 8 compute nodes, each with four 1.3 GHz Intel Itanium2 CPUs, 6 GB of memory, 31 GB local scratch space, a Myrinet-2000 interconnect and access to 0.8 TB of cluster GPFS scratch space. NOTE: 100% of this resource is available for TeraGrid usage.

<strong class=       ""attention       "">IMPORTANT: </strong>There will be no more future allocations on this resource.",Indiana University (AVIDD-I64 IA-64 Cluster),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
40,GridFTP from spinning disk storage Big Red or from the IU Data Capacitor (via Lustre clients).  IU is also in the process of creating a Web portal interface for GridFTP access,decommissioned,data.iu.teragrid.org,iu.teragrid.org,decommissioned,2013-01-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': False, 'sensitive_data_support_description': None, 'user_guide_url': '', 'is_accounted': True, 'xsedenet_participant': None, 'databases': 'Oracle,MySQL', 'file_space_tb': 100.0, 'supports_sensitive_data': None}",88.0,XSEDE,XSEDE Level 1,139323,storage,Storage of persistent data collections on disk in any standard format as well as in Oracle and MySQL da,,Dedicated (nonpurged) disk for databases and data collections,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-01-01', 'friendly_begin_date': None, 'post_production_end_date': '2012-12-31', 'coming_soon_end_date': None, 'post_production_begin_date': '2012-12-31', 'retired_end_date': None, 'production_end_date': '2012-12-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
41,,decommissioned,dtf.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2009-06-30,,"{'latitude': None, 'node_count': 256.0, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': 'Myrinet', 'is_publicly_available': True, 'peak_teraflops': 3.1, 'advance_max_reservable_su': 524.0, 'is_accounted': True, 'sensitive_data_support_description': None, 'job_manager': 'jobmanager-forkjobmanager-pbs', 'batch_system': 'PBS', 'platform_name': 'IA-64 Cluster', 'nfs_network': 'Gigabit Ethernet', 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '1.7 TB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'wilkinsn@sdsc.edu', 'login_hostname': 'tg-login.sdsc.teragrid.org', 'storage_network': 'Fibre Channel', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Intel Itanium2 - Madison', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 524.0, 'nickname': 'SDSC DTF', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'GPFS: 50 TB', 'user_guide_url': 'http://www.sdsc.edu/us/resources/ia64/', 'disk_size_tb': 34.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.5, 'cpu_count_per_node': 2, 'operating_system': 'Linux 2.4 (SuSE 8.0)', 'community_software_area': True}",18.0,XSEDE,XSEDE Level 1,125874,compute,,"The TeraGrid cluster at SDSC comprises 262 IBM Itaninum2 nodes with two processors per node. Each node is built with SuSE Linux and interconnected with Myricom      's Myrinet. The system has a peak performance of 3.1 Teraflops, a total memory of 1 TB, and a total of 50 TB GPFS disk through the SAN network. Jobs are scheduled and run by the Catalina-scheduler and PBS-batch system.",SDSC DTF,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-06-30', 'friendly_begin_date': None, 'post_production_end_date': '2009-06-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2009-06-01', 'retired_end_date': None, 'production_end_date': '2009-05-30', 'friendly_end_date': None, 'production_begin_date': '2004-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
42,,decommissioned,mss.ncar.teragrid.org,ncar.teragrid.org,decommissioned,2011-03-21,,"{'organizations': [{'organization_abbreviation': 'NCAR', 'organization_name': 'National Center for Atmospheric Research', 'organization_code': '4007944'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,71,resource,,,NCAR Mass Storage System (Single or Double Copy),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-03-21', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-03-20', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
43,,decommissioned,database.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2009-01-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'user_guide_url': None, 'is_accounted': True, 'xsedenet_participant': None, 'databases': 'PostgreSQL,MySQL,Oracle', 'file_space_tb': 0.0, 'supports_sensitive_data': None}",77.0,XSEDE,XSEDE Level 1,142038,storage,,,NCSA Database,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
44,,production,comet.sdsc.xsede.org,sdsc.xsede.org,production,2015-04-01,2019-01-30,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,49,resource,,"Comet is a 2.0 Petaflop (PF) Dell integrated compute cluster, with next-generation Intel Haswell processors (with AVX2), interconnected with Mellanox FDR InfiniBand in a hybrid fat-tree topology. Full bisection bandwidth is available at rack level (72 nodes) and there is a 4:1 oversubscription cross-rack. Compute nodes feature 320 GB of SSD storage and 128GB of DRAM per node. The system also features 7PB of performance storage (200GB/s aggregate), and 6PB of durable storage. 36 nodes feature 4 NVIDIA GPUs per node (2 dual GPU K-80s). Additionally, four 1.5TB large memory nodes and additional nodes for Gateway hosting and VM image repositories are available. Comet will enable high performance virtualization using the single root I/O virtualization (SR-IOV) technology.",SDSC Dell Cluster with Intel Haswell Processors (Comet),"{'coming_soon_begin_date': '2014-09-12', 'retired_begin_date': None, 'decommissioned_begin_date': '2019-01-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2014-09-13', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-01-30', 'friendly_end_date': None, 'production_begin_date': '2015-04-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2014-09-14', 'pre_production_end_date': '2015-03-31'}",2016-05-19 00:06:12.305
45,,decommissioned,pople.psc.teragrid.org,psc.teragrid.org,decommissioned,2011-09-15,,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,26,resource,,"Pople is an SGI Altix 4700 comprising 192 blades with 8 GB of memory and 2 sockets on each blade.  Each socket is a 1.66GHz dual-core Intel Itanium 2 processor (Montvale).  Pople has a total of 384 sockets, 768 cores and 1.5 TB (2GB per core) of RAM.

The blades are linked with a NUMAlink interconnect.",PSC SGI Altix (Pople),"{'coming_soon_begin_date': '2007-12-31', 'retired_begin_date': '2008-01-01', 'decommissioned_begin_date': '2011-09-15', 'friendly_begin_date': None, 'post_production_end_date': '2011-09-14', 'coming_soon_end_date': '2008-06-29', 'post_production_begin_date': '2011-09-01', 'retired_end_date': '2008-08-31', 'production_end_date': '2011-08-31', 'friendly_end_date': None, 'production_begin_date': '2008-07-15', 'decommissioned_end_date': None, 'pre_production_begin_date': '2008-06-30', 'pre_production_end_date': '2008-07-14'}",
46,,decommissioned,abe-queenbee-steele.grid.teragrid.org,grid.teragrid.org,decommissioned,2010-01-01,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,105,resource,,,NCSA/LONI/Purdue Dell PowerEdge Linux Clusters (Abe/Queen Bee/Steele),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
47,,friendly,hpc.tufts.xsede.org,tufts.xsede.org,friendly,2014-06-01,2017-06-01,"{'organizations': [{'organization_abbreviation': 'Tufts U', 'organization_name': 'Tufts University', 'organization_code': '0022194'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 3,122,resource,,Tufts University High Performance Compute (HPC) Cluster is refreshed every three years. Previous generations of nodes are often kept online and active. ,Tufts University High Performance Compute (HPC) Cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': '2014-06-01', 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': '2017-06-01', 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-11-03 02:01:14.842
48,,production,staff-storage.xsede.xsede.org,xsede.xsede.org,production,2013-11-01,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': True}",,XSEDE,XSEDE Level 1,107,resource,,,XSEDE Staff Storage Grid,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2013-11-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-07-27 12:29:32.183
49,,decommissioned,lincoln.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2011-10-17,,"{'latitude': None, 'node_count': 192.0, 'manufacturer': 'Dell/Intel', 'rmax': None, 'interconnect': '', 'is_publicly_available': True, 'peak_teraflops': 47.5, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530136, 'conversion_factors': [{'begin_date': '2008-12-31', 'factor': 3.212, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'NCSA Dell with NVIDIA Tesla S1070 GPU Cluster (Lincoln)', 'allocations_description': None, 'allocable_id': 126690}, 'job_manager': '', 'batch_system': '', 'platform_name': 'PowerEdge 1950 with NVIDIA Tesla S1070', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Lincoln', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'Lustre (shared with Intel 64 Cluster abe)', 'user_guide_url': '', 'disk_size_tb': 400.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.33, 'cpu_count_per_node': 8, 'operating_system': 'Red Hat Enterprise Linux 4', 'community_software_area': False}",21.0,XSEDE,XSEDE Level 1,126690,compute,Lincoln is intended for applications that can make use of the heterogenous processors (CPU and GPU) that comprise this system.,Lincoln consists of 192 compute nodes (Dell PowerEdge 1950 dual-socket nodes with quad-core Intel Harpertown 2.33GHz processors and 16GB of memory) and 96 NVIDIA Tesla S1070 accelerator units. Each Tesla unit provides 345.6 gigaflops of double-precision performance and 16GB of memory.,NCSA Lincoln Supercluster,"{'coming_soon_begin_date': '2008-09-08', 'retired_begin_date': None, 'decommissioned_begin_date': '2011-10-17', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2008-10-14', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-10-16', 'friendly_end_date': None, 'production_begin_date': '2009-02-02', 'decommissioned_end_date': None, 'pre_production_begin_date': '2008-10-15', 'pre_production_end_date': '2009-02-01'}",
50,,decommissioned,dash.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2012-02-01,,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,27,resource,,"Dash is a cluster from Appro with 64 compute nodes connected by DDR InfiniBand. Each node has two Intel Nehalem quad-core processors running at 2.4 GHz and contains 48 GB of DRAM. Thus the entire 512-core cluster has 4.9 Tflop/s of peak performance and 3 TB of DRAM.

Dash is a prototype of Gordon, a much larger cluster targeted at data-intensive computing that will be installed in 2011. Hence Dash has two novel features, which will be scaled up in Gordon.

1. A large amount of flash memory contained in 64 Intel solid state drives (SSDs). Each 64-GB SSD will initially be attached to a separate compute node.

2. vSMP, ScaleMP 's virtual shared memory that currently allows aggregation of DRAM and flash memory across up to 16 nodes.

Dash hosts several file systems. NFS is provided for home and work space, GPFS-WAN is available for parallel I/O, and a scratch file system supports the SSDs.

The initial deployment is two 16-node partitions, one with vSMP and one without to allow for comparative benchmarking. Each partition has 768 GB of DRAM and 1 TB of flash memory. The remaining 32 compute nodes will be made available in late 2010 following acceptance testing of the 32-node vSMP functionality. ","SDSC Appro Flash Disk, vSMP Cluster (Dash)","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2012-02-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2012-01-31', 'friendly_end_date': None, 'production_begin_date': '2010-04-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
51,,decommissioned,gpfs-wan.ncar.teragrid.org,ncar.teragrid.org,decommissioned,2011-07-01,,"{'organizations': [{'organization_abbreviation': 'NCAR', 'organization_name': 'National Center for Atmospheric Research', 'organization_code': '4007944'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,79,resource,,,GPFS-WAN,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-06-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-03-01 16:19:29.291
52,,decommissioned,ember.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2011-07-31,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,28,resource,,The NCSA SGI Altix UV Ember comprises of four Intel Nehalem-EX shared-memory systems each with 384 cores and 2 TB of memory. ,NCSA SGI Altix UV (Ember),"{'coming_soon_begin_date': None, 'retired_begin_date': '2010-10-04', 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': '2010-11-01', 'production_end_date': '2011-07-30', 'friendly_end_date': None, 'production_begin_date': '2010-11-02', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
53,,production,orion.gsu.xsede.org,gsu.xsede.org,production,2015-07-01,2020-07-01,"{'organizations': [{'organization_abbreviation': 'Georgia State U', 'organization_name': 'Georgia State University', 'organization_code': '0015743'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 3,123,resource,,"Orion is a XSEDE Level3 heterogeneous resource located at the Georgia State University, Atlanta Georgia. Orion provide necessary hardware and software facilities , including large memory nodes, NVIDIA and Intel accelerators, hardware specifically designed for Hadoop/Spark based distributed memory computations and more than 150 centrally managed software packages to GSU researchers .  Major potion of Orion resources is centrally funded through university internal funding and available for the GSU community.  Portion of Orion is funded through NSF-CNS-1205650 and available for the local and the international collaborators through Curriculum Development and Educational Resources (CDER) project.  Orion also has condominium model to better serve needs of  individual research groups.",GSU  heterogeneous scientific computing infrastructure (Orion),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2020-07-01', 'friendly_end_date': None, 'production_begin_date': '2015-07-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-11-07 20:24:20.946
54,,,cowboy.okstate.xsede.org,okstate.xsede.org,,,,"{'organizations': [{'organization_abbreviation': 'Oklahoma State U', 'organization_name': 'Oklahoma State University', 'organization_code': '0088062'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 3,112,resource,,"In 2012, OSUHPCC deployed the largest externally funded supercomputer in Oklahoma history through NSF MRI grant “Acquisition of a High Performance Compute Cluster for Multidisciplinary Research,” OCI-1126330, 9/1/11-8/31/14, $908,812, PI Brunson.  This cluster, Cowboy, consists of 254 compute nodes, each with dual Intel Xeon E5-2620 “Sandy Bridge” hex core 2.0 GHz CPUs, most with 32 GB of 1333 MHz RAM and two “fat nodes” with 256 GB RAM.  The aggregate peak speed is 48.8 TFLOPs, with 3048 cores, 8576 GB of RAM and approximately 92 TB of globally accessible disk. The cluster also includes two NVIDIA Tesla C2075 cards.  Cowboy also includes 92 TB of globally accessible high performance disk provided by three shelves of Panasas ActivStor12, this includes 20x 2TB drives and peak speed of 1500MB/s read and 1600MB/s write per shelf.  The total solution provides an aggregate of 4.5GB/s read and 4.8GB/s write.  The interconnect networks are Infiniband for message passing, Gigabit Ethernet for I/O, and an ethernet management network.   The Infiniband for message passing is Mellanox Connect-X 3 QDR in a 2:1 oversubscription.  There are a total of 15x MIS5025Q switches providing both the leaf and spine components.  Each leaf is connects to 24 compute nodes, and 12x 40Gb QDR links to the spine.  Point-to-point latency is approx 1 microsecond.  The ethernet network includes 11 leaf gigabit switches that connect to 24 compute nodes.  Each leaf is uplinked via 2x 10G network ports to the spine 64 port Mellanox MSX1016 10 Gigabit switch.  The network configuration provides a 1.2:1 oversubscription.  

In January 2015, via faculty funds, OSUHPCC deployed 11 additional compute nodes each with four Xeon Phi accelerator cards.  Each node as dual Intel Xeon E5-2670v3 twelve core 2.3 GHz CPUs with 128 GB 2133 MHz RAM and four Intel Xeon Phi 31S1 8 GB, 1.1 GHz, 57 core coprocessors.  These compute nodes are shared with all users.  Additional storage for the Fennel lab is was also acquired and is maintained by OSUHPCC.",Oklahoma State cluster (Cowboy),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': '2012-09-02', 'pre_production_end_date': '2012-10-14'}",2016-07-11 19:21:54.919
55,,decommissioned,lemieux.psc.teragrid.org,psc.teragrid.org,decommissioned,2007-01-01,,"{'latitude': None, 'node_count': 410.0, 'manufacturer': 'HP', 'rmax': None, 'interconnect': 'Quadrics', 'is_publicly_available': True, 'peak_teraflops': 4.08, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 500063, 'conversion_factors': [{'begin_date': '2004-03-15', 'factor': 0.359, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'AAB', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'NRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-PSC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}], 'display_resource_name': 'TeraGrid PSC TCS1', 'allocations_description': None, 'allocable_id': 125580}, 'job_manager': 'jobmanager-lemieux-pbs', 'batch_system': 'PBS', 'platform_name': 'Alpha EV68', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '/home\n355 GB', 'rpeak': None, 'machine_type': 'MPP', 'gpu_description': None, 'community_software_area_email': None, 'login_hostname': 'tg-login1.lemieux.psc.teragrid.org', 'storage_network': 'SCSI2', 'graphics_card': None, 'is_visualization': False, 'memory_per_cpu_gb': 1.0, 'cpu_type': 'Alpha EV68', 'alternate_login_hostname': 'tg-login2.lemieux.psc.teragrid.org', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Lemieux', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': None, 'parallel_file_system': 'SCFS Maximum File System = 4 TB\n\n16 TB Total', 'user_guide_url': 'http://www.psc.edu/machines/tcs/lemieux.html', 'disk_size_tb': 78.13, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.0, 'cpu_count_per_node': 4, 'operating_system': 'Tru64 Unix', 'community_software_area': False}",4.0,XSEDE,XSEDE Level 1,125580,compute,Lemieux is primarily intended to run applications of very high levels of parallelism or concurrency (512 - 2048 processors).,"Lemieux comprises 610 Compaq Alphaserver ES45 nodes and two separate front end nodes. Each computational node is a 4 processor SMP, with 1-GHz Alpha EV68 processors and 4 Gbytes of memory.  They run the Tru64 Unix operating system. A dual-rail Quadrics interconnect links  the nodes.

IMPORTANT: No further allocations to this resource.",PSC TCS1,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2007-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
56,,production,stampede.tacc.xsede.org,tacc.xsede.org,production,2013-01-07,2017-08-31,"{'organizations': [{'organization_abbreviation': 'UT Austin', 'organization_name': 'University of Texas at Austin', 'organization_code': '0036582'}, {'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,64,resource,,"The new Stampede Dell PowerEdge C8220 Cluster is configured with 6,400 Dell DCS Zeus compute nodes, each with two 2.7 GHz E5-2680 Intel Xeon (Sandy Bridge) processors.  With 32 GB of memory and 50 GB of storage per node, users have access to an aggregate of 205 TB of memory and 275+ TB of local storage.  The cluster is also equipped with Intel Xeon Phi coprocessors based on Intel Many Integrated Core (Intel MIC) architecture.  Stampede will deliver 2+ PF of peak performance on the main cluster and 7+ PF of peak performance on the Intel Xeon Phi coprocessors.  Stampede also provides access to 16 large memory nodes with 1TB each of RAM, and 128 nodes containing an NVIDIA Kepler 2 GPU, giving users access to large shared-memory computing and remote visualization capabilities, respectively. Compute nodes have access to a 14 PB Lustre Parallel file system.  An FDR InfiniBand switch fabric interconnects the nodes through a fat-tree topology with a point-to-point bandwidth of 40GB/sec (unidirectional speed).",TACC Dell PowerEdge C8220 Cluster with Intel Xeon Phi coprocessors (Stampede),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2017-08-31', 'friendly_end_date': None, 'production_begin_date': '2013-01-07', 'decommissioned_end_date': None, 'pre_production_begin_date': '2012-09-15', 'pre_production_end_date': '2013-01-06'}",2017-01-11 14:12:03.328
57,,decommissioned,teradre.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2010-07-01,,"{'organizations': [{'organization_abbreviation': 'Purdue U', 'organization_name': 'Purdue University', 'organization_code': '0018259'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,13,resource,,"The Purdue TeraDRE is a high-throughput visualization resource built on the Purdue Condor Pools. A 48-node subcluster featuring Nvidia GeForce 6600 GT GPUs are available for GPU-accelerated programs and hardware-accelerated rendering using 
Gelato.",Purdue Distributed Rendering Environment (TeraDRE),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-06-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
58,,coming soon,stampede2.tacc.xsede.org,tacc.xsede.org,coming soon,,,"{'organizations': [{'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}, {'organization_abbreviation': 'UT Austin', 'organization_name': 'University of Texas at Austin', 'organization_code': '0036582'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,126,resource,,"The new Stampede2 Dell/Intel Knights Landing (KNL) System is configured with 4204 Dell KNL compute nodes, each with a new stand alone Intel Xeon Phi Knights Landing bootable processor.  Each KNL node will include 68 cores, 16GB MCDRAM, 96GB DDR-4 memory and a 200GB SSD drive.  Stampede2 will deliver an estimated 13PF of peak performance. Compute nodes have access to dedicated Lustre Parallel file systems totaling 28PB raw, provided by Seagate. An Intel Omni-Path Architecture switch fabric connects the nodes and storage through a fat-tree topology with a point to point bandwidth of 100 Gb/s (unidirectional speed). 16 additional login and management servers complete the system.  Later in 2017, Stampede2 Phase 2 consisting of next generation Xeon servers and additional management nodes will be deployed.",TACC Dell/Intel Knight's Landing System (Stampede2 - Phase 1),"{'coming_soon_begin_date': '2017-03-14', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2017-05-08', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2021-09-30', 'friendly_end_date': None, 'production_begin_date': '2017-07-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2017-05-09', 'pre_production_end_date': '2017-06-30'}",2017-03-16 12:44:50.403
59,,decommissioned,radium.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2007-04-16,,"{'latitude': None, 'node_count': 1.0, 'manufacturer': None, 'rmax': None, 'interconnect': None, 'is_publicly_available': True, 'peak_teraflops': 1.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530100, 'conversion_factors': [{'begin_date': '2006-04-01', 'factor': 0.046, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-NCSA', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}], 'display_resource_name': 'NCSA Condor Pool', 'allocations_description': None, 'allocable_id': 125658}, 'job_manager': None, 'batch_system': None, 'platform_name': 'Condor Flock', 'nfs_network': None, 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': None, 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': None, 'login_hostname': None, 'storage_network': None, 'graphics_card': None, 'is_visualization': False, 'memory_per_cpu_gb': 1.0, 'cpu_type': None, 'alternate_login_hostname': None, 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Radium', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': None, 'parallel_file_system': None, 'user_guide_url': 'http://www.ncsa.uiuc.edu/UserInfo/Resources/Hardware/Condor/', 'disk_size_tb': 1.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.0, 'cpu_count_per_node': 1, 'operating_system': None, 'community_software_area': False}",6.0,XSEDE,XSEDE Level 1,125658,compute,,This resource is no longer available to TeraGrid.,NCSA Condor Pool (Radium),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2007-04-16', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
60,,decommissioned,maverick.tacc.teragrid.org,tacc.teragrid.org,decommissioned,2008-12-01,,"{'organizations': [{'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,58,resource,," The TACC Visualization System Maverick is a Ssun Fire E25K intended for serial and parallel, visualization applications that take advantage of a large shared memory and multiple  (up to 128) UltraSPARC IV compute processors and (8) Sun Jasmine graphics processors  ",TACC Sun Large-Scale Visualization (Maverick),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-12-01', 'friendly_begin_date': None, 'post_production_end_date': '2008-11-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2008-01-01', 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
61,,decommissioned,tape.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2011-07-01,,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,70,resource,,,SDSC Tape Storage,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-01', 'friendly_begin_date': None, 'post_production_end_date': '2011-06-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2010-09-30', 'retired_end_date': None, 'production_end_date': '2010-09-29', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
62,,decommissioned,queenbee.loni-lsu.teragrid.org,loni-lsu.teragrid.org,decommissioned,2011-07-31,,"{'organizations': [{'organization_abbreviation': 'LONI-LSU', 'organization_name': 'Louisiana Optical Network Initiative', 'organization_code': 'T103341'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,17,resource,,Queen Bee is a 50.7 TFlops Peak Performance 668 node Dell PowerEdge 1950 cluster running the Red Hat Enterprise Linux 4 operating system. Each node contains two Quad Core Intel Xeon 2.33GHz 64-bit processors and 8 GB of memory. The cluster is interconnected with 10 Gb/sec Infniband and has total 192TB (raw) of storage in shared Lustre filesystems.,queen-bee pops name,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-02-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
63,,decommissioned,ember.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2011-07-31,,"{'latitude': None, 'node_count': 4.0, 'manufacturer': 'SGI', 'rmax': None, 'interconnect': 'SGI NUMAlink', 'is_publicly_available': True, 'peak_teraflops': 16.0, 'advance_max_reservable_su': 378.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530144, 'conversion_factors': [{'begin_date': None, 'factor': 2.212, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'NCSA SGI Altix UV (Ember)', 'allocations_description': None, 'allocable_id': 142232}, 'job_manager': 'MOAB Scheduler', 'batch_system': 'Portable Batch System (PBS Pro)', 'platform_name': 'Altix UV', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'login-ember.ncsa.teragrid.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 5.3, 'cpu_type': 'Intel Nehalem-EX', 'alternate_login_hostname': 'ember.ncsa.illinois.edu', 'xsedenet_participant': None, 'max_reservable_su': 378.0, 'nickname': 'Ember', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '141.142.160.38', 'parallel_file_system': 'GPFS', 'user_guide_url': 'https://www.xsede.org/web/guest/ncsa-ember', 'disk_size_tb': 400.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.66, 'cpu_count_per_node': 384, 'operating_system': 'SGI ProPack 7 ', 'community_software_area': False}",28.0,XSEDE,XSEDE Level 1,142232,compute,Ember is primarily intended for running moderate to large shared-memory applications. ,The NCSA SGI Altix UV Ember comprises of four Intel Nehalem-EX shared-memory systems each with 384 cores and 2 TB of memory. ,NCSA SGI Altix UV (Ember),"{'coming_soon_begin_date': None, 'retired_begin_date': '2010-10-04', 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': '2010-11-01', 'production_end_date': '2011-07-30', 'friendly_end_date': None, 'production_begin_date': '2010-11-02', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
64,,production,TeraGrid.teragrid.org,teragrid.org,production,2003-01-01,,"{'organizations': [{'organization_abbreviation': 'TG', 'organization_name': 'TeraGrid', 'organization_code': '00000TG'}], 'xsede_services_only': True}",,XSEDE,XSEDE Level 1,103,resource,,,TeraGrid Wide Roaming Access,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2003-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-07-27 12:28:29.363
65,,"friendly, production",bridges.psc.xsede.org,psc.xsede.org,production,2016-01-01,2019-11-30,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,38,resource,,Regular Shared Memory nodes each consist of two Intel Xeon EP-series CPUs and 128GB of 2133 MHz DDR4 RAM configured as 8 DIMMs with 16GB per DIMM.  A subset of RSM nodes contain NVIDIA Tesla GPUs: 16 nodes will contain two K80 GPUs each. We anticipate adding 32 RSM nodes with two Pascal GPUs each in late 2016.  Bridges contains many hundreds of RSM nodes for capacity and flexibility. ,PSC Regular Memory (Bridges),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2019-11-30', 'friendly_begin_date': '2016-03-11', 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-11-30', 'friendly_end_date': None, 'production_begin_date': '2016-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2015-09-15', 'pre_production_end_date': '2015-12-31'}",2017-02-14 18:40:39.528
66,,production,arc-ts.umich.xsede.org,umich.xsede.org,production,2016-01-01,,"{'organizations': [{'organization_abbreviation': 'U Michigan', 'organization_name': 'University of Michigan', 'organization_code': '0090910'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 3,129,resource,,"http://arc-ts.umich.edu/

ARC-TS is part of ARC (http://arc.umich.edu) providing research resources to faculty at the University of Michigan.
",Advanced Research Computing - Technology Services University of Michigan,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2016-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-17 20:50:51.545
67,,decommissioned,project-work-space.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2009-01-01,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,69,resource,,,NCSA Project Work Space Per Host,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
68,"Through local NCSA login for data update, and public accessible depending on data requirements from user",decommissioned,data-resource.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2010-09-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': 'Andrew File System (AFS) storage', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'user_guide_url': None, 'is_accounted': True, 'xsedenet_participant': None, 'databases': 'N/A', 'file_space_tb': 2.0, 'supports_sensitive_data': None}",74.0,XSEDE,XSEDE Level 1,141663,storage,Web/FTP access to smaller datasets.   Users that required a collection to be accessed by users through anonymous ftp or have web portal applications can request data storage ,,NCSA Data Resource Services,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-09-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-08-31', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
69,,decommissioned,tungsten.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2008-07-21,,"{'latitude': None, 'node_count': 1280.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': None, 'is_publicly_available': True, 'peak_teraflops': 16.38, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530102, 'conversion_factors': [{'begin_date': '2006-02-01', 'factor': 0.931, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-NCSA', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}], 'display_resource_name': 'NCSA Xeon Linux Supercluster (Tungsten)', 'allocations_description': None, 'allocable_id': 125700}, 'job_manager': 'jobmanager (default)\n\njobmanager-lsf', 'batch_system': None, 'platform_name': 'Xeon IA-32 Cluster', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '122 TB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': None, 'login_hostname': 'login-w.ncsa.teragrid.org', 'storage_network': None, 'graphics_card': None, 'is_visualization': False, 'memory_per_cpu_gb': 1.5, 'cpu_type': 'Intel Xeon', 'alternate_login_hostname': None, 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Tungsten', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': None, 'parallel_file_system': None, 'user_guide_url': 'http://www.ncsa.uiuc.edu/UserInfo/Resources/Hardware/XeonCluster/', 'disk_size_tb': 109.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 3.2, 'cpu_count_per_node': 2, 'operating_system': 'RedHat Linux 9', 'community_software_area': False}",9.0,XSEDE,XSEDE Level 1,125700,compute,"[DECOMM]<!-- please use this space for Recommended Use and Status -->
The NCSA Xeon Linux Cluster (tungsten) is primarily intended to run applications of moderate to high levels of parallelism, particularly those needing a 32-bit environment and codes that perform well in a distributed cluster environment.<br />

<strong>Status</strong>
<!--
In production and accepting allocation requests
-->
This resource was retired from service on July 21, 2008","This cluster is based on Dell PowerEdge 1750 servers, each with two Intel IA-32 Xeon 3.2 GHz processors and 3 GB of memory. It is running Red Hat Linux and Myricom               's Myrinet cluster interconnect network, and the CFS Lustre shared parallel filesystem.",NCSA Xeon Linux Supercluster (Tungsten),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-07-21', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
70,,decommissioned,gpfs-wan.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2009-01-01,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,81,resource,,,GPFS-WAN,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
71,"The Data Supercell is available to XSEDE users who
have an allocation on PSC resources.",production,data.psc.xsede.org,psc.xsede.org,production,2012-04-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': 'The Data Supercell is a high-bandwidth, high-capacity, high-reliability, complex disk-based system accessible through a variety of file transfer methods.', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'GigaBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530164, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2015-08-01', 'required_resource_display_name': 'PSC HP Superdome and HP DL580 (Greenfield)', 'end_date': '2016-12-31', 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530187, 'allocable_id': 144544}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': 0.25}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-08-01', 'required_resource_display_name': 'PSC HP Superdome and HP DL580 (Greenfield)', 'end_date': '2016-12-31', 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530187, 'allocable_id': 144544}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.25}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-08-01', 'required_resource_display_name': 'PSC HP Superdome and HP DL580 (Greenfield)', 'end_date': '2016-12-31', 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530187, 'allocable_id': 144544}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.25}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.25}], 'display_resource_name': 'PSC Persistent disk storage (Data SuperCell)', 'allocations_description': None, 'allocable_id': 142290}, 'is_accounted': True, 'user_guide_url': 'https://portal.xsede.org/psc-data-supercell', 'databases': '', 'supports_sensitive_data': None, 'file_space_tb': 4000.0, 'xsedenet_participant': None}",85.0,XSEDE,XSEDE Level 1,142290,storage,PSC's file archival system,,PSC Persistent disk storage (Data SuperCell),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2012-04-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
72,,decommissioned,trestles.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2015-05-01,,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,40,resource,,"Trestles consists of 324 compute nodes, with a total of 10,368 cores across the system. Each node is powerful and memory-rich, with four 8-core 2.4 GHz AMD Magny-Cours processors, for a total of 32 cores per node, 64 GB of DDR3 RAM and a 120 GB SSD local disk drive. The compute nodes are connected via QDR InfiniBand interconnect in a fat-tree topology, with each link capable of 8 GB/s (bidirectional). Trestles’ scratch Lustre parallel file system has 800 TB capacity and 20 GB/s bandwidth. In addition, Trestles users receive a default 500 GB persistent storage allocation on SDSC’s “Project Storage” resource, and can request up to 50 TB storage as a separate allocation.",SDSC Appro Linux Cluster (Trestles),"{'coming_soon_begin_date': '2010-10-01', 'retired_begin_date': None, 'decommissioned_begin_date': '2015-05-01', 'friendly_begin_date': None, 'post_production_end_date': '2015-04-30', 'coming_soon_end_date': '2010-12-31', 'post_production_begin_date': '2014-09-14', 'retired_end_date': None, 'production_end_date': '2014-09-13', 'friendly_end_date': None, 'production_begin_date': '2011-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
73,,production,grid1.osg.xsede.org,osg.xsede.org,production,2012-04-01,2018-05-31,"{'organizations': [{'organization_abbreviation': 'OSG', 'organization_name': 'Open Science Grid', 'organization_code': 'T-OSG'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,48,resource,,A virtual HTCondor pool made up of resources from the Open Science Grid,Open Science Grid (OSG),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2018-05-31', 'friendly_end_date': None, 'production_begin_date': '2012-04-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2012-03-01', 'pre_production_end_date': '2012-03-31'}",2016-05-11 04:05:58.169
74,,production,oasis-dm.sdsc.xsede.org,sdsc.xsede.org,production,2012-09-11,,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,86,resource,,,SDSC Medium-term disk storage (Data Oasis),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2012-09-11', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
75,,production,glade.ncar.xsede.org,ncar.xsede.org,production,2012-12-01,,"{'organizations': [{'organization_abbreviation': 'NCAR', 'organization_name': 'National Center for Atmospheric Research', 'organization_code': '4007944'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 2,92,resource,,,NCAR central file systems and data storage (GLADE),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2012-12-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-06-08 16:55:43.889
76,,production,TeraGrid.teragrid.org,teragrid.org,production,2003-01-01,,"{'is_accounted': True, 'is_authenticated': True, 'user_guide_url': '', 'is_authorized': True, 'is_publicly_available': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'GridResource', 'allocable_resource_id': 530090, 'conversion_factors': [], 'allocation_type_specifics': [], 'display_resource_name': 'TeraGrid Wide Roaming Access', 'allocations_description': None, 'allocable_id': 142227}, 'resources_in_grid': []}",103.0,XSEDE,XSEDE Level 1,142227,grid,,TeraGrid Wide Roaming Access,TeraGrid Wide Roaming Access,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2003-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
77,,production,stampede.tacc.xsede.org,tacc.xsede.org,production,2013-01-07,2017-08-31,"{'latitude': None, 'node_count': 6400.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'FDR InfiniBand', 'is_publicly_available': True, 'peak_teraflops': 9600.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Core-hours', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530159, 'conversion_factors': [{'begin_date': None, 'factor': 4.599, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.016600917}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': 50000.0, 'allocation_type': 'Startup', 'dollar_value': 0.016600917}, {'relative_order': None, 'required_resources': [{'begin_date': '2016-09-01', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.016600917}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.016600917}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 0.016600917}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.016600917}], 'display_resource_name': 'TACC Dell PowerEdge C8220 Cluster with Intel Xeon Phi coprocessors (Stampede)', 'allocations_description': '', 'allocable_id': 142291}, 'job_manager': '', 'batch_system': 'SLURM', 'platform_name': 'Dell PowerEdge C8220 Cluster with Intel Xeon Phi coprocessors', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '14 PB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': 'xsede-support@tacc.utexas.edu', 'login_hostname': 'stampede.tacc.xsede.org', 'storage_network': 'FDR InfiniBand', 'graphics_card': 'NVIDIA Kepler 2', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Intel Xeon E5-2680', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Stampede', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://portal.xsede.org/tacc-stampede', 'disk_size_tb': 14336.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.7, 'cpu_count_per_node': 16, 'operating_system': 'Linux (CentOS)', 'community_software_area': True}",64.0,XSEDE,XSEDE Level 1,142291,compute,"Stampede is intended primarily for parallel applications scalable to tens of thousands of cores.  Normal batch queues will enable users to run simulations up to 48 hours.  Jobs requiring run times and more cores than allowed by the normal queues will be run in a special queue after approval of TACC staff.  Serial and development queues are configured. Additionally, users will be able to run jobs via special queues using up to 1) 4 large memory nodes each containing a MIC and GPU, 2) 256 nodes with Intel Xeon Phi coprocessors, 3) 32 nodes containing NVIDIA Kepler 2 GPUs, and 4) 32 nodes containing GPUs plus VNC visualization services.","The new Stampede Dell PowerEdge C8220 Cluster is configured with 6,400 Dell DCS Zeus compute nodes, each with two 2.7 GHz E5-2680 Intel Xeon (Sandy Bridge) processors.  With 32 GB of memory and 50 GB of storage per node, users have access to an aggregate of 205 TB of memory and 275+ TB of local storage.  The cluster is also equipped with Intel Xeon Phi coprocessors based on Intel Many Integrated Core (Intel MIC) architecture.  Stampede will deliver 2+ PF of peak performance on the main cluster and 7+ PF of peak performance on the Intel Xeon Phi coprocessors.  Stampede also provides access to 16 large memory nodes with 1TB each of RAM, and 128 nodes containing an NVIDIA Kepler 2 GPU, giving users access to large shared-memory computing and remote visualization capabilities, respectively. Compute nodes have access to a 14 PB Lustre Parallel file system.  An FDR InfiniBand switch fabric interconnects the nodes through a fat-tree topology with a point-to-point bandwidth of 40GB/sec (unidirectional speed).",TACC Dell PowerEdge C8220 Cluster with Intel Xeon Phi coprocessors (Stampede),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2017-08-31', 'friendly_end_date': None, 'production_begin_date': '2013-01-07', 'decommissioned_end_date': None, 'pre_production_begin_date': '2012-09-15', 'pre_production_end_date': '2013-01-06'}",2017-01-12 15:43:37.495
78,,decommissioned,longhorn.tacc.teragrid.org,tacc.teragrid.org,decommissioned,2014-04-18,,"{'organizations': [{'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,35,resource,,"The Longhorn Dell/NVIDIA Visualization and Data Analysis cluster is configured with 256 dual-socket nodes, each with significant compute and graphics capability. Total system resources include 2048 compute cores (2.53 GHzIntel Nehalem), 512 GPUs (128 NVIDIA Quadro Plex S4s, each containing 4 NVIDIA FX 5800s), 13.5 TB of distributed memory and a 210 TB local Lustre parallel file system.",TACC Dell/NVIDIA Visualization and Data Analysis Cluster (Longhorn),"{'coming_soon_begin_date': '2009-09-10', 'retired_begin_date': '2009-11-02', 'decommissioned_begin_date': '2014-04-18', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2009-11-01', 'post_production_begin_date': None, 'retired_end_date': '2009-12-07', 'production_end_date': '2014-04-17', 'friendly_end_date': None, 'production_begin_date': '2010-01-04', 'decommissioned_end_date': None, 'pre_production_begin_date': '2009-11-02', 'pre_production_end_date': '2010-01-03'}",
79,,production,staff-storage.xsede.xsede.org,xsede.xsede.org,production,2013-11-01,,"{'is_accounted': False, 'is_authenticated': False, 'user_guide_url': '', 'is_authorized': False, 'is_publicly_available': False, 'allocations_info': {'unit_type': 'GigaBytes', 'allocable_type': 'GridResource', 'allocable_resource_id': 530169, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Staff', 'dollar_value': None}], 'display_resource_name': 'XSEDE Staff Storage Grid', 'allocations_description': None, 'allocable_id': 142320}, 'resources_in_grid': []}",107.0,XSEDE,XSEDE Level 1,142320,grid,,XSEDE Staff Storage Grid,XSEDE Staff Storage Grid,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2013-11-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
80,,production,supermic.cct-lsu.xsede.org,cct-lsu.xsede.org,production,2014-10-01,,"{'latitude': None, 'node_count': 360.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'Infiniband', 'is_publicly_available': False, 'peak_teraflops': 925.0, 'advance_max_reservable_su': None, 'is_accounted': False, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530170, 'conversion_factors': [{'begin_date': None, 'factor': 4.599, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.049120000000000004}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.049120000000000004}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.049120000000000004}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': 50000.0, 'allocation_type': 'Startup', 'dollar_value': 0.049120000000000004}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.049120000000000004}], 'display_resource_name': 'LSU Cluster (superMIC)', 'allocations_description': None, 'allocable_id': 142323}, 'job_manager': 'jobmanager-pbs', 'batch_system': 'Torque/Moab', 'platform_name': 'Intel 64 Linux Cluster', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': 'Lustre', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'supermic.cct-lsu.xsede.org', 'storage_network': 'InfiniBand and Gigabit Ethernet', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 3.2, 'cpu_type': 'Intel 64', 'alternate_login_hostname': 'smic1.hpc.lsu.edu', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'SuperMIC', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '204.90.40.21', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://portal.xsede.org/lsu-supermic', 'disk_size_tb': 840.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.8, 'cpu_count_per_node': 20, 'operating_system': 'Red Hat Enterprise Linux 6 (Linux 2.6.32-358)', 'community_software_area': False}",65.0,XSEDE,XSEDE Level 2,142323,compute,"SuperMIC is intended to service parallel applications that are scalable to 10,000 cores or more. Representative applications in this catagory include, but are not limited to, molecular dynamics simulations, adaptive sparse grids, ray tracing, finite element solvers, lattice QCD, and Monte Carlo / Black-Sholes stated-preference analysis. Highly distributed, embarrissingly parallel problems involving identical analysis across huge numbers of input files are also candidates with suitable task management. The hybrid nodes with a 7120P and a K20X are intended for the exploration of mixed programming methodologies.
","SuperMIC is a 925 TFlop Peak Performance Xeon Phi accelerated cluster.  SuperMIC has 360 nodes each with 20 Intel Ivybridge 2.8 GHz cores, 64 GB of RAM, and two Intel Xeon Phi 7120P co-processors. There are 20 nodes that have NVIDIA K20X GPUs. This cluster is 40% allocated to the XSEDE user community and 60% dedicated to authorized users of the LSU community.  Access is restricted to those who meet the criteria as stated on our website.",LSU Cluster (superMIC),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2014-10-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2014-09-01', 'pre_production_end_date': '2014-09-30'}",
81,"GPFS-WAN is currently mounted on the following machines:
IU: tg-login.iu.teragrid.org (Big Red PPC Linux Cluster), SDSC: tg-login.sdsc",decommissioned,gpfs-wan.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2011-10-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'user_guide_url': None, 'is_accounted': True, 'xsedenet_participant': None, 'databases': 'Not applicable', 'file_space_tb': 700.0, 'supports_sensitive_data': None}",80.0,XSEDE,XSEDE Level 1,140743,storage,"TeraGrid GPFS-WAN (Global Parallel File System-Wide Area Network) is a large-scale storage system mounted on several TeraGrid platforms. Although the system is physically located at SDSC, it looks lik",,GPFS-WAN,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-10-01', 'friendly_begin_date': None, 'post_production_end_date': '2011-09-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2011-07-01', 'retired_end_date': None, 'production_end_date': '2011-06-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
82,,decommissioned,dtf.anl.teragrid.org,anl.teragrid.org,decommissioned,2009-07-01,,"{'latitude': None, 'node_count': 64.0, 'manufacturer': 'Intel', 'rmax': None, 'interconnect': 'Myrinet', 'is_publicly_available': True, 'peak_teraflops': 0.61, 'advance_max_reservable_su': 124.0, 'is_accounted': True, 'sensitive_data_support_description': None, 'job_manager': 'Default/Interactive: jobmanager-fork  Batch: jobmanager-pbs', 'batch_system': '', 'platform_name': 'IA-64 Cluster', 'nfs_network': 'Gigabit Ethernet', 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'tg-login.uc.teragrid.org', 'storage_network': 'Fibre Channel', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Intel Itanium 2', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 124.0, 'nickname': 'tg-login.uc.teragrid.org', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'GPFS: 11 TB scratch\r\n4 TB home/software\r\n\r\nPVFS: 5 TB scratch', 'user_guide_url': 'http://www.uc.teragrid.org/user-guide.html', 'disk_size_tb': 4.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.5, 'cpu_count_per_node': 2, 'operating_system': 'Red Hat Enterprise Linux 4', 'community_software_area': False}",32.0,XSEDE,XSEDE Level 1,125910,compute,,"The IA-64 TeraGrid Linux Cluster at UC/ANL consists of 62 nodes with dual Intel Itanium 2 processors, with 4 GB of memory per node. The cluster is running Red Hat Enterprise Linux and is using the Myricom Myrinet cluster interconnect network. There is a 16 TB local high-performance GPFS, and access to the TeraGrid-wide GPFS-WAN file-system.",UChicago/Argonne compute cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-07-01', 'friendly_begin_date': None, 'post_production_end_date': '2009-06-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2009-06-01', 'retired_end_date': None, 'production_end_date': '2009-05-31', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
83,,production,bridges.psc.xsede.org,psc.xsede.org,production,2016-01-01,,"{'latitude': None, 'node_count': 752.0, 'manufacturer': 'HP', 'rmax': None, 'interconnect': 'Intel Omni-Path Fabric', 'is_publicly_available': True, 'peak_teraflops': 894.6, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530189, 'conversion_factors': [{'begin_date': None, 'factor': 6.165, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Storage (Bridges Pylon)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530191, 'allocable_id': 144548}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.02315}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Storage (Bridges Pylon)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530191, 'allocable_id': 144548}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.02315}, {'relative_order': None, 'required_resources': [{'begin_date': '2016-09-01', 'required_resource_display_name': 'PSC Storage (Bridges Pylon)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530191, 'allocable_id': 144548}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.02315}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Storage (Bridges Pylon)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530191, 'allocable_id': 144548}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': 50000.0, 'allocation_type': 'Startup', 'dollar_value': 0.02315}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-09-01', 'required_resource_display_name': 'PSC Storage (Bridges Pylon)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530191, 'allocable_id': 144548}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.02315}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 0.02315}], 'display_resource_name': 'PSC Regular Memory (Bridges)', 'allocations_description': '1 Service Unit = 1 Core Hour', 'allocable_id': 144546}, 'job_manager': 'SLURM', 'batch_system': 'SLURM', 'platform_name': 'HPE Apollo 2000s', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': True, 'model': 'HPE Apollo 2000s', 'primary_storage_shared_gb': 'Pylon', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '16 GPU nodes in  Phase 1 of Bridges.   They are HPE Apollo 2000s, each with 2 NVIDIA K80 GPUs, 2 Intel Xeon E5-2695 v3 CPUs (14 cores per CPU) and 128GB RAM.', 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': 'NVIDIA K80 GPUs', 'is_visualization': False, 'memory_per_cpu_gb': 128.0, 'cpu_type': 'Intel Xeon EP-series', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Bridges Regular Memory', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': True, 'ip_address': '', 'parallel_file_system': 'Pylon', 'user_guide_url': 'https://portal.xsede.org/psc-bridges', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': 8.0, 'cpu_speed_ghz': 2.3, 'cpu_count_per_node': 28, 'operating_system': 'CentOS', 'community_software_area': False}",38.0,XSEDE,XSEDE Level 1,144546,compute,"Regular Shared Memory nodes will provide substantial capacity, especially for jobs such as optimizations and parameter sweeps that require vast numbers of separate runs or threads, support interactivity and dedicated access, power complex workflows, support the Hadoop ecosystem, and enable accelerated computing.  Large-memory GPUs will accelerate important alogrithms such as deep learning.",Regular Shared Memory nodes each consist of two Intel Xeon EP-series CPUs and 128GB of 2133 MHz DDR4 RAM configured as 8 DIMMs with 16GB per DIMM.  A subset of RSM nodes contain NVIDIA Tesla GPUs: 16 nodes will contain two K80 GPUs each. We anticipate adding 32 RSM nodes with two Pascal GPUs each in late 2016.  Bridges contains many hundreds of RSM nodes for capacity and flexibility.,PSC Regular Memory (Bridges),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2016-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2015-09-15', 'pre_production_end_date': '2015-12-31'}",2016-09-12 16:47:20.423
84,,,greenfield.psc.xsede.org,psc.xsede.org,,,,"{'latitude': None, 'node_count': 3.0, 'manufacturer': 'HP', 'rmax': None, 'interconnect': '', 'is_publicly_available': True, 'peak_teraflops': None, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530187, 'conversion_factors': [{'begin_date': '2015-07-15', 'factor': 11.4944, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'Transfers Only', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.64}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-08-01', 'required_resource_display_name': 'PSC Persistent disk storage (Data SuperCell)', 'end_date': '2016-12-31', 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530164, 'allocable_id': 142290}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'Transfers Only', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.64}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-08-01', 'required_resource_display_name': 'PSC Persistent disk storage (Data SuperCell)', 'end_date': '2016-12-31', 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530164, 'allocable_id': 142290}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'Transfers Only', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.64}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-08-01', 'required_resource_display_name': 'PSC Persistent disk storage (Data SuperCell)', 'end_date': '2016-12-31', 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530164, 'allocable_id': 142290}], 'default_amount': 1000.0, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'Transfers Only', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': 0.64}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.64}], 'display_resource_name': 'PSC HP Superdome and HP DL580 (Greenfield)', 'allocations_description': '', 'allocable_id': 144544}, 'job_manager': '', 'batch_system': 'PBS', 'platform_name': 'PSC HP Superdome and HP DL580 (Greenfield) ', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'greenfield.psc.xsede.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 51.2, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Greenfield', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': '', 'user_guide_url': 'http://www.psc.edu/index.php/resources-for-users/computing-resources/greenfield', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.3, 'cpu_count_per_node': 120, 'operating_system': 'CentOS', 'community_software_area': False}",62.0,XSEDE,XSEDE Level 1,144544,compute,Greenfield is intended for applications that require a large shared memory for computational tasks such as data analytics.,"Greenfield comprises an HP Superdome and  two HP DL580s. 

The Superdome contains 240 cores and 12TB of memory; the HP DL580s have 60 cores and 3 TB each. ",PSC HP Superdome and HP DL580 (Greenfield) ,"{'coming_soon_begin_date': '2015-07-15', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2015-08-04', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2016-12-31', 'friendly_end_date': None, 'production_begin_date': '2015-08-05', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-10-04 20:25:28.729
85,,decommissioned,mercury.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2010-03-31,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,24,resource,,"NCSA's IA-64 TeraGrid Linux Cluster consists of 887 IBM nodes: 256 nodes with dual 1.3 GHz Intel Itanium 2 processors (half with 4 GB of memory per node, and the other half with 12 GB of memory per node), and 631 nodes with dual 1.5 GHz Intel Itanium 2 processors (4 GB of memory per node). The cluster is running SuSE Linux and is using Myricom's Myrinet cluster interconnect network, and the GPFS parallel filesystem.",Mercury,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
86,,,data.psc.xsede.org,psc.xsede.org,,,,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,85,resource,,,PSC Persistent disk storage (Data SuperCell),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2016-12-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2016-12-31', 'friendly_end_date': None, 'production_begin_date': '2012-04-01', 'decommissioned_end_date': '2016-12-31', 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-01-11 15:54:01.513
87,,,arc-ts.umich.xsede.org,umich.xsede.org,,,,"{'latitude': None, 'node_count': None, 'manufacturer': '', 'rmax': None, 'interconnect': '', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': False, 'sensitive_data_support_description': '', 'job_manager': '', 'batch_system': '', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'flux-login.arc-ts.umich.edu', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': None, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Flux', 'supports_sensitive_data': False, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': 'http://arc-ts.umich.edu/systems-and-services/flux/', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': None, 'cpu_count_per_node': None, 'operating_system': '', 'community_software_area': False}",129.0,XSEDE,XSEDE Level 3,144588,compute,,"http://arc-ts.umich.edu/systems-and-services/flux/

Flux is the shared, Linux-based high-performance computing (HPC) cluster available to all researchers at the University of Michigan.

Flux consists of approximately 27,000 cores – including 1,372 compute nodes composed of multiple CPU cores, with at least 4 GB of RAM per core, interconnected with InfiniBand networking.","Flux - Cluster for CPU, GPU, and Large Memory workloads","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-17 20:40:13.934
88,,decommissioned,data-collections.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2009-01-01,,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,73,resource,,,SDSC Collections Disk Space,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
89,,production,stand-alone.tg.teragrid.org,tg.teragrid.org,production,2007-01-01,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,106,resource,,,Stand Alone,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2007-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
90,,decommissioned,albedo.psc.xsede.org,psc.xsede.org,decommissioned,2012-11-14,,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,84,resource,,,"TeraGrid Albedo, Wide Area File System","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2012-11-14', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2012-11-13', 'friendly_end_date': None, 'production_begin_date': '2011-02-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
91,,post-production,cowboy.okstate.xsede.org,okstate.xsede.org,post-production,,,"{'latitude': 36.121951, 'node_count': 265.0, 'manufacturer': '', 'rmax': None, 'interconnect': 'QDR infiniband', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': 0.0, 'is_accounted': False, 'sensitive_data_support_description': None, 'job_manager': '', 'batch_system': 'Torque/Maui', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '120', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': 'NVIDIA Tesla C2075 ', 'community_software_area_email': 'hpcc@okstate.edu', 'login_hostname': 'cowboy.hpc.okstate.edu', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 32.0, 'cpu_type': 'Intel Xeon E5-2620, E5-2670v3 ', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Cowboy', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': -97.071446, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': 'https://hpcc.okstate.edu', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.0, 'cpu_count_per_node': 12, 'operating_system': 'CentOS', 'community_software_area': True}",112.0,XSEDE,XSEDE Level 3,144570,compute,Diverse user and applications to meet the modest needs of campus and state researchers and educators.,"In 2012, OSUHPCC deployed the largest externally funded supercomputer in Oklahoma history through NSF MRI grant “Acquisition of a High Performance Compute Cluster for Multidisciplinary Research,” OCI-1126330, 9/1/11-8/31/14, $908,812, PI Brunson.  This cluster, Cowboy, consists of 254 compute nodes, each with dual Intel Xeon E5-2620 “Sandy Bridge” hex core 2.0 GHz CPUs, most with 32 GB of 1333 MHz RAM and two “fat nodes” with 256 GB RAM.  The aggregate peak speed is 48.8 TFLOPs, with 3048 cores, 8576 GB of RAM and approximately 92 TB of globally accessible disk. The cluster also includes two NVIDIA Tesla C2075 cards.  Cowboy also includes 92 TB of globally accessible high performance disk provided by three shelves of Panasas ActivStor12, this includes 20x 2TB drives and peak speed of 1500MB/s read and 1600MB/s write per shelf.  The total solution provides an aggregate of 4.5GB/s read and 4.8GB/s write.  The interconnect networks are Infiniband for message passing, Gigabit Ethernet for I/O, and an ethernet management network.   The Infiniband for message passing is Mellanox Connect-X 3 QDR in a 2:1 oversubscription.  There are a total of 15x MIS5025Q switches providing both the leaf and spine components.  Each leaf is connects to 24 compute nodes, and 12x 40Gb QDR links to the spine.  Point-to-point latency is approx 1 microsecond.  The ethernet network includes 11 leaf gigabit switches that connect to 24 compute nodes.  Each leaf is uplinked via 2x 10G network ports to the spine 64 port Mellanox MSX1016 10 Gigabit switch.  The network configuration provides a 1.2:1 oversubscription.  
In January 2015, via faculty funds, OSUHPCC deployed 11 additional compute nodes each with four Xeon Phi accelerator cards.  Each node as dual Intel Xeon E5-2670v3 twelve core 2.3 GHz CPUs with 128 GB 2133 MHz RAM and four Intel Xeon Phi 31S1 8 GB, 1.1 GHz, 57 core coprocessors.  These compute nodes are shared with all users.  Additional storage for the Fennel lab is was also acquired and is maintained by OSUHPCC.
",Oklahoma State cluster (Cowboy),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': '2012-10-15', 'retired_end_date': None, 'production_end_date': '2012-10-14', 'friendly_end_date': None, 'production_begin_date': '2012-09-02', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-09-13 09:36:21.470
92,,production,ecss.ecss.xsede.org,ecss.xsede.org,production,2011-06-01,,"{'latitude': None, 'user_guide_url': '', 'is_accounted': False, 'is_authenticated': False, 'allocations_info': {'unit_type': '[Yes = 1, No = 0]', 'allocable_type': 'OtherResource', 'allocable_resource_id': 530154, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'To request a longer term, focused collaboration with XSEDE staff, please answer all 5 required questions below.<br>Additional information is at <a href=""https://portal.xsede.org/ecss-justification"" target=new>https://portal.xsede.org/ecss-justification</a', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'To request a longer term, focused collaboration with XSEDE staff, please answer all 5 required questions below.<br>Additional information is at <a href=""https://portal.xsede.org/ecss-justification"" target=new>https://portal.xsede.org/ecss-justification</a', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'To request a longer term, focused collaboration with XSEDE staff, please answer all 5 required questions below.<br>Additional information is at <a href=""https://portal.xsede.org/ecss-justification"" target=new>https://portal.xsede.org/ecss-justification</a', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'To request a longer term, focused collaboration with XSEDE staff, please answer all 5 required questions below.<br>Additional information is at <a href=""https://portal.xsede.org/ecss-justification"" target=new>https://portal.xsede.org/ecss-justification</a', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'To request a longer term, focused collaboration with XSEDE staff, please answer all 5 required questions below.<br>Additional information is at <a href=""https://portal.xsede.org/ecss-justification"" target=new>https://portal.xsede.org/ecss-justification</a', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}], 'display_resource_name': 'XSEDE Extended Collaborative Support', 'allocations_description': 'To request a longer term, focused collaboration with XSEDE staff, please answer all 5 required questions below.<br>Additional information is at <a href=""https://portal.xsede.org/ecss-justification"" target=new>https://portal.xsede.org/ecss-justification</a>', 'allocable_id': 142312}, 'longitude': None, 'supports_sensitive_data': None, 'is_authorized': False, 'is_publicly_available': False, 'sensitive_data_support_description': None, 'xsedenet_participant': None}",96.0,XSEDE,XSEDE Level 1,142312,other,,XSEDE Extended Collaborative Support,XSEDE Extended Collaborative Support,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2011-06-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
93,,decommissioned,dtf.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2009-06-30,,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,18,resource,,"The TeraGrid cluster at SDSC comprises 262 IBM Itaninum2 nodes with two processors per node. Each node is built with SuSE Linux and interconnected with Myricom      's Myrinet. The system has a peak performance of 3.1 Teraflops, a total memory of 1 TB, and a total of 50 TB GPFS disk through the SAN network. Jobs are scheduled and run by the Catalina-scheduler and PBS-batch system.",SDSC DTF,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-06-30', 'friendly_begin_date': None, 'post_production_end_date': '2009-06-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2009-06-01', 'retired_end_date': None, 'production_end_date': '2009-05-30', 'friendly_end_date': None, 'production_begin_date': '2004-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
94,,production,bridges.pylon.psc.xsede.org,pylon.psc.xsede.org,production,2016-01-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'GigaBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530191, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Regular Memory (Bridges)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530189, 'allocable_id': 144546}, {'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Large Memory (Bridges Large)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530190, 'allocable_id': 144547}], 'default_amount': 500.0, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-09-01', 'required_resource_display_name': 'PSC Regular Memory (Bridges)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530189, 'allocable_id': 144546}, {'begin_date': '2015-09-01', 'required_resource_display_name': 'PSC Large Memory (Bridges Large)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530190, 'allocable_id': 144547}], 'default_amount': 500.0, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Regular Memory (Bridges)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530189, 'allocable_id': 144546}, {'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Large Memory (Bridges Large)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530190, 'allocable_id': 144547}], 'default_amount': 500.0, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2016-09-01', 'required_resource_display_name': 'PSC Regular Memory (Bridges)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530189, 'allocable_id': 144546}, {'begin_date': '2016-09-01', 'required_resource_display_name': 'PSC Large Memory (Bridges Large)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530190, 'allocable_id': 144547}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Regular Memory (Bridges)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530189, 'allocable_id': 144546}, {'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Large Memory (Bridges Large)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530190, 'allocable_id': 144547}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}], 'display_resource_name': 'PSC Storage (Bridges Pylon)', 'allocations_description': None, 'allocable_id': 144548}, 'is_accounted': True, 'user_guide_url': 'https://portal.xsede.org/psc-bridges#file-spaces', 'databases': '', 'supports_sensitive_data': None, 'file_space_tb': 10000.0, 'xsedenet_participant': None}",95.0,XSEDE,XSEDE Level 1,144548,storage,The Pylon file system is a persistent file system. You should use it for long-term storage of your files and not for working space for running jobs,,PSC Storage (Bridges Pylon),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2016-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-11-08 22:01:34.211
95,,decommissioned,cobalt.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2010-03-31,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,5,resource,,The NCSA SGI Altix consists of several Intel Itanium 2 processor shared-memory systems. The Altix  uses the CXFS shared parallel filesystem from SGI.,NCSA SGI Altix (Cobalt),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-02-28', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
96,HSI (for more information see HPSS),decommissioned,hpss.ncar.teragrid.org,ncar.teragrid.org,decommissioned,2011-07-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': 'HPSS', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'user_guide_url': None, 'is_accounted': True, 'xsedenet_participant': None, 'databases': '', 'file_space_tb': 1000.0, 'supports_sensitive_data': None}",72.0,XSEDE,XSEDE Level 1,140382,storage,"This archival system features a maximum file size of 1 TB, initial per-user quota of 5 TB with opportunity for requesting increased allocations, 
ability to write multiple copies",,NCAR HPSS Storage System,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-06-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
97,"GPFS-WAN is currently mounted on the following machines:
            IU : tg-login.iu.teragrid.org (Big Red PPC Linux Cluster)",decommissioned,gpfs-wan.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2009-01-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'user_guide_url': None, 'is_accounted': True, 'xsedenet_participant': None, 'databases': 'Not applicable', 'file_space_tb': 700.0, 'supports_sensitive_data': None}",81.0,XSEDE,XSEDE Level 1,140562,storage,"TeraGrid GPFS-WAN (Global Parallel File System-Wide Area Network) is a large-scale storage system mounted on several TeraGrid platforms. Although the system is physically located at SDSC, it looks lik",,GPFS-WAN,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
98,,production,bridges.large.psc.xsede.org,large.psc.xsede.org,production,2016-01-01,,"{'latitude': None, 'node_count': 10.0, 'manufacturer': 'HPE', 'rmax': None, 'interconnect': 'Omni-Path', 'is_publicly_available': True, 'peak_teraflops': 894.6, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530190, 'conversion_factors': [{'begin_date': '2016-04-29', 'factor': 64.7325, 'conversion': 'Teragrid', 'end_date': '2020-01-01'}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Storage (Bridges Pylon)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530191, 'allocable_id': 144548}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.678}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Storage (Bridges Pylon)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530191, 'allocable_id': 144548}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.678}, {'relative_order': None, 'required_resources': [{'begin_date': '2016-09-02', 'required_resource_display_name': 'PSC Storage (Bridges Pylon)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530191, 'allocable_id': 144548}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.678}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-09-01', 'required_resource_display_name': 'PSC Storage (Bridges Pylon)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530191, 'allocable_id': 144548}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.678}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-10-19', 'required_resource_display_name': 'PSC Storage (Bridges Pylon)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530191, 'allocable_id': 144548}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': 1000.0, 'allocation_type': 'Startup', 'dollar_value': 0.678}], 'display_resource_name': 'PSC Large Memory (Bridges Large)', 'allocations_description': '1 Service Unit = 1 TB Memory Hours', 'allocable_id': 144547}, 'job_manager': 'SLURM', 'batch_system': 'SLURM', 'platform_name': 'HPE Integrity Superdome X servers (4, each with 12TB), HPE ProLiant DL580 servers (42, each with 3TB)', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': False, 'model': 'HPE Integrity Superdome X servers (18 cores and 12TB per node), HPE ProLiant DL580 servers (16 cores and  3TB per node)', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'bridges.psc.edu', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 128.0, 'cpu_type': 'Intel Xeon EX-series', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Bridges Large Memory', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': True, 'ip_address': '', 'parallel_file_system': 'Pylon', 'user_guide_url': 'https://portal.xsede.org/psc-bridges', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.3, 'cpu_count_per_node': 16, 'operating_system': 'CentOS', 'community_software_area': False}",51.0,XSEDE,XSEDE Level 1,144547,compute,"Extreme Shared Memory (ESM) nodes will serve the most memory intensive applications, e.g., de novo and metagenome sequence assembly, graph analytics, large in-memory databases, machine learning applications, and large-memory applications written in threaded languages.   Large Shared Memory nodes will provide capacity for applications similar to ESM nodes but needing only up to 3TB (e.g., in statistics, bioinformatics, causal analysis, machine learning, graph analytics), large-memory back-ends to support gateways and interactive analytics and visualization.","Two node types comprise Bridges' large memory nodes:  Extreme Shared Memory (ESM) and Large Shared Memory nodes.  ESMs are HP Integrity Superdome X servers.  Each will have 16 Intel Xeon EX-series CPUs and 12TB of hardware-enabled, cache-coherent memory.  LSM  nodes are HP DL580 servers with 4 Intel Xeon EX-series CPUs and 3TB each of hardware-enabled, cache-coherent shared memory.  ",PSC Large Memory Nodes (Bridges Large),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2016-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2015-09-15', 'pre_production_end_date': '2015-12-31'}",2016-09-12 16:45:43.649
99,,decommissioned,bluegene.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2008-10-01,,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,3,resource,,"The IBM BlueGene system at SDSC is housed in three racks with 3072 compute nodes and 384 I/O nodes. This maximum ratio of I/O to compute nodes was chosen to support data-intensive computing. Each node consists of two PowerPC processors that run at 700 MHz and share 512 MB of memory, giving an aggregate peak speed of 17.2 teraflops and a total memory of 1.5 TB. Blue Gene has achieved 13.83 teraflops on the widely quoted Linpack benchmark.

Decommissioned 10/1/2008",SDSC IBM Blue Gene,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-10-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
100,,decommissioned,gpfs-wan.tg.teragrid.org,tg.teragrid.org,decommissioned,2011-07-31,,"{'latitude': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'specifications': '', 'is_publicly_available': False, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'TeraBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530107, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-TG', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}], 'display_resource_name': 'GPFS-WAN Disk Space', 'allocations_description': None, 'allocable_id': 142314}, 'is_accounted': False, 'user_guide_url': '', 'databases': '', 'supports_sensitive_data': None, 'file_space_tb': None, 'xsedenet_participant': None}",82.0,XSEDE,XSEDE Level 1,142314,storage,GPFS WAN,,TeraGrid GPFS-WAN Disk Space,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
101,,decommissioned,bluegene.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2008-10-01,,"{'latitude': None, 'node_count': 3072.0, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': '3-D torus for point-to-point message passing', 'is_publicly_available': True, 'peak_teraflops': 17.2, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530103, 'conversion_factors': [{'begin_date': '2006-03-31', 'factor': 0.558, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-SDSC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}], 'display_resource_name': 'SDSC IBM Blue Gene', 'allocations_description': None, 'allocable_id': 125574}, 'job_manager': None, 'batch_system': 'LoadLeveler', 'platform_name': 'BlueGene/L', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': None, 'rpeak': None, 'machine_type': 'MPP', 'gpu_description': None, 'community_software_area_email': None, 'login_hostname': 'bglogin.sdsc.edu', 'storage_network': 'Fibre Channel', 'graphics_card': None, 'is_visualization': False, 'memory_per_cpu_gb': 0.256, 'cpu_type': 'IBM BlueGene/L', 'alternate_login_hostname': None, 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'BlueGene', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': None, 'parallel_file_system': '225 TB GPFS-WAN (/gpfs-wan)\n18 TB GPFS (/bggpfs)', 'user_guide_url': 'http://www.sdsc.edu/us/resources/bluegene/', 'disk_size_tb': 19.5, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 0.7000000000000001, 'cpu_count_per_node': 2, 'operating_system': 'SuSE Linux 9 [Frontend] BLRTS [Compute]', 'community_software_area': False}",3.0,XSEDE,XSEDE Level 1,125574,compute,"[DECOMM]Blue Gene is primarily intended to run codes that scale very well, have only fine-grained parallelism, and do not use much memory per node.
<!-- please use this space for Recommended Use and Status -->
<strong>Status</strong>
In production but no longer accepting allocation requests","The IBM BlueGene system at SDSC is housed in three racks with 3072 compute nodes and 384 I/O nodes. This maximum ratio of I/O to compute nodes was chosen to support data-intensive computing. Each node consists of two PowerPC processors that run at 700 MHz and share 512 MB of memory, giving an aggregate peak speed of 17.2 teraflops and a total memory of 1.5 TB. Blue Gene has achieved 13.83 teraflops on the widely quoted Linpack benchmark.

Decommissioned 10/1/2008",SDSC IBM Blue Gene,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-10-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
102,,,hpc.ksu.xsede.org,ksu.xsede.org,,,,"{'organizations': [{'organization_abbreviation': 'Kansas State U', 'organization_name': 'Kansas State University', 'organization_code': '0019281'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 3,124,resource,,This is the campus HPC provider for Kansas State University.,KSU HPC Cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-01-19 21:46:50.613
103,,decommissioned,tiger.iu.teragrid.org,iu.teragrid.org,decommissioned,2008-01-01,,"{'latitude': None, 'node_count': 8.0, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': 'Myrinet', 'is_publicly_available': True, 'peak_teraflops': 0.17, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530091, 'conversion_factors': [{'begin_date': '2004-10-01', 'factor': 1.005, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'AAB', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'NRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}], 'display_resource_name': 'TeraGrid Indiana University (AVIDD-I64 IA-64 Cluster)', 'allocations_description': None, 'allocable_id': 125750}, 'job_manager': 'jobmanager-fork (default)\n\njobmanager-pbs', 'batch_system': None, 'platform_name': 'AVIDD IA-64', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '30 GB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': None, 'login_hostname': 'tg-login1.iu.teragrid.org', 'storage_network': 'Fibre Channel', 'graphics_card': None, 'is_visualization': False, 'memory_per_cpu_gb': 6.0, 'cpu_type': 'Intel Itanium2 - Madison', 'alternate_login_hostname': None, 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Tiger', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': None, 'parallel_file_system': '1 TB', 'user_guide_url': 'http://racinfo.indiana.edu/rats/research/avidd-t/index.shtml', 'disk_size_tb': 1.09, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.3, 'cpu_count_per_node': 4, 'operating_system': 'SuSE Enterprise Server 8', 'community_software_area': False}",10.0,XSEDE,XSEDE Level 1,125750,compute,[DECOMM]The IU AVIDD IA-64 cluster is intended to run reasonably small parallel applications.,"The IU AVIDD IA-64 cluster has 8 compute nodes, each with four 1.3 GHz Intel Itanium2 CPUs, 6 GB of memory, 31 GB local scratch space, a Myrinet-2000 interconnect and access to 0.8 TB of cluster GPFS scratch space. NOTE: 100% of this resource is available for TeraGrid usage.

<strong class=       ""attention       "">IMPORTANT: </strong>There will be no more future allocations on this resource.",Indiana University (AVIDD-I64 IA-64 Cluster),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
104,,decommissioned,radon.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2006-06-30,,"{'latitude': None, 'node_count': None, 'manufacturer': '', 'rmax': None, 'interconnect': '', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': False, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530089, 'conversion_factors': [{'begin_date': None, 'factor': 0.0, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'AAB', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'NRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}], 'display_resource_name': 'TeraGrid Purdue IA-32 Linux Cluster', 'allocations_description': None, 'allocable_id': 142317}, 'job_manager': '', 'batch_system': '', 'platform_name': '', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': None, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Radon', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '', 'user_guide_url': '', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': None, 'cpu_count_per_node': None, 'operating_system': '', 'community_software_area': False}",54.0,XSEDE,XSEDE Level 1,142317,compute,,Radon,Purdue IA-32 Linux Cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2006-06-30', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
105,,decommissioned,gpfs-wan.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2011-10-01,,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,80,resource,,,GPFS-WAN,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-10-01', 'friendly_begin_date': None, 'post_production_end_date': '2011-09-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2011-07-01', 'retired_end_date': None, 'production_end_date': '2011-06-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
106,,decommissioned,mercury.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2010-03-31,,"{'latitude': None, 'node_count': 631.0, 'manufacturer': 'Intel', 'rmax': None, 'interconnect': 'Myrinet', 'is_publicly_available': True, 'peak_teraflops': 10.23, 'advance_max_reservable_su': 1774.0, 'is_accounted': True, 'sensitive_data_support_description': None, 'job_manager': 'jobmanager (default),  jobmanager-pbs', 'batch_system': '', 'platform_name': 'IA-64 Cluster', 'nfs_network': 'Gigabit Ethernet', 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '1 TB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'mikep@ncsa.uiuc.edu', 'login_hostname': 'grid-hg.ncsa.teragrid.org', 'storage_network': 'Fibre Channel', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Intel Itanium2 - Madison', 'alternate_login_hostname': 'grid-hg.ncsa.teragrid.org', 'xsedenet_participant': None, 'max_reservable_su': 1774.0, 'nickname': 'Mercury', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'GPFS: 170 TB', 'user_guide_url': 'http://www.ncsa.uiuc.edu/UserInfo/Resources/Hardware/TGIA64LinuxCluster/', 'disk_size_tb': 60.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.5, 'cpu_count_per_node': 2, 'operating_system': 'Linux 2.4.21-SMP', 'community_software_area': True}",24.0,XSEDE,XSEDE Level 1,142230,compute,"The NCSA IA-64 Linux Cluster (mercury) is primarily intended to run applications of moderate to high levels of parallelism, particularly those needing a 64-bit environment and codes that perform well in a distributed cluster environment. 
<br />
Note: mercury is co-allocated with the SDSC and ANL IA-64 Clusters.","NCSA's IA-64 TeraGrid Linux Cluster consists of 887 IBM nodes: 256 nodes with dual 1.3 GHz Intel Itanium 2 processors (half with 4 GB of memory per node, and the other half with 12 GB of memory per node), and 631 nodes with dual 1.5 GHz Intel Itanium 2 processors (4 GB of memory per node). The cluster is running SuSE Linux and is using Myricom's Myrinet cluster interconnect network, and the GPFS parallel filesystem.",Mercury,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
107,,decommissioned,forge.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2012-09-28,,"{'latitude': None, 'node_count': 36.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'QDR IB', 'is_publicly_available': True, 'peak_teraflops': 150.0, 'advance_max_reservable_su': None, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530153, 'conversion_factors': [{'begin_date': None, 'factor': 21.279, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 50000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'NCSA Dell with NVIDIA Fermi GPU Cluster (Forge)', 'allocations_description': None, 'allocable_id': 142279}, 'job_manager': 'Moab Scheduler', 'batch_system': 'PBS Torque', 'platform_name': 'PowerEdge C6145 with NVIDIA Fermi M2070', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': 'GPFS', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'login-forge.ncsa.xsede.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 4.0, 'cpu_type': 'AMD Magny-Cours 6136', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Forge', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'GPFS', 'user_guide_url': 'https://www.xsede.org/web/guest/ncsa-forge', 'disk_size_tb': 600.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.4, 'cpu_count_per_node': 16, 'operating_system': 'Linux', 'community_software_area': False}",34.0,XSEDE,XSEDE Level 1,142279,compute,Forge is intended for applications that make use of GPGPUs.,Forge consists of 36 Dell PowerEdge C6145 quad-socket nodes with dual 8-core AMD Magny-Cours 6136 processors and 64 GB of memory. Each node supports 6 NVIDIA Fermi M2070 GPUs.,NCSA Dell with NVIDIA Fermi GPU Cluster (Forge),"{'coming_soon_begin_date': '2011-06-01', 'retired_begin_date': None, 'decommissioned_begin_date': '2012-09-28', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2011-09-18', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2012-09-27', 'friendly_end_date': None, 'production_begin_date': '2011-10-03', 'decommissioned_end_date': None, 'pre_production_begin_date': '2011-09-19', 'pre_production_end_date': '2011-10-02'}",
108,,decommissioned,maverick.tacc.teragrid.org,tacc.teragrid.org,decommissioned,2008-12-01,,"{'latitude': None, 'node_count': 1.0, 'manufacturer': 'Sun', 'rmax': None, 'interconnect': 'myrinet', 'is_publicly_available': True, 'peak_teraflops': None, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530087, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'AAB', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'NRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}], 'display_resource_name': 'TACC Sun Terascale Visualization System (Maverick)', 'allocations_description': None, 'allocable_id': 125948}, 'job_manager': 'SGE', 'batch_system': 'SGE', 'platform_name': 'Sun Fire E25K ', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'login.maverick.tacc.teragrid.org', 'storage_network': '', 'graphics_card': '8 Sun Jasmine graphics adapters', 'is_visualization': True, 'memory_per_cpu_gb': 256.0, 'cpu_type': 'UltraSPARC IV', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Maverick', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': '', 'user_guide_url': 'http://www.tacc.utexas.edu/services/userguides/maverick/', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.05, 'cpu_count_per_node': 4, 'operating_system': 'Solaris', 'community_software_area': False}",58.0,XSEDE,XSEDE Level 1,125948,compute,"The TACC Visualization System Maverick is intended for serial and parallel, visualization applications that take advantage of a large shared memory and multiple compute processors (up to 128) and graphics processors (32).  The TACC developed, web-based EnVision software tool enables user to remotely execute serial and parallel visualization applications and easily manipulate terascale data sets.  During prime-time hours Maverick is dedicated to interactive use with a batch queue available for computational use during non prime-time hours."," The TACC Visualization System Maverick is a Ssun Fire E25K intended for serial and parallel, visualization applications that take advantage of a large shared memory and multiple  (up to 128) UltraSPARC IV compute processors and (8) Sun Jasmine graphics processors  ",TACC Sun Large-Scale Visualization (Maverick),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-12-01', 'friendly_begin_date': None, 'post_production_end_date': '2008-11-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2008-01-01', 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
109,,decommissioned,steele.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2013-07-31,,"{'organizations': [{'organization_abbreviation': 'Purdue U', 'organization_name': 'Purdue University', 'organization_code': '0018259'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,12,resource,,"The Steele cluster consists of 893 dual quad-core Dell 1950 compute nodes, running Red Hat Enterprise Linux, version 4. Each node thus has 8 64-bit Intel 2.33 GHz E5410 CPUs and either 16 GB or 32 GB of RAM. They are interconnected with either Gigabit Ethernet or InfiniBand. The machine offers access to the RCAC scratch space. Steele's peak performance is rated at 66.59 TFLOPS.",Purdue Dell PowerEdge Linux Cluster (Steele),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2013-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
110,,production,wrangler.tacc.xsede.org,tacc.xsede.org,production,2015-02-28,2019-02-28,"{'latitude': None, 'node_count': 96.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': ' FDR IB and 40GigE ', 'is_publicly_available': False, 'peak_teraflops': 62.0, 'advance_max_reservable_su': 1.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Node Hours', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530174, 'conversion_factors': [{'begin_date': '2016-07-26', 'factor': 7.0, 'conversion': 'Teragrid', 'end_date': '2020-07-26'}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2016-09-01', 'required_resource_display_name': 'TACC Long-term Storage (Wrangler Storage)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530175, 'allocable_id': 142690}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.06876755200000001}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-02-11', 'required_resource_display_name': 'TACC Long-term Storage (Wrangler Storage)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530175, 'allocable_id': 142690}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': 1000.0, 'allocation_type': 'Startup', 'dollar_value': 0.06876755200000001}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-09-14', 'required_resource_display_name': 'TACC Long-term Storage (Wrangler Storage)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530175, 'allocable_id': 142690}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.06876755200000001}], 'display_resource_name': 'TACC Data Analytics System (Wrangler)', 'allocations_description': '', 'allocable_id': 142689}, 'job_manager': '', 'batch_system': 'SLURM', 'platform_name': 'PowerEdge Haswell Linux Cluster', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': True, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'wrangler.tacc.xsede.org', 'storage_network': '', 'graphics_card': 'No', 'is_visualization': False, 'memory_per_cpu_gb': 5.33, 'cpu_type': 'Intel(R) Xeon(R) CPU E5-2680 v3', 'alternate_login_hostname': 'login1.wrangler.tacc.utexas.edu', 'xsedenet_participant': None, 'max_reservable_su': 1.0, 'nickname': 'Wrangler', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'http://portal.xsede.org/tacc-wrangler', 'disk_size_tb': 5000.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.5, 'cpu_count_per_node': 24, 'operating_system': 'Linux (CentOS)', 'community_software_area': False}",67.0,XSEDE,XSEDE Level 1,142689,compute,"For research projects previously limited by I/O capabilities or scale of services for data analysis and sharing on local resources.


","The Wrangler Data Analytics system is designed to satisfy the needs for many in Data Computing who find their I/O patterns are not well suited for classic HPC systems.  Wrangler features 0.5PB of usable flash-based storage accessible directly via the PCI bus to all 96 compute nodes at TACC.  Unlike SSD solutions, this configuration gives all of the compute nodes direct PCI level access to all of the storage providing I/O rates of 1 TB/s and 250 million IOPS.  Wrangler features 10PB of replicated storage for both input and result data hosted at TACC and at Indiana University.  ",TACC Data Analytics System (Wrangler),"{'coming_soon_begin_date': '2014-09-12', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2014-09-14', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-02-28', 'friendly_end_date': None, 'production_begin_date': '2015-02-28', 'decommissioned_end_date': None, 'pre_production_begin_date': '2014-09-15', 'pre_production_end_date': '2015-01-30'}",2016-04-18 18:02:21.639
111,,production,stand-alone.tg.teragrid.org,tg.teragrid.org,production,2007-01-01,,"{'is_accounted': False, 'is_authenticated': False, 'is_authorized': False, 'is_publicly_available': False, 'user_guide_url': '', 'resources_in_grid': []}",106.0,XSEDE,XSEDE Level 1,142322,grid,,Stand Alone,Stand Alone,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2007-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
112,,decommissioned,blacklight.psc.teragrid.org,psc.teragrid.org,decommissioned,2015-08-16,,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,60,resource,,"Blacklight is an SGI UV 1000cc-NUMA shared-memory system comprising 256 blades.
Each blade holds 2 Intel Xeon X7560 (Nehalem) eight-core processors, for a total of 4096 cores
across the whole machine. Each core has a clock rate of 2.27 GHz, supports two hardware threads
and can perform 9 Gflops. The sixteen cores on each blade share 128 Gbytes of local memory.
Users can run shared memory jobs of up to 16 Tbytes.

Blacklight jobs have access to scratch storage. Blacklight users should also apply for between 100 GB
and 40 TB of persistent storage, as needed,  on the Data SuperCell (DSC). They may also request storage on the XSEDE-wide file system (XWFS). 
",PSC SGI Altix UV (Blacklight),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2015-08-16', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2015-08-15', 'friendly_end_date': None, 'production_begin_date': '2011-01-17', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-07-03 02:22:03.844
113,,decommissioned,wispy.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2013-07-31,,"{'latitude': None, 'node_count': 32.0, 'manufacturer': 'HP', 'rmax': None, 'interconnect': '1 GigE', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': False, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530147, 'conversion_factors': [{'begin_date': None, 'factor': 0.0, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 10.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'Purdue Cloud (Wispy)', 'allocations_description': None, 'allocable_id': 142286}, 'job_manager': '', 'batch_system': '', 'platform_name': 'DL140g3', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 16.0, 'cpu_type': 'Intel Xeon 5140', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Wispy', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '', 'user_guide_url': '', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.33, 'cpu_count_per_node': 4, 'operating_system': 'RHEL 5', 'community_software_area': False}",44.0,XSEDE,XSEDE Level 1,142286,compute,"Wispy provides a generic cloud computing environment that allows for a wide variety of software to gain access to remote resources. Users master a complete computer inside of a file and then have the ability to spawn that computer many times over to complete their research.
","Wispy is an experimental Nibus Compute Cloud. It consists of 32 64bit, 4-core HP DL140 nodes which are all connected using 1 Gigabit Ethernet (1GigE).

",Purdue Cloud (Wispy),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2013-07-30', 'friendly_end_date': None, 'production_begin_date': '2010-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
114,,production,jetstream-storage.tacc.xsede.org,tacc.xsede.org,production,2016-01-01,,"{'organizations': [{'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}, {'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}, {'organization_abbreviation': 'UT Austin', 'organization_name': 'University of Texas at Austin', 'organization_code': '0036582'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,94,resource,,Storage for use with Jetstream Computing,IU/TACC Storage (Jetstream Storage),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2016-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-09-01 13:20:44.001
115,,production,Red.unl.xsede.org,unl.xsede.org,production,2005-08-01,2017-12-31,"{'organizations': [{'organization_abbreviation': 'U Nebraska, Lincoln', 'organization_name': 'University of Nebraska, Lincoln', 'organization_code': '0025650'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 3,114,resource,,"Red is a U.S. CMS Tier2 resource located at the University of Nebraska. Resources are provided via the Open Science Grid (OSG) primarily to the CMS project but also opportunistically to the broader OSG community.  As a grid resource, it has been added to and upgraded annually since 2005. ",University of Nebraska U.S. CMS Tier2 grid resource ,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2017-12-31', 'friendly_end_date': None, 'production_begin_date': '2005-08-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-08-08 17:53:07.278
116,,production,spruceknob.wvu.xsede.org,wvu.xsede.org,production,2013-11-15,2021-08-01,"{'organizations': [{'organization_abbreviation': 'West Virginia U', 'organization_name': 'West Virginia University', 'organization_code': '0038273'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 3,115,resource,,"Spruce Knob is a HPC Cluster that has compute cycles freely available for anyone in higher education throughout the state of West Virginia.  The system is refreshed annually.

WVU's High Performance Computing facilities (including Spruce Knob) are funded by the National Science Foundation EPSCoR Research Infrastructure Improvement Cooperative Agreement #1003907, the state of West Virginia (WVEPSCoR via the Higher Education Policy Commission), the WVU Research Corporation and faculty investments.",WVU Spruce Knob HPC Cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2021-08-01', 'friendly_end_date': None, 'production_begin_date': '2013-11-15', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-08-14 18:12:58.439
117,,decommissioned,queenbee.loni-lsu.teragrid.org,loni-lsu.teragrid.org,decommissioned,2011-07-31,,"{'latitude': None, 'node_count': 668.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'InfiniBand', 'is_publicly_available': True, 'peak_teraflops': 50.0, 'advance_max_reservable_su': 1024.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530123, 'conversion_factors': [{'begin_date': '2008-10-10', 'factor': 1.6059999999999999, 'conversion': 'Teragrid', 'end_date': None}, {'begin_date': '2008-10-10', 'factor': 2.0, 'conversion': 'Abe', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}], 'display_resource_name': 'LONI Dell Intel 64 Linux Cluster (QueenBee - Available February 1, 2008)', 'allocations_description': None, 'allocable_id': 126324}, 'job_manager': 'jobmanager-pbs', 'batch_system': 'Torque/Moab', 'platform_name': 'Intel 64 Linux Cluster', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': 'Lustre', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'tg-support@loni.org', 'login_hostname': 'login1-qb.loni-lsu.teragrid.org', 'storage_network': 'InfiniBand and Gigabit Ethernet', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 1.0, 'cpu_type': 'Intel 64', 'alternate_login_hostname': 'qb1.loni.org', 'xsedenet_participant': None, 'max_reservable_su': 1024.0, 'nickname': 'Queen Bee', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '208.100.92.21', 'parallel_file_system': 'Lustre', 'user_guide_url': 'http://www.loni.org/teragrid/', 'disk_size_tb': 192.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.0, 'cpu_count_per_node': 8, 'operating_system': 'Red Hat Enterprise Linux 4 (Linux 2.6.9-55)', 'community_software_area': True}",17.0,XSEDE,XSEDE Level 1,126324,compute,"Queen Bee is primarily intended for parallel applications scalable up to 5344 processing cores. 

Note: Queen Bee is in production as of February 1, 2008. Starting with April 1 2008 allocations, LONI Queen Bee system is co-allocated with the NCSA Abe system.",Queen Bee is a 50.7 TFlops Peak Performance 668 node Dell PowerEdge 1950 cluster running the Red Hat Enterprise Linux 4 operating system. Each node contains two Quad Core Intel Xeon 2.33GHz 64-bit processors and 8 GB of memory. The cluster is interconnected with 10 Gb/sec Infniband and has total 192TB (raw) of storage in shared Lustre filesystems.,queen-bee pops name,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-02-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
118,,decommissioned,radon.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2006-06-30,,"{'organizations': [{'organization_abbreviation': 'Purdue U', 'organization_name': 'Purdue University', 'organization_code': '0018259'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,54,resource,,Radon,Purdue IA-32 Linux Cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2006-06-30', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
119,,decommissioned,brutus.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2010-03-31,,"{'latitude': None, 'node_count': 1.0, 'manufacturer': 'SGI', 'rmax': None, 'interconnect': 'NUMALink', 'is_publicly_available': True, 'peak_teraflops': 0.042, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530129, 'conversion_factors': [{'begin_date': '2008-04-01', 'factor': 1.0, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 10000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}], 'display_resource_name': 'Purdue FPGA Prototyping Environment (Brutus)', 'allocations_description': None, 'allocable_id': 126560}, 'job_manager': None, 'batch_system': None, 'platform_name': '450', 'nfs_network': None, 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': None, 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': None, 'login_hostname': 'brutus.rcac.purdue.edu', 'storage_network': None, 'graphics_card': None, 'is_visualization': False, 'memory_per_cpu_gb': 3.75, 'cpu_type': 'FPGA (Xilinx Virtrex SGI RC100)', 'alternate_login_hostname': None, 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Brutus', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': None, 'parallel_file_system': None, 'user_guide_url': None, 'disk_size_tb': 22.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 0.2, 'cpu_count_per_node': 4, 'operating_system': 'SUSE Linux', 'community_software_area': False}",14.0,XSEDE,XSEDE Level 1,126560,compute,This resource should <strong>only</strong> be used for FPGA accelerated applications.,"These resources consist of an SGI 450 (brutus.rcac.purdue.edu) with two RC100 FPGA blades, totaling 4 available FPGAs. Also available is a Sun Fire X2200 M2 (portia.rcac.purdue.edu) which serves both as a place & route node for preparing FPGA code for use on Brutus and as an entry point for GSI-SSH and job submission to Brutus by TeraGrid users. <br> 

<b>NOTE:</b> All references to CPU below should be interpreted as referring to be FPGAs.",Purdue FPGA Prototyping Environment (Brutus),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-02-28', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
120,,,hpc.tufts.xsede.org,tufts.xsede.org,,,,"{'latitude': None, 'node_count': None, 'manufacturer': 'Cisco', 'rmax': None, 'interconnect': '', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': 0.0, 'is_accounted': False, 'sensitive_data_support_description': None, 'job_manager': 'Slurm', 'batch_system': 'Slurm', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'login.cluser.tufts.edu', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': None, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'login.tufts.xsede.org', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': '', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': None, 'cpu_count_per_node': None, 'operating_system': 'Redhat', 'community_software_area': False}",122.0,XSEDE,XSEDE Level 3,144578,compute,Main login node for compute cluster.,Main login node for compute cluster.,Tufts University High Performance Compute (HPC) Cluster Login Node,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-11-10 19:56:34.891
121,Open for use by any academic in Kansas and their partners.,,hpc.ksu.xsede.org,ksu.xsede.org,,,,"{'latitude': 39.1836, 'node_count': 230.0, 'manufacturer': 'varies', 'rmax': None, 'interconnect': 'QDR Infiniband and 100GbE', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': 0.0, 'is_accounted': False, 'sensitive_data_support_description': None, 'job_manager': '', 'batch_system': 'SGE', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': True, 'model': '', 'primary_storage_shared_gb': '2300', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'beocat.cs.ksu.edu', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 64.0, 'cpu_type': 'Intel Xeon', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'beocat', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': 96.5717, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': 'https://support.beocat.ksu.edu/', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.1, 'cpu_count_per_node': 16, 'operating_system': 'Gentoo Linux', 'community_software_area': False}",124.0,XSEDE,XSEDE Level 3,144582,compute,,HPC Compute cluster for Kansas State University,KSU Beocat Computer Cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-01-19 21:53:14.442
122,,decommissioned,condor.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2013-07-31,,"{'latitude': None, 'node_count': 1750.0, 'manufacturer': '', 'rmax': None, 'interconnect': 'Gigabit Ethernet', 'is_publicly_available': True, 'peak_teraflops': 60.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530098, 'conversion_factors': [{'begin_date': '2005-10-01', 'factor': 0.849, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 200000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'Purdue Condor Pool', 'allocations_description': None, 'allocable_id': 125778}, 'job_manager': 'jobmanager-condor', 'batch_system': 'Condor', 'platform_name': 'Condor Pool', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'tg-condor.purdue.teragrid.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Intel Pentium III, Pentium 4, Xeon, Opteron', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Condor Pool', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '', 'user_guide_url': 'https://www.xsede.org/web/guest/purdue-condor', 'disk_size_tb': 170.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.3, 'cpu_count_per_node': 8, 'operating_system': 'Linux Debian Etch, RHEL4', 'community_software_area': False}",37.0,XSEDE,XSEDE Level 1,125778,compute,"Condor is designed for high-throughput computing, and is excellent for parameter sweeps, Monte Carlo simulation, or most serial application.","The Purdue Condor pools consist of over 14000 CPUs of computation: 8000 LINUX/X86_64 CPUs, 400 LINUX/INTEL (ia32) CPUs, and 5000 WINNT51/INTEL CPUs, as well as a small number of Itanium Linux, Solaris and MacOS X machines. Memory on compute nodes range from 512 MB to 32 GB, and most CPUs run at 3 GHz or better. With a total of over 60 TFLOPS available, the Purdue Condor pools can provide large numbers of cycles in a short amount of time. All shared areas and software packages available on Steele are available on Condor.",Purdue Condor Pool,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2013-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
123,,production,grid1.osg.xsede.org,osg.xsede.org,production,2012-04-01,2017-05-30,"{'latitude': None, 'node_count': 60000.0, 'manufacturer': 'Various', 'rmax': None, 'interconnect': 'None', 'is_publicly_available': True, 'peak_teraflops': 50.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530157, 'conversion_factors': [{'begin_date': '2012-06-04', 'factor': 1.0, 'conversion': 'Teragrid', 'end_date': '2015-02-11'}, {'begin_date': '2015-02-12', 'factor': 3.147, 'conversion': 'Teragrid', 'end_date': '2020-12-31'}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': 200000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': None, 'allocation_type': 'Staff', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}], 'display_resource_name': 'Open Science Grid (OSG)', 'allocations_description': None, 'allocable_id': 142288}, 'job_manager': 'condor', 'batch_system': 'Condor', 'platform_name': 'Linux', 'nfs_network': 'None', 'sensitive_data_support_description': None, 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': 'No shared storage', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'submit-1.osg.xsede.org', 'storage_network': 'None', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'x86_64', 'alternate_login_hostname': 'xd-login.opensciencegrid.org', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Open Science Grid', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': True, 'ip_address': '129.79.53.198', 'parallel_file_system': 'No parallel file system', 'user_guide_url': 'https://portal.xsede.org/OSG-User-Guide', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.4, 'cpu_count_per_node': 1, 'operating_system': 'CentOS', 'community_software_area': False}",48.0,XSEDE,XSEDE Level 1,142288,compute,"High throughput jobs using a single core, or a small number of threads which can fit on a single compute node.",A virtual HTCondor pool made up of resources from the Open Science Grid,Open Science Grid (OSG),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2017-05-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2017-05-30', 'friendly_end_date': None, 'production_begin_date': '2012-04-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2012-03-01', 'pre_production_end_date': '2012-03-31'}",2016-06-06 17:02:11.172
124,,decommissioned,abe-queenbee-steele.grid.teragrid.org,grid.teragrid.org,decommissioned,2010-01-01,,"{'is_accounted': False, 'is_authenticated': False, 'user_guide_url': '', 'is_authorized': False, 'is_publicly_available': False, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'GridResource', 'allocable_resource_id': 530141, 'conversion_factors': [], 'allocation_type_specifics': [], 'display_resource_name': 'NCSA/LONI/Purdue Dell PowerEdge Linux Clusters (Abe/Queen Bee/Steele)', 'allocations_description': None, 'allocable_id': 142304}, 'resources_in_grid': []}",105.0,XSEDE,XSEDE Level 1,142304,grid,,NCSA/LONI/Purdue Dell PowerEdge Linux Clusters (Abe/Queen Bee/Steele),NCSA/LONI/Purdue Dell PowerEdge Linux Clusters (Abe/Queen Bee/Steele),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
125,,,futuregrid0.futuregrid.xsede.org,futuregrid.xsede.org,,,,"{'organizations': [{'organization_abbreviation': 'Purdue U', 'organization_name': 'Purdue University', 'organization_code': '0018259'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,97,resource,,,FutureGrid Testbed (FutureGrid),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2014-09-30', 'friendly_end_date': None, 'production_begin_date': '2013-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-07-27 21:41:46.318
126,,decommissioned,lonestar.tacc.teragrid.org,tacc.teragrid.org,decommissioned,2010-12-13,,"{'organizations': [{'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,36,resource,,"The Lonestar Dell PowerEdge Linux Cluster is configured with 5,840 compute-node cores, 11.6 TB of total memory and 106TB of local disk space. The peak performance rated is 62 TFLOPS. The system supports a 70TB globally accessible, Lustre parallel file system. Nodes are interconnected with InfiniBand technology in a fat-tree topology with a 1GB/sec point-to-point bandwidth. Also, a 2.8 petabyte archive system and a 5TB SAN are available through the login/development nodes. ",TACC Dell PowerEdge Linux Cluster (Lonestar),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-12-13', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-12-12', 'friendly_end_date': None, 'production_begin_date': '2006-07-20', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
127,,production,ecss.ecss.xsede.org,ecss.xsede.org,production,2011-06-01,,"{'organizations': [{'organization_abbreviation': 'XD-ALLOCATIONS', 'organization_name': 'XD-ALLOCATIONS', 'organization_code': 'XD-ALLOCATIONS'}], 'xsede_services_only': True}",,XSEDE,XSEDE Level 1,96,resource,,,XSEDE Extended Collaborative Support,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2011-06-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-07-27 12:52:54.028
128,,decommissioned,longhorn.tacc.teragrid.org,tacc.teragrid.org,decommissioned,2014-04-18,,"{'latitude': None, 'node_count': 256.0, 'manufacturer': 'Dell / NVIDIA', 'rmax': None, 'interconnect': 'InfiniBand QDR', 'is_publicly_available': True, 'peak_teraflops': 20.7, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530139, 'conversion_factors': [{'begin_date': '2010-10-10', 'factor': 1.935, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': 3000.0, 'allocation_type': 'Startup', 'dollar_value': 0.12202760300000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.12202760300000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': 0.12202760300000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.12202760300000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 0.12202760300000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.12202760300000001}], 'display_resource_name': 'TACC Dell/NVIDIA Visualization and Data Analysis Cluster (Longhorn)', 'allocations_description': None, 'allocable_id': 142228}, 'job_manager': '', 'batch_system': 'SGE', 'platform_name': 'Visualization & Data Analysis Cluster', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '210 TB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'tg-login.longhorn.tacc.teragrid.org', 'storage_network': 'InfiniBand QDR', 'graphics_card': 'NVIDIA Quadro FX 5800', 'is_visualization': True, 'memory_per_cpu_gb': 6.0, 'cpu_type': 'Intel Nehalem', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Longhorn', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'Lustre Parallel File System', 'user_guide_url': 'https://www.xsede.org/web/guest/tacc-longhorn', 'disk_size_tb': 210.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.5300000000000002, 'cpu_count_per_node': 8, 'operating_system': 'Linux (CentOS)', 'community_software_area': False}",35.0,XSEDE,XSEDE Level 1,142228,compute,"Longhorn is a hybrid CPU/GPU system designed for remote, interactive visualization and data analysis, but it also supports production, compute-intensive calculations on both the CPUs and GPUs via off-hour queues. The large per-node memory (48GB on 240 Dell R610 nodes and 144GB on 16 Dell R710 nodes) is intended for serial and parallel visualization and analysis applications that take advantage of large memories, multiple computing cores, and multiple graphics processors.  Longhorn is an ideal companion resource for working with large data sets created on Ranger, since Longhorn can directly access Ranger's Lustre parallel file system through a 10 GigE network link.","The Longhorn Dell/NVIDIA Visualization and Data Analysis cluster is configured with 256 dual-socket nodes, each with significant compute and graphics capability. Total system resources include 2048 compute cores (2.53 GHzIntel Nehalem), 512 GPUs (128 NVIDIA Quadro Plex S4s, each containing 4 NVIDIA FX 5800s), 13.5 TB of distributed memory and a 210 TB local Lustre parallel file system.",TACC Dell/NVIDIA Visualization and Data Analysis Cluster (Longhorn),"{'coming_soon_begin_date': '2009-09-10', 'retired_begin_date': '2009-11-02', 'decommissioned_begin_date': '2014-04-18', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2009-11-01', 'post_production_begin_date': None, 'retired_end_date': '2009-12-07', 'production_end_date': '2014-04-17', 'friendly_end_date': None, 'production_begin_date': '2010-01-04', 'decommissioned_end_date': None, 'pre_production_begin_date': '2009-11-02', 'pre_production_end_date': '2010-01-03'}",
129,,,Abe-QB-Grid.teragrid.org,teragrid.org,,,,"{'organizations': [{'organization_abbreviation': 'TG-POPS', 'organization_name': 'TG-POPS', 'organization_code': 'TG-POPS'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,104,resource,,,NCSA/LONI Dell PowerEdge Linux Clusters (Abe/Queenbee),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2014-05-30', 'friendly_end_date': None, 'production_begin_date': '2008-10-10', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-07-27 21:44:00.632
130,,"friendly, production",bridges.large.psc.xsede.org,large.psc.xsede.org,production,2016-01-01,2019-11-30,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,51,resource,,"Two node types comprise Bridges' large memory nodes:  Extreme Shared Memory (ESM) and Large Shared Memory nodes.  ESMs are HP Integrity Superdome X servers.  Each will have 16 Intel Xeon EX-series CPUs and 12TB of hardware-enabled, cache-coherent memory.  LSM  nodes are HP DL580 servers with 4 Intel Xeon EX-series CPUs and 3TB each of hardware-enabled, cache-coherent shared memory. ",PSC Large Memory Nodes (Bridges Large),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2019-11-30', 'friendly_begin_date': '2016-03-11', 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-11-30', 'friendly_end_date': None, 'production_begin_date': '2016-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2015-09-15', 'pre_production_end_date': '2015-12-31'}",2017-02-14 18:39:33.397
131,,decommissioned,abe.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2011-04-15,,"{'latitude': None, 'node_count': 1200.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'InfiniBand', 'is_publicly_available': True, 'peak_teraflops': 89.0, 'advance_max_reservable_su': 9600.0, 'is_accounted': True, 'sensitive_data_support_description': None, 'job_manager': 'jobmanager-pbs', 'batch_system': 'Torque/Moab', 'platform_name': 'Intel 64 Linux Cluster', 'nfs_network': '', 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': 'Lustre', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'mikep@ncsa.uiuc.edu', 'login_hostname': 'login-abe.ncsa.teragrid.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 1.0, 'cpu_type': 'Intel 64', 'alternate_login_hostname': 'abe.ncsa.uiuc.edu', 'xsedenet_participant': None, 'max_reservable_su': 9600.0, 'nickname': 'Abe', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'Lustre (shared with Intel 64 Tesla cluster lincoln)', 'user_guide_url': 'http://www.ncsa.uiuc.edu/UserInfo/Resources/Hardware/Intel64Cluster/', 'disk_size_tb': 400.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.0, 'cpu_count_per_node': 8, 'operating_system': 'Red Hat Enterprise Linux 4 (Linux 2.6.9)', 'community_software_area': True}",19.0,XSEDE,XSEDE Level 1,126120,compute,"The NCSA Intel 64 cluster (Abe) is intended for highly scalable parallel applications.
<br />Note: Abe is in production as of July 9, 2007.  Starting with April 1 2008 allocations, Abe will be co-allocated with the LONI Queen Bee system.","This Dell blade system has 1,200 PowerEdge 1955 dual socket, quad core compute blades, an InfiniBand interconnect and 100 TB of storage in a Lustre filesystem.",Abe,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-04-15', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-04-14', 'friendly_end_date': None, 'production_begin_date': '2007-07-09', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
132,,decommissioned,gpfs-wan.tg.teragrid.org,tg.teragrid.org,decommissioned,2011-07-31,,"{'organizations': [{'organization_abbreviation': 'XD-ALLOCATIONS', 'organization_name': 'XD-ALLOCATIONS', 'organization_code': 'XD-ALLOCATIONS'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,82,resource,,,TeraGrid GPFS-WAN Disk Space,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
133,,,hpss.nics.xsede.org,nics.xsede.org,,,,"{'organizations': [{'organization_abbreviation': 'NICS', 'organization_name': 'National Institute for Computational Sciences', 'organization_code': 'T103349'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,89,resource,,,NICS Long-term tape Archival Storage (HPSS),"{'coming_soon_begin_date': None, 'retired_begin_date': '2013-01-01', 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': '2013-06-28', 'production_end_date': '2014-09-30', 'friendly_end_date': None, 'production_begin_date': '2013-07-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
134,,production,ranch.tacc.xsede.org,tacc.xsede.org,production,2013-03-15,,"{'organizations': [{'organization_abbreviation': 'UT Austin', 'organization_name': 'University of Texas at Austin', 'organization_code': '0036582'}, {'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,90,resource,,,TACC Long-term tape Archival Storage (Ranch),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2013-03-15', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-04-18 16:10:26.242
135,,post-production,staff.tg.teragrid.org,tg.teragrid.org,post-production,,,"{'organizations': [{'organization_abbreviation': 'LSU CCT', 'organization_name': 'LSU Center for Computation and Technology', 'organization_code': 'T103901'}], 'xsede_services_only': True}",,XSEDE,XSEDE Level 1,108,resource,,,XSEDE Staff Grid,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': '2007-01-01', 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-07-27 12:29:08.574
136,,decommissioned,keeneland.gatech.xsede.org,gatech.xsede.org,decommissioned,2014-12-31,,"{'organizations': [{'organization_abbreviation': 'Georgia Tech', 'organization_name': 'Georgia Institute of Technology', 'organization_code': '0087239'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,46,resource,,GaTech Keeneland,"GaTech HP/NVIDIA, Heterogeneous Computing System (Keeneland)","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2014-12-31', 'friendly_begin_date': None, 'post_production_end_date': '2014-12-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2014-08-12', 'retired_end_date': None, 'production_end_date': '2014-08-11', 'friendly_end_date': None, 'production_begin_date': '2012-10-31', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
137,,decommissioned,data-resource.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2010-09-01,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,74,resource,,,NCSA Data Resource Services,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-09-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-08-31', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
138,,decommissioned,rachel.psc.teragrid.org,psc.teragrid.org,decommissioned,2008-01-01,,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,22,resource,,Rachel is a loosely-coupled pair of SMP machines.  Each  system has 64 1.15 GHz EV7 processors with 256 Gbytes of shared memory.  Logins are to a front end node with 2 EV67  processors.  Both the front end node and  the SMP machines run the Tru64 Unix operating system.,PSC HP Marvel (Rachel),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
139,,decommissioned,keeneland.gatech.xsede.org,gatech.xsede.org,decommissioned,2014-12-31,,"{'latitude': None, 'node_count': 264.0, 'manufacturer': 'HP and NVIDIA', 'rmax': None, 'interconnect': 'Mellanox FDR InfiniBand ', 'is_publicly_available': True, 'peak_teraflops': 615.0, 'advance_max_reservable_su': None, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530160, 'conversion_factors': [{'begin_date': None, 'factor': 34.0, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.5664259060000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': None, 'allocation_type': 'Staff', 'dollar_value': 0.5664259060000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.5664259060000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.5664259060000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 0.5664259060000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': 10000.0, 'allocation_type': 'Startup', 'dollar_value': 0.5664259060000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.5664259060000001}], 'display_resource_name': 'GaTech HP/NVIDIA, Heterogeneous Computing System (Keeneland)', 'allocations_description': None, 'allocable_id': 142289}, 'job_manager': '', 'batch_system': 'Torque/Moab ', 'platform_name': 'KFS', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '1720 Tbytes after planned expansion', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'keeneland.gatech.xsede.org', 'storage_network': '', 'graphics_card': 'NVIDIA M2090', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': '2 Intel Sandy Bridge (Xeon E5) processor with 3 NVIDIA M2090 GPU accelerators', 'alternate_login_hostname': 'keeneland.nics.utk.edu', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Keeneland', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://portal.xsede.org/gatech-keeneland', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.6, 'cpu_count_per_node': 16, 'operating_system': 'CentOS', 'community_software_area': False}",46.0,XSEDE,XSEDE Level 1,142289,compute,-- Keeneland is a balanced hybrid CPU/GPGPU system for use with codes that can take advantage of accelerator performance. ,GaTech Keeneland,"GaTech HP/NVIDIA, Heterogeneous Computing System (Keeneland)","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2014-12-31', 'friendly_begin_date': None, 'post_production_end_date': '2014-12-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2014-08-12', 'retired_end_date': None, 'production_end_date': '2014-08-11', 'friendly_end_date': None, 'production_begin_date': '2012-10-31', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
140,,decommissioned,kids.gatech.xsede.org,gatech.xsede.org,decommissioned,2014-08-30,,"{'latitude': None, 'node_count': 120.0, 'manufacturer': 'HP and NVIDIA', 'rmax': None, 'interconnect': 'Mellanox QDR InfiniBand', 'is_publicly_available': True, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530158, 'conversion_factors': [{'begin_date': None, 'factor': 0.0, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': None, 'allocation_type': 'Staff', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'Transfers Only', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}], 'display_resource_name': 'GaTech HP/NVIDIA, Heterogeneous Computing System (KIDS)', 'allocations_description': None, 'allocable_id': 142292}, 'job_manager': '', 'batch_system': 'Torque/Moab', 'platform_name': 'KIDS', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'kidlogin.nics.utk.edu', 'storage_network': '', 'graphics_card': 'NVIDIA M2090', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': '2 Intel Westmere processor with NVIDIA 3 M2090 GPU', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Keeneland-KIDS', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://www.xsede.org/web/guest/gatech-keeneland', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.6, 'cpu_count_per_node': 12, 'operating_system': 'CentOS', 'community_software_area': False}",45.0,XSEDE,XSEDE Level 1,142292,compute,-- Keeneland is a balanced hybrid CPU/GPGPU system for use with codes that can take advantage of accelerator performance. This is the initial delivery system (KIDS).,GaTech Keeneland KIDS ,"GaTech HP/NVIDIA, Heterogeneous Computing System (KIDS)","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2014-08-30', 'friendly_begin_date': None, 'post_production_end_date': '2014-08-29', 'coming_soon_end_date': None, 'post_production_begin_date': '2014-08-11', 'retired_end_date': None, 'production_end_date': '2014-08-10', 'friendly_end_date': None, 'production_begin_date': '2012-07-02', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
141,,decommissioned,data.iu.teragrid.org,iu.teragrid.org,decommissioned,2013-01-01,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,88,resource,,,Dedicated (nonpurged) disk for databases and data collections,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-01-01', 'friendly_begin_date': None, 'post_production_end_date': '2012-12-31', 'coming_soon_end_date': None, 'post_production_begin_date': '2012-12-31', 'retired_end_date': None, 'production_end_date': '2012-12-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
142,,decommissioned,dtf.caltech.teragrid.org,caltech.teragrid.org,decommissioned,1005-06-17,,"{'organizations': [{'organization_abbreviation': 'Caltech', 'organization_name': 'California Institute of Technology', 'organization_code': '0011312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,53,resource,,DTF Cal Tech,Cal Tech DTF,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '1005-06-17', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
143,,,mason.iu.xsede.org,iu.xsede.org,,,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,63,resource,,"Mason has 18 HP DL580 G7 compute nodes, each with four eight-core 1.86 GHz Intel L7555 CPUs, 512 GB memory, 400GB of local scratch disk, and a 10gb Ethernet network interface.",Indiana University HP DL580 Large Memory Cluster (Mason),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2016-03-31', 'friendly_begin_date': None, 'post_production_end_date': '2016-03-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2015-09-15', 'retired_end_date': None, 'production_end_date': '2015-03-30', 'friendly_end_date': None, 'production_begin_date': '2013-04-15', 'decommissioned_end_date': '2016-05-06', 'pre_production_begin_date': '2013-04-02', 'pre_production_end_date': '2013-04-14'}",2016-07-27 17:17:03.177
144,,decommissioned,kids.gatech.xsede.org,gatech.xsede.org,decommissioned,2014-08-30,,"{'organizations': [{'organization_abbreviation': 'Georgia Tech', 'organization_name': 'Georgia Institute of Technology', 'organization_code': '0087239'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,45,resource,,GaTech Keeneland KIDS ,"GaTech HP/NVIDIA, Heterogeneous Computing System (KIDS)","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2014-08-30', 'friendly_begin_date': None, 'post_production_end_date': '2014-08-29', 'coming_soon_end_date': None, 'post_production_begin_date': '2014-08-11', 'retired_end_date': None, 'production_end_date': '2014-08-10', 'friendly_end_date': None, 'production_begin_date': '2012-07-02', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
145,,decommissioned,data-capacitor.iu.teragrid.org,iu.teragrid.org,decommissioned,2014-01-07,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,75,resource,,,Lustre file space (IU Data Capacitor),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2014-01-07', 'friendly_begin_date': None, 'post_production_end_date': '2014-01-06', 'coming_soon_end_date': None, 'post_production_begin_date': '2012-12-31', 'retired_end_date': None, 'production_end_date': '2012-12-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
146,,friendly,mason.iu.xsede.org,iu.xsede.org,friendly,2016-05-07,,"{'latitude': None, 'node_count': 18.0, 'manufacturer': 'HP', 'rmax': None, 'interconnect': '10gb Ethernet', 'is_publicly_available': True, 'peak_teraflops': 4.284, 'advance_max_reservable_su': None, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530168, 'conversion_factors': [{'begin_date': '2013-04-15', 'factor': 1.483, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'Mason is only available to NSF funded life sciences and genomics researchers with large memory needs. Please clearly justify this in your request.', 'allocation_state': 'No Actions Allowed', 'maximum_amount': 15000.0, 'allocation_type': 'Startup', 'dollar_value': 0.0}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'Mason is only available to NSF funded life sciences and genomics researchers with large memory needs. Please clearly justify this in your request.', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.0}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'Mason is only available to NSF funded life sciences and genomics researchers with large memory needs. Please clearly justify this in your request.', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.0}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'Mason is only available to NSF funded life sciences and genomics researchers with large memory needs. Please clearly justify this in your request.', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 0.0}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'Mason is only available to NSF funded life sciences and genomics researchers with large memory needs. Please clearly justify this in your request.', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.0}], 'display_resource_name': 'Indiana University HP DL580 Large Memory Cluster (Mason)', 'allocations_description': '<span class=""important"">Mason is only available to NSF funded life sciences and genomics researchers with large memory needs. Please clearly justify this in your request.</span>', 'allocable_id': 142294}, 'job_manager': '', 'batch_system': 'Torque 4.x\r\nMoab 7.x', 'platform_name': 'DL580', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': 'Lustre (3.5PB)', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'mason.iu.xsede.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 16.0, 'cpu_type': 'Intel L7555', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Mason', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://portal.xsede.org/iu-mason', 'disk_size_tb': 7956.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.8599999999999999, 'cpu_count_per_node': 32, 'operating_system': 'Redhat Enterprise Linux version 6', 'community_software_area': False}",63.0,XSEDE,XSEDE Level 1,142294,compute,"Mason is intended to run primarily large memory (more than 16GB and up to 500GB) serial jobs. The system is targeted at life sciences related work and operated by the National Center for Genome Analysis Support at IU, http://ncgas.org/.","Mason has 18 HP DL580 G7 compute nodes, each with four eight-core 1.86 GHz Intel L7555 CPUs, 512 GB memory, 400GB of local scratch disk, and a 10gb Ethernet network interface.",Indiana University HP DL580 Large Memory Cluster (Mason),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2016-03-31', 'friendly_begin_date': '2016-05-07', 'post_production_end_date': '2016-03-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2015-09-15', 'retired_end_date': None, 'production_end_date': '2015-03-30', 'friendly_end_date': None, 'production_begin_date': '2013-04-15', 'decommissioned_end_date': '2016-05-06', 'pre_production_begin_date': '2013-04-02', 'pre_production_end_date': '2013-04-14'}",
147,,decommissioned,avid32.iu.teragrid.org,iu.teragrid.org,decommissioned,2008-01-01,,"{'latitude': None, 'node_count': 96.0, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': '', 'is_publicly_available': True, 'peak_teraflops': 2.2, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530097, 'conversion_factors': [{'begin_date': None, 'factor': 0.0, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}], 'display_resource_name': 'TeraGrid Indiana University (AVIDD-I32 IA-32 Cluster)', 'allocations_description': None, 'allocable_id': 142229}, 'job_manager': 'jobmanager-fork (default)  jobmanager-pbs', 'batch_system': '', 'platform_name': 'IA-32', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '30 GB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'tg-login-ia32.iu.teragrid.org', 'storage_network': 'Fibre Channel', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 1.25, 'cpu_type': '', 'alternate_login_hostname': 'tg-login-ia32.iu.teragrid.org', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'AVIDD', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '1 TB', 'user_guide_url': 'http://rac.uits.iu.edu/rats/research/avidd-i/index.shtml', 'disk_size_tb': 3.55, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.4, 'cpu_count_per_node': 2, 'operating_system': 'RedHat Enterprise 3', 'community_software_area': False}",23.0,XSEDE,XSEDE Level 1,142229,compute,The AVIDD IA-32 cluster is intended to run parallel as well as serial applications.,"The IU AVIDD IA-32 cluster has 192 compute nodes, each with two 2.4 GHz Intel Pentium4 Xeon CPUs, 2.5 GB memory, 10.4 GB of local scratch, a Myrinet-2000 interconnect and access to the 1.6TB GPFS scratch space. NOTE: 20% of this resource was available for TeraGrid usage.",Indiana University (AVIDD-I32 IA-32 Cluster),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
148,,decommissioned,condor.iu.teragrid.org,iu.teragrid.org,decommissioned,2010-03-30,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,52,resource,,Condor IU,Indiana University RenderPortal/Windows Condor Pool,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-30', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
149,,decommissioned,archive.iu.teragrid.org,iu.teragrid.org,decommissioned,2013-01-01,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,87,resource,,,Indiana University DC-WAN Lustre Filesystem,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-01-01', 'friendly_begin_date': None, 'post_production_end_date': '2012-12-31', 'coming_soon_end_date': None, 'post_production_begin_date': '2012-12-31', 'retired_end_date': None, 'production_end_date': '2012-12-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
150,,decommissioned,bigred.iu.teragrid.org,iu.teragrid.org,decommissioned,2011-07-31,,"{'latitude': None, 'node_count': 768.0, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': 'PCI-X Myrinet-2000', 'is_publicly_available': True, 'peak_teraflops': 30.7, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530109, 'conversion_factors': [{'begin_date': '2006-10-01', 'factor': 1.782, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'Indiana University IBM e1350 (Big Red)', 'allocations_description': None, 'allocable_id': 126074}, 'job_manager': 'jobmanager-loadleveler', 'batch_system': 'LoadLeveler 3.4\r\nMoab client 4.5.0', 'platform_name': 'e1350', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': 'GPFS(266TB) + Lustre (535TB)', 'rpeak': None, 'machine_type': 'MPP', 'gpu_description': None, 'community_software_area_email': 'rsheppar@indiana.edu', 'login_hostname': 'login.bigred.iu.teragrid.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'PowerPC 970MP', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Big Red', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'GPFS + Lustre', 'user_guide_url': '', 'disk_size_tb': 266.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.5, 'cpu_count_per_node': 4, 'operating_system': 'SuSE Linux Enterprise Server 9', 'community_software_area': True}",20.0,XSEDE,XSEDE Level 1,126074,compute,"Big Red is a distributed shared-memory cluster, intended to run parallel as well as serial applications.<br />","Big Red has 768 IBM JS21 compute nodes, each with two dual-core 2.5 GHz PowerPC 970MP CPUs, 8 GB memory, 72 GB of local scratch disk, and a PCI-X Myrinet-2000 interconnect for high-bandwidth low-latency MPI applications. It has access to 266 TB of local GPFS scratch space, the TeraGrid-wide GPFS-WAN file system, and to the 535 TB Lustre file system provided by the Data Capacitor. NOTE: 6.5 TFlops is available for TeraGrid usage.",Indiana University IBM e1350 (Big Red),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
151,,decommissioned,tg-viz.uc.teragrid.org,uc.teragrid.org,decommissioned,2009-07-01,,"{'organizations': [{'organization_abbreviation': 'ANL', 'organization_name': 'Argonne National Laboratory', 'organization_code': '9000126'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,33,resource,,"The IA-32 TeraGrid Linux Visualization Cluster at UC/ANL consists of 96 nodes with dual Intel Xeon processors, with 4 GB of memory and nVidia GeFORCE 6600GT AGP graphics card per node. The cluster is running Red Hat Enterprise Linux and is using Myricom       's Myrinet cluster interconnect network.  There is a 16 TB local high-performance GPFS, and access to the TeraGrid-wide GPFS-WAN file-system.",UC/ANL Visualization Cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-07-01', 'friendly_begin_date': None, 'post_production_end_date': '2009-06-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2009-06-01', 'retired_end_date': None, 'production_end_date': '2009-05-31', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
152,,production,comet.gpu.sdsc.xsede.org,gpu.sdsc.xsede.org,production,2015-04-01,2019-01-30,"{'latitude': None, 'sensitive_data_support_description': '', 'is_accounted': False, 'is_authenticated': False, 'xsedenet_participant': None, 'is_authorized': False, 'supports_sensitive_data': False, 'is_publicly_available': False, 'user_guide_url': 'https://portal.xsede.org/sdsc-comet', 'longitude': None}",128.0,XSEDE,XSEDE Level 1,144587,other,"GPUs are a specialized resource that performs well for certain classes of algorithms and applications. There is a large and growing base of community codes that have been optimized for GPUs including those in molecular dynamics, and machine learning. GPU-enabled applications on Comet include: Amber, Gromacs, BEAST, OpenMM, TensorFlow, and NAMD.
","Comet has 36 general purpose GPU nodes, with 2 Tesla K80 GPU graphics cards per node, each with 2 GK210 GPUs (144 GPUs in total). Each GPU node also features 2 Intel Haswell processors of the same design and performance as the standard compute nodes (described separately). The GPU nodes are integrated into the Comet resource and available through the Slurm scheduler for either dedicated or shared node jobs (i.e., a user can run on 1 or more GPUs/node and will be charged accordingly). Like the Comet standard compute nodes, the GPU nodes feature a local SSD which can be specified as a scratch resource during job execution – in many cases using SSD’s can alleviate I/O bottlenecks associated with using the shared Lustre parallel file system.
",SDSC Comet GPU Nodes (Comet GPU),"{'coming_soon_begin_date': '2014-09-12', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2014-09-13', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-01-30', 'friendly_end_date': None, 'production_begin_date': '2015-04-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2014-09-14', 'pre_production_end_date': '2015-03-31'}",2017-03-15 20:22:07.363
153,,decommissioned,gpfs-wan.iu.teragrid.org,iu.teragrid.org,decommissioned,2011-07-01,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,78,resource,,,GPFS-WAN,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-06-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
154,,decommissioned,hpss.iu.teragrid.org,iu.teragrid.org,decommissioned,2011-07-31,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,83,resource,,,Indiana University HPSS Archival Storage,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
155,,decommissioned,datastar-p655.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2008-10-01,,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,1,resource,,"DataStar has 272 8-way P655+ compute nodes -- 176 nodes with 1.5-GHz Power4+ CPUs and 16 GB of memory, and 96 with 1.7 GHz Power4+ CPUs and 32 GB of memory. The nodes are connected by an IBM high-speed Federation switch and have access to 130 TB of GPFS.","SDSC IBM p655 (DataStar, 8-way)","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-10-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
156,,decommissioned,datastar.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2008-10-01,,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,2,resource,,"DataStar has 6 32-way P690 compute nodes with 1.7 GHz Power4+ processors. Five nodes have 128 GB of shared memory, and one node has 256 GB of memory. All nodes are connected via DataStar   's Federation switch and have access to 130 TB of GPFS.","SDSC IBM p690 (DataStar, 32-way SMP)","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-10-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
157,,decommissioned,datastar.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2008-10-01,,"{'latitude': None, 'node_count': 5.0, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': 'IBM Federation', 'is_publicly_available': True, 'peak_teraflops': 1.3, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530080, 'conversion_factors': [{'begin_date': '2004-04-01', 'factor': 0.914, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'NRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'PRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-SDSC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}], 'display_resource_name': 'SDSC DataStar IBM p690 (32-way SMP)', 'allocations_description': None, 'allocable_id': 125570}, 'job_manager': 'jobmanager-fork\n\njobmanager-loadleveler', 'batch_system': 'LoadLeveler', 'platform_name': 'pSeries 690', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '2 TB', 'rpeak': None, 'machine_type': 'MPP', 'gpu_description': None, 'community_software_area_email': None, 'login_hostname': 'dslogin.sdsc.edu', 'storage_network': 'Fibre Channel', 'graphics_card': None, 'is_visualization': False, 'memory_per_cpu_gb': 4.0, 'cpu_type': 'IBM Power4+', 'alternate_login_hostname': None, 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'DataStar', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': None, 'parallel_file_system': 'GPFS: 130 TB', 'user_guide_url': 'http://www.sdsc.edu/us/resources/datastar/', 'disk_size_tb': 115.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.7000000000000002, 'cpu_count_per_node': 32, 'operating_system': 'AIX 5.2L', 'community_software_area': False}",2.0,XSEDE,XSEDE Level 1,125570,compute,"[DECOMM]<!-- please use this space for Recommended Use and Status -->The p690s are primarily intended to run applications that require large amounts of shared memory.
<br /><strong>Status</strong>

Decommissioned 10/1/2008","DataStar has 6 32-way P690 compute nodes with 1.7 GHz Power4+ processors. Five nodes have 128 GB of shared memory, and one node has 256 GB of memory. All nodes are connected via DataStar   's Federation switch and have access to 130 TB of GPFS.","SDSC IBM p690 (DataStar, 32-way SMP)","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-10-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
158,,decommissioned,tg-viz.uc.teragrid.org,uc.teragrid.org,decommissioned,2009-07-01,,"{'latitude': None, 'node_count': 96.0, 'manufacturer': '', 'rmax': None, 'interconnect': 'Myrinet', 'is_publicly_available': True, 'peak_teraflops': 0.0, 'advance_max_reservable_su': 192.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530105, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}], 'display_resource_name': 'UC/ANL Visualization Cluster', 'allocations_description': None, 'allocable_id': 125988}, 'job_manager': 'jobmanager-pbs', 'batch_system': 'Torque & Moab', 'platform_name': 'IA-32 Visualization Cluster', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'tg-viz-login.uc.teragrid.org', 'storage_network': 'Fibre Channel', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Intel Xeon', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 192.0, 'nickname': 'tg-viz.uc', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'GPFS: 11 TB scratch\r\n4 TB home/software\r\n\r\nPVFS: 5 TB scratch', 'user_guide_url': 'http://www.uc.teragrid.org/user-guide.html', 'disk_size_tb': 4.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.0, 'cpu_count_per_node': 2, 'operating_system': 'Red Hat Enterprise Linux 4', 'community_software_area': False}",33.0,XSEDE,XSEDE Level 1,125988,compute,,"The IA-32 TeraGrid Linux Visualization Cluster at UC/ANL consists of 96 nodes with dual Intel Xeon processors, with 4 GB of memory and nVidia GeFORCE 6600GT AGP graphics card per node. The cluster is running Red Hat Enterprise Linux and is using Myricom       's Myrinet cluster interconnect network.  There is a 16 TB local high-performance GPFS, and access to the TeraGrid-wide GPFS-WAN file-system.",UC/ANL Visualization Cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-07-01', 'friendly_begin_date': None, 'post_production_end_date': '2009-06-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2009-06-01', 'retired_end_date': None, 'production_end_date': '2009-05-31', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
159,,decommissioned,radium.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2007-04-16,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,6,resource,,This resource is no longer available to TeraGrid.,NCSA Condor Pool (Radium),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2007-04-16', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
160,,decommissioned,copper.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2007-09-30,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,7,resource,,"The pSeries 690 is a symmetric multiprocessor (SMP) system based on the POWER4 processor. The NCSA system consists of 12 32-processor hosts running AIX, eight of which have 64 GB of memory, and the other four have 256 GB of memory. It uses the GPFS parallel file system from IBM.",NCSA IBM p690 (Copper),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2007-09-30', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
161,,decommissioned,nstg.ornl.teragrid.org,ornl.teragrid.org,decommissioned,2011-07-31,,"{'latitude': None, 'node_count': 28.0, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': '', 'is_publicly_available': True, 'peak_teraflops': 0.34, 'advance_max_reservable_su': 32.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530137, 'conversion_factors': [{'begin_date': '2004-10-01', 'factor': 0.993, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'ORNL Intel Xeon Cluster (NSTG)', 'allocations_description': None, 'allocable_id': 125610}, 'job_manager': 'jobmanager-fork  jobmanager-pbs', 'batch_system': '', 'platform_name': 'IA-32 Cluster', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'tg-support@ornl.gov', 'login_hostname': 'tg-login.ornl.teragrid.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.5, 'cpu_type': 'Intel Xeon', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 32.0, 'nickname': 'NSTG Cluster', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '', 'user_guide_url': 'http://www.ornl.teragrid.org/guides/userguide.html', 'disk_size_tb': 2.14, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 3.06, 'cpu_count_per_node': 2, 'operating_system': 'SuSE Linux 10.2', 'community_software_area': True}",8.0,XSEDE,XSEDE Level 1,125610,compute,Available for general use. Also available for special request use for long duration or experimental infrastructure test deployments.,"The ORNL NSTG cluster has 28 nodes, 16 of which are dedicated to running compute jobs. Each compute node has two 3.06 GHz Intel Pentium4 Xeon CPUs, 2.5 GB memory, and 26 GB of local scratch. 800 GB of shared scratch is provided across the private gigabit interconnect. Four additional nodes are dedicated to running GridFTP servers, and each is configured with 4 GB of memory.",ORNL Intel Xeon Cluster (NSTG),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
162,,decommissioned,tungsten.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2008-07-21,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,9,resource,,"This cluster is based on Dell PowerEdge 1750 servers, each with two Intel IA-32 Xeon 3.2 GHz processors and 3 GB of memory. It is running Red Hat Linux and Myricom               's Myrinet cluster interconnect network, and the CFS Lustre shared parallel filesystem.",NCSA Xeon Linux Supercluster (Tungsten),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-07-21', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
163,,decommissioned,lear.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2008-01-01,,"{'latitude': None, 'node_count': 512.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'Gigabit Ethernet', 'is_publicly_available': True, 'peak_teraflops': 6.6, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530110, 'conversion_factors': [{'begin_date': '2006-01-01', 'factor': 0.849, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}], 'display_resource_name': 'Purdue EM64T Linux Cluster (Lear)', 'allocations_description': None, 'allocable_id': 125840}, 'job_manager': 'jobmanager-fork, jobmanager-PBS', 'batch_system': 'PBSPro 8.0', 'platform_name': 'EM64T Cluster', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '/home\n10 TB<br>\n\n/scratch/local\n8.8 TB<br>\n\n/scratch/teragrid\n4.8 TB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': None, 'login_hostname': 'tg-login.purdue.teragrid.org', 'storage_network': None, 'graphics_card': None, 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Intel em64T Xeon', 'alternate_login_hostname': None, 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Lear', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': None, 'parallel_file_system': None, 'user_guide_url': None, 'disk_size_tb': 28.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 3.2, 'cpu_count_per_node': 2, 'operating_system': 'RedHat Enterprise 4', 'community_software_area': False}",11.0,XSEDE,XSEDE Level 1,125840,compute,"[DECOMM]Lear is well suited for a wide range of both serial and parallel jobs.<br />
<strong>Status</strong>
This resource was retired on May 5th 2008.","The Lear Cluster consists of 512 dual-CPU Dell PowerEdge 1425 compute nodes, running Red Hat Enterprise Linux, version 4. Each node has two 64-bit EM64T 3.2 GHz Xeon CPUs and 4 GB of RAM. The cluster is interconnected with Gigabit Ethernet, and offers 8.8 TB of NFS scratch storage available to all Lear users plus a 4.8 TB NFS scratch area dedicated to TeraGrid users. Lear users may also access a 1.3 PB DXUL archive system. Lear   's peak performance is rated at 6.6 TFLOPS.",Purdue EM64T Linux Cluster (Lear),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
164,,decommissioned,steele.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2013-07-31,,"{'latitude': None, 'node_count': 893.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'Gigabit Ethernet/Infiniband', 'is_publicly_available': True, 'peak_teraflops': 66.59, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530134, 'conversion_factors': [{'begin_date': '2008-05-01', 'factor': 1.6059999999999999, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 200000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'Purdue Dell PowerEdge Linux Cluster (Steele)', 'allocations_description': None, 'allocable_id': 126498}, 'job_manager': 'jobmanager-pbs', 'batch_system': 'PBSPro 9.1', 'platform_name': '1950 Cluster', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '/home\r\n250 GB\r\n\r\n/scratch\r\n55 TB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'tg-login.purdue.teragrid.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 32.0, 'cpu_type': 'Intel E5410', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Steele', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '', 'user_guide_url': 'https://www.xsede.org/web/guest/purdue-steele', 'disk_size_tb': 130.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.33, 'cpu_count_per_node': 8, 'operating_system': 'RedHat Enterprise 4', 'community_software_area': False}",12.0,XSEDE,XSEDE Level 1,126498,compute,Steele is well suited for a wide range of both serial and parallel jobs.,"The Steele cluster consists of 893 dual quad-core Dell 1950 compute nodes, running Red Hat Enterprise Linux, version 4. Each node thus has 8 64-bit Intel 2.33 GHz E5410 CPUs and either 16 GB or 32 GB of RAM. They are interconnected with either Gigabit Ethernet or InfiniBand. The machine offers access to the RCAC scratch space. Steele's peak performance is rated at 66.59 TFLOPS.",Purdue Dell PowerEdge Linux Cluster (Steele),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2013-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
165,,decommissioned,tungsten.iu.teragrid.org,iu.teragrid.org,decommissioned,2008-01-01,,"{'latitude': None, 'node_count': 2.0, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': '', 'is_publicly_available': True, 'peak_teraflops': 2.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'sensitive_data_support_description': None, 'job_manager': 'jobmanager-fork (default)  jobmanager-pbs', 'batch_system': '', 'platform_name': 'AVIDD IA-32', 'nfs_network': 'Gigabit Ethernet', 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '30 GB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'tg-login-ia32.iu.teragrid.org', 'storage_network': 'Fibre Channel', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 1.0, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'tungsten', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '1 TB', 'user_guide_url': 'http://rac.uits.iu.edu/rats/research/avidd-i/index.shtml', 'disk_size_tb': 3.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.0, 'cpu_count_per_node': 96, 'operating_system': 'RedHat Enterprise 3', 'community_software_area': False}",15.0,XSEDE,XSEDE Level 1,125724,compute,"[DECOMM]The AVIDD IA-32 cluster is intended to run parallel as well as serial applications.

<br />
<strong>Status</strong><br />
This machine has been decommissioned.","The IU AVIDD IA-32 cluster has 192 compute nodes, each with two 2.4 GHz Intel Pentium4 Xeon CPUs, 2.5 GB memory, 10.4 GB of local scratch, a Myrinet-2000 interconnect and access to the 1.6TB GPFS scratch space. NOTE: 20% of this resource was available for TeraGrid usage.

<strong class=      ""attention      "">IMPORTANT: </strong>There will be no more future allocations on this resource.",NCSA Xeon Linux Supercluster (Tungsten),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
166,,decommissioned,bigben.psc.teragrid.org,psc.teragrid.org,decommissioned,2010-03-31,,"{'latitude': None, 'node_count': 2090.0, 'manufacturer': 'Cray', 'rmax': None, 'interconnect': 'Cray Seastar', 'is_publicly_available': True, 'peak_teraflops': 21.0, 'advance_max_reservable_su': 4096.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530096, 'conversion_factors': [{'begin_date': '2006-12-19', 'factor': 1.024, 'conversion': 'Teragrid', 'end_date': None}, {'begin_date': '2005-10-01', 'factor': 0.935, 'conversion': 'Teragrid', 'end_date': '2006-12-18'}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-PSC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 200000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}], 'display_resource_name': 'PSC XT3 (Big Ben)', 'allocations_description': None, 'allocable_id': 125598}, 'job_manager': 'jobmanager-bigben-pbs', 'batch_system': 'PBS', 'platform_name': 'XT3', 'nfs_network': '10 Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '/home\n4 TB', 'rpeak': None, 'machine_type': 'MPP', 'gpu_description': None, 'community_software_area_email': 'raymond@psc.edu', 'login_hostname': 'tg-login.bigben.psc.teragrid.org', 'storage_network': 'Infiniband', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 1.0, 'cpu_type': 'AMD Opteron', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 4096.0, 'nickname': 'BigBen', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'Lustre PFS: 236 TB Total', 'user_guide_url': 'http://www.psc.edu/machines/cray/xt3/bigben.html', 'disk_size_tb': 100.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 200.0, 'cpu_count_per_node': 2, 'operating_system': 'SuSE Linux [Frontend] Catamount [Compute]', 'community_software_area': True}",16.0,XSEDE,XSEDE Level 1,125598,compute,BigBen is primarily intended to run applications with very high levels of parallelism or concurrency (512 - 4096 processes).,"BigBen is a Cray XT3 MPP system with 2068 2.6-GHz dual-core AMD Opteron compute nodes linked by a custom-designed interconnect. Twenty-two dedicated I/O processors are also connected to this network.  Each compute node has 2 Gbytes of memory shared by its two cores, and runs the Catamount operating system. The front end processors run SuSE Linux.",PSC XT3 (Big Ben),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-03-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
167,,friendly,nautilus.nics.teragrid.org,nics.teragrid.org,friendly,2015-05-01,,"{'latitude': None, 'node_count': 1.0, 'manufacturer': 'SGI', 'rmax': None, 'interconnect': 'SGI NUMAlink 5', 'is_publicly_available': True, 'peak_teraflops': 8.2, 'advance_max_reservable_su': None, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530143, 'conversion_factors': [{'begin_date': '2010-10-01', 'factor': 1.572, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'Transfers Only', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'Transfers Only', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}], 'display_resource_name': 'NICS SGI/NVIDIA, Visualization and Data Analysis System (Nautilus)', 'allocations_description': None, 'allocable_id': 142269}, 'job_manager': '', 'batch_system': 'Torque/Moab', 'platform_name': 'SGI/NVIDIA, Visualization and Data Analysis System', 'nfs_network': 'GigE', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '960 TBs', 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': None, 'community_software_area_email': 'help@teragrid.org', 'login_hostname': 'login.nautilus.nics.xsede.org', 'storage_network': 'Infiniband', 'graphics_card': 'NVIDIA Tesla', 'is_visualization': True, 'memory_per_cpu_gb': 4.0, 'cpu_type': 'Intel Nehalem EX', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Nautilus', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://portal.xsede.org/nics-nautilus', 'disk_size_tb': 960.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.0, 'cpu_count_per_node': 1024, 'operating_system': 'Linux (SUSE v11)', 'community_software_area': True}",39.0,XSEDE,XSEDE Level 1,142269,compute,"Nautilus is intended for serial and parallel visualization and analysis applications that take advantage of large memories, multiple computing cores, and multiple graphics processors. Allowing both exploitation of a large number of processors for distributed processing and the execution of legacy serial analysis algorithms for very large data processing by large numbers of users simultaneously.

This machine was initially decommissioned on 9/30/2013. However, it is again available as an XSEDE production resource starting May 1, 2014. This machine will only be available for supplement requests.","Nautilus is an SGI Altix UV 1000 system.  It has 1024 cores (Intel Nehalem EX processors), 4 terabytes of global shared memory, and 8 GPUs in a single system image.

This machine was initially decommissioned on 9/30/2013. However, it is again available as an XSEDE production resource starting May 1, 2014. This machine will only be available for supplement requests.","NICS SGI/NVIDIA, Visualization and Data Analysis System (Nautilus)","{'coming_soon_begin_date': '2010-03-17', 'retired_begin_date': '2010-09-07', 'decommissioned_begin_date': '2015-04-30', 'friendly_begin_date': '2015-05-01', 'post_production_end_date': '2015-04-29', 'coming_soon_end_date': '2010-04-06', 'post_production_begin_date': '2014-08-11', 'retired_end_date': '2010-09-23', 'production_end_date': '2014-04-30', 'friendly_end_date': None, 'production_begin_date': '2010-10-01', 'decommissioned_end_date': '2015-04-30', 'pre_production_begin_date': '2010-04-07', 'pre_production_end_date': '2010-09-30'}",
168,,decommissioned,lincoln.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2011-10-17,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,21,resource,,Lincoln consists of 192 compute nodes (Dell PowerEdge 1950 dual-socket nodes with quad-core Intel Harpertown 2.33GHz processors and 16GB of memory) and 96 NVIDIA Tesla S1070 accelerator units. Each Tesla unit provides 345.6 gigaflops of double-precision performance and 16GB of memory.,NCSA Lincoln Supercluster,"{'coming_soon_begin_date': '2008-09-08', 'retired_begin_date': None, 'decommissioned_begin_date': '2011-10-17', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2008-10-14', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-10-16', 'friendly_end_date': None, 'production_begin_date': '2009-02-02', 'decommissioned_end_date': None, 'pre_production_begin_date': '2008-10-15', 'pre_production_end_date': '2009-02-01'}",
169,,decommissioned,avid32.iu.teragrid.org,iu.teragrid.org,decommissioned,2008-01-01,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,23,resource,,"The IU AVIDD IA-32 cluster has 192 compute nodes, each with two 2.4 GHz Intel Pentium4 Xeon CPUs, 2.5 GB memory, 10.4 GB of local scratch, a Myrinet-2000 interconnect and access to the 1.6TB GPFS scratch space. NOTE: 20% of this resource was available for TeraGrid usage.",Indiana University (AVIDD-I32 IA-32 Cluster),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
170,,production,xstream.stanford.xsede.org,stanford.xsede.org,production,2016-07-01,,"{'organizations': [{'organization_abbreviation': 'Stanford U', 'organization_name': 'Stanford University', 'organization_code': '0013052'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 2,57,resource,,"XStream is a Cray GPU cluster interconnected with FDR InfiniBand (56 Gb/s) in a fat-tree topology. It differs from traditional CPU-based HPC systems as it has almost a Petaflop (PF)  of GPU compute power. Each of the 65 nodes has 8 NVIDIA K80 cards or 16 NVIDIA Kepler GPUs, interconnected through PCI-Express PLX-based switches. Each GPU has 12 GB of GDDR5 memory. Compute nodes also feature 2 Intel Ivy-Bridge 10-core CPUs, 256 GB of DRAM and 450 GB of local SSD storage. The system features 1.4 PB of Lustre storage (22 GB/s aggregate).",Stanford University GPU Cluster (XStream),"{'coming_soon_begin_date': '2016-03-15', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': '2016-04-16', 'post_production_end_date': None, 'coming_soon_end_date': '2016-04-16', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': '2016-06-30', 'production_begin_date': '2016-07-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2016-04-16', 'pre_production_end_date': '2016-06-30'}",2016-06-06 15:56:56.560
171,,decommissioned,bigred.iu.teragrid.org,iu.teragrid.org,decommissioned,2011-07-31,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,20,resource,,"Big Red has 768 IBM JS21 compute nodes, each with two dual-core 2.5 GHz PowerPC 970MP CPUs, 8 GB memory, 72 GB of local scratch disk, and a PCI-X Myrinet-2000 interconnect for high-bandwidth low-latency MPI applications. It has access to 266 TB of local GPFS scratch space, the TeraGrid-wide GPFS-WAN file system, and to the 535 TB Lustre file system provided by the Data Capacitor. NOTE: 6.5 TFlops is available for TeraGrid usage.",Indiana University IBM e1350 (Big Red),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
172,,decommissioned,dtf.uc.teragrid.org,uc.teragrid.org,decommissioned,2009-07-01,,"{'organizations': [{'organization_abbreviation': 'ANL', 'organization_name': 'Argonne National Laboratory', 'organization_code': '9000126'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,25,resource,,"The IA-64 TeraGrid Linux Cluster at UC/ANL consists of 62 nodes with dual Intel Itanium 2 processors, with 4 GB of memory per node. The cluster is running Red Hat Enterprise Linux and is using the Myricom Myrinet cluster interconnect network. There is a 16 TB local high-performance GPFS, and access to the TeraGrid-wide GPFS-WAN file-system.",IA-64 Cluster Test,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
173,,decommissioned,pople.psc.teragrid.org,psc.teragrid.org,decommissioned,2011-09-15,,"{'latitude': None, 'node_count': 384.0, 'manufacturer': 'SGI', 'rmax': None, 'interconnect': 'SGI NUMAlink', 'is_publicly_available': True, 'peak_teraflops': 5.0, 'advance_max_reservable_su': 768.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530132, 'conversion_factors': [{'begin_date': '2008-06-29', 'factor': 1.4849999999999999, 'conversion': 'Teragrid', 'end_date': '2011-08-31'}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-PSC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'PSC SGI Altix (Pople)', 'allocations_description': None, 'allocable_id': 126438}, 'job_manager': '', 'batch_system': 'PBS', 'platform_name': 'Altix 4700', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': None, 'community_software_area_email': 'raymond@psc.edu', 'login_hostname': 'tg-login.pople.psc.teragrid.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'dual-core Intel Itanium', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 768.0, 'nickname': 'Pople', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': '', 'user_guide_url': 'http://www.psc.edu/machines/sgi/altix/pople.php', 'disk_size_tb': 150.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.67, 'cpu_count_per_node': 2, 'operating_system': 'SuSE  Linux', 'community_software_area': True}",26.0,XSEDE,XSEDE Level 1,126438,compute,Pople is intended for applications utilizing shared memory and hybrid architectures.,"Pople is an SGI Altix 4700 comprising 192 blades with 8 GB of memory and 2 sockets on each blade.  Each socket is a 1.66GHz dual-core Intel Itanium 2 processor (Montvale).  Pople has a total of 384 sockets, 768 cores and 1.5 TB (2GB per core) of RAM.

The blades are linked with a NUMAlink interconnect.",PSC SGI Altix (Pople),"{'coming_soon_begin_date': '2007-12-31', 'retired_begin_date': '2008-01-01', 'decommissioned_begin_date': '2011-09-15', 'friendly_begin_date': None, 'post_production_end_date': '2011-09-14', 'coming_soon_end_date': '2008-06-29', 'post_production_begin_date': '2011-09-01', 'retired_end_date': '2008-08-31', 'production_end_date': '2011-08-31', 'friendly_end_date': None, 'production_begin_date': '2008-07-15', 'decommissioned_end_date': None, 'pre_production_begin_date': '2008-06-30', 'pre_production_end_date': '2008-07-14'}",
174,,decommissioned,spur.tacc.teragrid.org,tacc.teragrid.org,decommissioned,2013-02-04,,"{'organizations': [{'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,42,resource,,"Spur, the TACC Sun Visualization Cluster, consists of 8 nodes, each with significant computing and graphics resources.  Total system resources include 128 compute cores, 1 TB distributed memory and 32 NVIDIA FX5600 GPUs.

The login node is a Sun Fire X4600 server with 8 dual-core AMD Opteron processors, 256 GB memory, and 2 NVIDIA Quadro Plex model 4 graphics cards.  Compute nodes include: 1 Sun Fire X4400 server with 4 quad-core AMD Opteron processors, 128 GB memory, and 2 NVIDIA Quadro Plex model 4 graphics cards  and 6 Sun Fire x4400 servers, each with 4 quad-core AMD Opteron processors, 128 GB memory, and an NVIDIA Quadro Plex S4 graphics card.

Spur shares the InfiniBand interconnect and Lustre Parallel file systems of the TACC Sun Constellation Cluster, Ranger.  Thus, Spur acts not only as a powerful, stand-alone visualization system: it also enables researchers to perform visualization tasks on Ranger-generated data without migrating to another file system and to integrate simulation and rendering tasks on a single network fabric.",TACC Sun Visualization System (Spur),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-02-04', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2013-02-03', 'friendly_end_date': None, 'production_begin_date': '2008-10-13', 'decommissioned_end_date': None, 'pre_production_begin_date': '2008-01-01', 'pre_production_end_date': '2008-10-12'}",
175,,decommissioned,gordonio.sdsc.xsede.org,sdsc.xsede.org,decommissioned,2016-08-31,,"{'latitude': None, 'node_count': 64.0, 'manufacturer': 'Appro', 'rmax': None, 'interconnect': '', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': False, 'allocations_info': {'unit_type': 'TeraBytes', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530156, 'conversion_factors': [{'begin_date': None, 'factor': 0.0, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '(One I/O node = 4.8 TB)', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '(One I/O node = 4.8 TB)', 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '(One I/O node = 4.8 TB)', 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '(One I/O node = 4.8 TB)', 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '(One I/O node = 4.8 TB)', 'allocation_state': 'Supplement, Appeal, Extension, Transfer Only', 'maximum_amount': 1.0, 'allocation_type': 'Startup', 'dollar_value': None}], 'display_resource_name': 'SDSC Appro with Intel Flash I/O Nodes (Gordon ION)', 'allocations_description': '<span class=""important"">(One I/O node = 4.8 TB)</span>', 'allocable_id': 142285}, 'job_manager': '', 'batch_system': '', 'platform_name': '', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '70TB home filesystems (4 servers), 1.6 PB /oasis/scratch, 1.36 PB projects filesystem.', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 4.0, 'cpu_type': 'Intel Westmere', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Gordon ION', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': '1.6PB Lustre Scratch filesystem, 1.36 PB Lustre projects filesystem', 'user_guide_url': 'https://portal.xsede.org/sdsc-gordon', 'disk_size_tb': 1628.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.66, 'cpu_count_per_node': 12, 'operating_system': '', 'community_software_area': False}",47.0,XSEDE,XSEDE Level 1,142285,compute,"Gordon ION is particularly well suited to database and data-mining applications where high levels of I/O concurrency exist or where the I/O is dominated by random access data patterns. The resource should be of interest to those who, for example, want to provide a high-performance query engine for scientific or other community databases. Consequently, Gordon ION allocations are for long-term, dedicated use of the awardee.

You may request up to 2 I/O nodes, though it is expected that most will request one unless scaling can be demonstrated that justifies 2. You should also request dedicated compute nodes if they are part of the application architecture. One compute node will be provided for each I/O node unless there is justification for more. In any case, no more than 16 compute nodes will be provided per I/O node, or 32 for allocations of 2 I/O nodes.  Whereas the Gordon Compute Cluster is allocated in units of SUs, the Gordon ION resource is allocated in units of storage.  A request for I/O node should specify 4 TB, while that for 2 I/O nodes would be for 8 TB.

Successful allocation requests must describe how you will make use of the I/O nodes. This should include relevant benchmarks on spinning disks, with projections of how the applications will scale when using flash drives. Additionally, the request should include a strong justification for why these should be provided as a dedicated resource—for example, providing long-term access to data for a large community.

If you are new to high-performance computing and do not yet have benchmarking data to support the use of Gordon ION, we encourage you to apply for a startup allocation.

If you require special help with using the Gordon ION resource, we encourage you to request Advanced User Support.

You may also apply for time on the Gordon Compute Cluster.

Accepting applications September 15 through October 15 for awards starting on January 1, 2012.","There are 64 I/O nodes in the Gordon compute cluster, and some are available as a dedicated resource outside the batch scheduler. You must request allocations for these separately from your Gordon Compute Cluster allocation. This I/O node resource is referred to as Gordon ION.

Gordon ION is a 64-node, flash-based I/O resource that is an integral part of Gordon Compute Cluster, as well as a distinct and individually allocated resource. Each node is composed of two hex-core, 2.66GHz Westmere processors, 48GB of DRAM memory, and 4 TB of high-performance Intel flash memory capable of delivering over 560k I/O IOPS.  Each I/O node can access Data Oasis, SDSC’s 4 PB Lustre-based file system, via two 10 GbE connections. This results in a sustained aggregate bandwidth into Gordon ION through all 64 nodes of 100 GB/s.",SDSC Appro with Intel Flash I/O Nodes (Gordon ION),"{'coming_soon_begin_date': '2011-09-01', 'retired_begin_date': None, 'decommissioned_begin_date': '2016-08-31', 'friendly_begin_date': None, 'post_production_end_date': '2016-08-30', 'coming_soon_end_date': '2011-12-31', 'post_production_begin_date': '2014-09-14', 'retired_end_date': None, 'production_end_date': '2014-09-13', 'friendly_end_date': None, 'production_begin_date': '2012-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-02-15 01:43:13.073
176,,production,bridges.pylon.psc.xsede.org,pylon.psc.xsede.org,production,2016-01-01,2019-11-30,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,95,resource,,Storage for Bridges Projects,PSC Storage (Bridges Pylon),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2019-11-30', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-11-30', 'friendly_end_date': None, 'production_begin_date': '2016-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-02-14 18:48:28.542
177,,pre-production,bridges.gpu.psc.xsede.org,gpu.psc.xsede.org,pre-production,,,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,127,resource,,"Bridges contains two kinds of GPU nodes: NVIDIA Tesla K80s and NVIDIA Tesla P100s.  Because of the difference in the performance of the nodes, the charges will be different for the two types of nodes.  The K80 nodes hold 4 GPU units each, each of which can be allocated separately.  Service units (SUs) are defined in terms of GPU-hours: 
1 GPU-hour = 1 SU Note that the use of an entire K80 GPU node for one hour would be charged 4 SUs. The P100 nodes hold 2 GPU units each, which can be allocated separately.  Service units (SUs) are defined in terms of GPU-hours: 1 GPU-hour = 2.5 SUs Note that the use of an entire P100 node for one hour would be charged 5 SUs.",PSC Bridges GPU (Bridges GPU),"{'coming_soon_begin_date': '2017-03-13', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2017-03-14', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-11-30', 'friendly_end_date': None, 'production_begin_date': '2017-07-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2017-03-15', 'pre_production_end_date': '2017-06-30'}",2017-03-16 12:42:52.952
178,,,arc-ts.umich.xsede.org,umich.xsede.org,,,,"{'latitude': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'specifications': '', 'is_publicly_available': False, 'sensitive_data_support_description': 'PHI/HIPAA\r\nFERPA\r\nSSN\r\n\r\nhttps://www.safecomputing.umich.edu/dataguide/', 'user_guide_url': 'http://arc-ts.umich.edu/systems-and-services/turbo/', 'is_accounted': False, 'xsedenet_participant': None, 'databases': '', 'file_space_tb': None, 'supports_sensitive_data': True}",129.0,XSEDE,XSEDE Level 3,144590,storage,"Turbo is a high-capacity, fast, reliable, and secure data storage service that allows investigators across U-M to connect their data to the computing resources necessary for their research, including U-M’s Flux HPC cluster. Turbo supports storage of sensitive data and ARC-TS’s Armis cluster.

Turbo can only be used for research data. It is tuned for large files (1MB or greater) but is capable of handling small files such as documents, spreadsheets, etc. Turbo in combination with Globus sharing should work well for sharing and hosting data for external collaborators and institutes.",,Turbo Research Storage,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-17 20:42:39.004
179,,production,comet.gpu.sdsc.xsede.org,gpu.sdsc.xsede.org,production,2015-04-01,2019-01-30,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,128,resource,,"Comet has 36 general purpose GPU nodes, with 2 Tesla K80 GPU graphics cards per node, each with 2 GK210 GPUs (144 GPUs in total). Each GPU node also features 2 Intel Haswell processors of the same design and performance as the standard compute nodes (described separately). The GPU nodes are integrated into the Comet resource and available through the Slurm scheduler for either dedicated or shared node jobs (i.e., a user can run on 1 or more GPUs/node and will be charged accordingly). Like the Comet standard compute nodes, the GPU nodes feature a local SSD which can be specified as a scratch resource during job execution – in many cases using SSD’s can alleviate I/O bottlenecks associated with using the shared Lustre parallel file system.",SDSC Comet GPU Nodes (Comet GPU),"{'coming_soon_begin_date': '2014-09-12', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2014-09-13', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-01-30', 'friendly_end_date': None, 'production_begin_date': '2015-04-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2014-09-14', 'pre_production_end_date': '2015-03-31'}",2017-03-15 18:38:50.564
180,,decommissioned,condor.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2013-07-31,,"{'organizations': [{'organization_abbreviation': 'Purdue U', 'organization_name': 'Purdue University', 'organization_code': '0018259'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,37,resource,,"The Purdue Condor pools consist of over 14000 CPUs of computation: 8000 LINUX/X86_64 CPUs, 400 LINUX/INTEL (ia32) CPUs, and 5000 WINNT51/INTEL CPUs, as well as a small number of Itanium Linux, Solaris and MacOS X machines. Memory on compute nodes range from 512 MB to 32 GB, and most CPUs run at 3 GHz or better. With a total of over 60 TFLOPS available, the Purdue Condor pools can provide large numbers of cycles in a short amount of time. All shared areas and software packages available on Steele are available on Condor.",Purdue Condor Pool,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2013-07-30', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
181,,decommissioned,wispy.purdue.teragrid.org,purdue.teragrid.org,decommissioned,2013-07-31,,"{'organizations': [{'organization_abbreviation': 'Purdue U', 'organization_name': 'Purdue University', 'organization_code': '0018259'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,44,resource,,"Wispy is an experimental Nibus Compute Cloud. It consists of 32 64bit, 4-core HP DL140 nodes which are all connected using 1 Gigabit Ethernet (1GigE).

",Purdue Cloud (Wispy),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2013-07-30', 'friendly_end_date': None, 'production_begin_date': '2010-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
182,,decommissioned,greenfield.psc.xsede.org,psc.xsede.org,decommissioned,2016-12-31,,"{'organizations': [{'organization_abbreviation': 'PSC', 'organization_name': 'Pittsburgh Supercomputing Center', 'organization_code': '6000715'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,62,resource,,"Greenfield comprises an HP Superdome and  two HP DL580s. 

The Superdome contains 240 cores and 12TB of memory; the HP DL580s have 60 cores and 3 TB each. ",PSC HP Superdome and HP DL580 (Greenfield) ,"{'coming_soon_begin_date': '2015-07-15', 'retired_begin_date': None, 'decommissioned_begin_date': '2016-12-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2015-08-04', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2016-12-31', 'friendly_end_date': None, 'production_begin_date': '2015-08-05', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-02-13 18:13:59.753
183,"GPFS-WAN is currently mounted on the following machines:
IU: tg-login.iu.teragrid.org (Big Red PPC Linux Cluster)
    ",decommissioned,gpfs-wan.iu.teragrid.org,iu.teragrid.org,decommissioned,2011-07-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'user_guide_url': '', 'is_accounted': True, 'xsedenet_participant': None, 'databases': 'Not Applicable', 'file_space_tb': 700.0, 'supports_sensitive_data': None}",78.0,XSEDE,XSEDE Level 1,140925,storage,"TeraGrid GPFS-WAN (Global Parallel File System-Wide Area Network) is a large-scale storage system mounted on several TeraGrid platforms. Although the system is physically located at SDSC, it looks lik",,GPFS-WAN,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-06-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
184,,production,supermic.cct-lsu.xsede.org,cct-lsu.xsede.org,production,2014-10-01,,"{'organizations': [{'organization_abbreviation': 'LSU CCT', 'organization_name': 'LSU Center for Computation and Technology', 'organization_code': 'T103901'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 2,65,resource,,"SuperMIC is a 925 TFlop Peak Performance Xeon Phi accelerated cluster.  SuperMIC has 360 nodes each with 20 Intel Ivybridge 2.8 GHz cores, 64 GB of RAM, and two Intel Xeon Phi 7120P co-processors. There are 20 nodes that have NVIDIA K20X GPUs. This cluster is 40% allocated to the XSEDE user community and 60% dedicated to authorized users of the LSU community.  Access is restricted to those who meet the criteria as stated on our website.",LSU Cluster (superMIC),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2014-10-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2014-09-01', 'pre_production_end_date': '2014-09-30'}",2016-04-14 18:10:00.630
185,,decommissioned,ranger.tacc.teragrid.org,tacc.teragrid.org,decommissioned,2013-02-04,,"{'organizations': [{'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,50,resource,,"The Ranger Sun Constellation Cluster is configured with 3,936 four-socket, quad-core AMD Opteron nodes (62,976 compute cores) and 125 TB of distributed memory.  With each core clocked at 2.3 GHz and capable of four flops/clock cycle, Ranger provides the user community access to a resource with a theoretical peak performance of 579.3 TFLOPS.  Multiple shared file systems (HOME, WORK, and PROJECTS) are configured from 1.7 PB of raw storage.  All file systems are managed via the Lustre Parallel File System.  Nodes are interconnected with InfiniBand technology with two non-blocking Sun Magnum switches acting as the core of the fabric.",TACC Sun Constellation Cluster (Ranger),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-02-04', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2013-02-03', 'friendly_end_date': None, 'production_begin_date': '2008-02-04', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
186,,friendly,lonestar4.tacc.teragrid.org,tacc.teragrid.org,friendly,2015-01-01,,"{'latitude': None, 'node_count': 1888.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'QDR InfiniBand', 'is_publicly_available': True, 'peak_teraflops': 302.0, 'advance_max_reservable_su': None, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530151, 'conversion_factors': [{'begin_date': None, 'factor': 2.09, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 50000.0, 'allocation_type': 'Startup', 'dollar_value': 0.015534073}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.015534073}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': 0.015534073}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 0.015534073}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.015534073}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.015534073}], 'display_resource_name': 'TACC Dell PowerEdge Westmere Linux Cluster (Lonestar)', 'allocations_description': None, 'allocable_id': 142271}, 'job_manager': '', 'batch_system': 'SGE', 'platform_name': 'PowerEdge Westmere Linux Cluster', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'tg-support@tacc.utexas.edu', 'login_hostname': 'tg-login.lonestar.tacc.teragrid.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Xeon, Westmere, dual processor, 6-core', 'alternate_login_hostname': 'lonestar.tacc.utexas.edu', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Lonestar4', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://portal.xsede.org/tacc-lonestar', 'disk_size_tb': 1000.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 3.3, 'cpu_count_per_node': 12, 'operating_system': 'Linux (CentOS)', 'community_software_area': True}",59.0,XSEDE,XSEDE Level 1,142271,compute,Lonestar is intended primarily for parallel applications scalable to thousands of cores.  Normal batch queues will enable users to run simulations up to 24 hours.  Jobs requiring run times and more cores that allowed by the normal queues will be run in a special queue after approval of TACC staff.  Serial and development queues will also be configured.,"The new Lonestar Dell PowerEdge Westmere Linux Cluster is configured with 1,888 Dell M610 blades, each with two 3.3 GHz Westmere processors.  With 24 GB of memory and 146 GB of storage per node, users have access to an aggregate of 44 TB of memory and 276 TB of local storage.  Lonestar also provides access to five large memory (1TB) nodes, and eight nodes containing two NVIDIA GPU's, giving users access to high-throughput computing and remote visualization capabilities respectively.  Compute nodes have access to a PB Lustre Parallel file system.   A QDR InfiniBand switch fabric interconnects the nodes through a fat-tree topology with a point-to-point bandwidth of 40GB/sec (unidirectional speed).",TACC Dell PowerEdge Westmere Linux Cluster (Lonestar),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2014-12-31', 'friendly_begin_date': '2015-01-01', 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2014-06-29', 'friendly_end_date': None, 'production_begin_date': '2011-02-01', 'decommissioned_end_date': '2014-12-31', 'pre_production_begin_date': None, 'pre_production_end_date': None}",
187,,production,xstream.stanford.xsede.org,stanford.xsede.org,production,2016-06-01,,"{'latitude': None, 'node_count': 65.0, 'manufacturer': 'Cray', 'rmax': 781.3, 'interconnect': 'Infiniband FDR', 'is_publicly_available': False, 'peak_teraflops': 1001.7, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530197, 'conversion_factors': [{'begin_date': '2016-06-01', 'factor': 85.0, 'conversion': 'Teragrid', 'end_date': '2019-05-31'}], 'allocation_type_specifics': [], 'display_resource_name': 'Stanford Cray CS-Storm K80 (XStream)', 'allocations_description': 'SUs are GPU hours', 'allocable_id': 144551}, 'job_manager': 'Slurm', 'batch_system': 'Slurm', 'platform_name': 'CS-Storm', 'nfs_network': 'Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': False, 'model': 'CS-Storm', 'primary_storage_shared_gb': '3500', 'rpeak': 1001.7, 'machine_type': 'Cluster', 'gpu_description': 'NVIDIA Tesla K80', 'community_software_area_email': '', 'login_hostname': 'xstream-login.stanford.edu', 'storage_network': 'Infiniband', 'graphics_card': 'NVIDIA Tesla K80', 'is_visualization': False, 'memory_per_cpu_gb': 12.8, 'cpu_type': 'Intel Xeon E5-2680 v2 (Ivy-Bridge)', 'alternate_login_hostname': 'xs.stanford.edu', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'XStream', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://portal.xsede.org/stanford-xstream', 'disk_size_tb': 1400.0, 'local_storage_per_node_gb': 450.0, 'cpu_speed_ghz': 2.8, 'cpu_count_per_node': 20, 'operating_system': 'Linux', 'community_software_area': False}",57.0,XSEDE,XSEDE Level 2,144551,compute,GPU computing,"Stanford University GPU Supercomputer (Cray CS-Storm, Intel Ivy-Bridge, NVIDIA K80)",Stanford University GPU Cluster (XStream),"{'coming_soon_begin_date': '2015-03-01', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2016-03-31', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2016-06-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2016-04-01', 'pre_production_end_date': '2016-05-31'}",2016-08-04 15:23:59.358
188,,production,futuregrid0.futuregrid.xsede.org,futuregrid.xsede.org,production,2013-01-01,,"{'latitude': None, 'user_guide_url': '', 'is_accounted': False, 'is_authenticated': False, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'OtherResource', 'allocable_resource_id': 530161, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': 'Suggested amount for a New user is 10,000', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Software Testbeds', 'dollar_value': None}], 'display_resource_name': 'FutureGrid Testbed (FutureGrid)', 'allocations_description': '<span class=""important"">Suggested amount for a New user is 10,000</span>', 'allocable_id': 142313}, 'longitude': None, 'supports_sensitive_data': None, 'is_authorized': False, 'is_publicly_available': False, 'sensitive_data_support_description': None, 'xsedenet_participant': None}",97.0,XSEDE,XSEDE Level 1,142313,other,,FutureGrid Testbed (FutureGrid),FutureGrid Testbed (FutureGrid),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2013-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
189,hsi and htar interfaces,,hpss.nics.xsede.org,nics.xsede.org,,,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': ""IBM's High Performance Storage System (HPSS)"", 'is_publicly_available': True, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'GigaBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530166, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}], 'display_resource_name': 'NICS Long-term tape Archival Storage (HPSS)', 'allocations_description': None, 'allocable_id': 142300}, 'is_accounted': True, 'user_guide_url': 'https://portal.xsede.org/nics-hpss', 'databases': '', 'supports_sensitive_data': None, 'file_space_tb': 17000.0, 'xsedenet_participant': None}",89.0,XSEDE,XSEDE Level 1,142300,storage,"Use of NICS HPSS is for archival storage of intermediate data, long term storage of results and NICS center backups",,NICS Long-term tape Archival Storage (HPSS),"{'coming_soon_begin_date': None, 'retired_begin_date': '2013-01-01', 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': '2013-06-28', 'production_end_date': '2014-09-30', 'friendly_end_date': None, 'production_begin_date': '2013-07-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
190,,decommissioned,hpss.iu.teragrid.org,iu.teragrid.org,decommissioned,2011-07-31,,"{'latitude': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'specifications': '', 'is_publicly_available': False, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'TeraBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530114, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}], 'display_resource_name': 'Indiana University HPSS Archival Storage', 'allocations_description': None, 'allocable_id': 142315}, 'is_accounted': False, 'user_guide_url': '', 'databases': '', 'supports_sensitive_data': None, 'file_space_tb': None, 'xsedenet_participant': None}",83.0,XSEDE,XSEDE Level 1,142315,storage,HPSS,,Indiana University HPSS Archival Storage,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
191,,decommissioned,darter.nics.xsede.org,nics.xsede.org,decommissioned,2016-07-01,,"{'organizations': [{'organization_abbreviation': 'NICS', 'organization_name': 'National Institute for Computational Sciences', 'organization_code': 'T103349'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,55,resource,,"Darter is a Cray XC30 (Cascade) supercomputer that runs the Cray Linux Environment (CLE) 5.0 upo 3 based on SLES 11.  It has 11,968 physical compute cores (23,936 logical cores with Hyper-Threading enabled), 24 TB of compute memory,    the interconnect is a Dragonfly network topology from Cray Aries technology.  Darter has 748 compute nodes with a peak performance of 250 Tflops. Darter has 334TB parallel Lustre file system for scratch on Cray Sonexion hardware,    2 login nodes with 10GigE uplinks and long term archival/storage available through HPSS
",NICS Cray XC30 - Darter,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2016-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2016-06-30', 'friendly_end_date': None, 'production_begin_date': '2014-05-15', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-04-11 16:46:05.927
192,,production,maverick.tacc.xsede.org,tacc.xsede.org,production,2014-03-03,,"{'organizations': [{'organization_abbreviation': 'UT Austin', 'organization_name': 'University of Texas at Austin', 'organization_code': '0036582'}, {'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,56,resource,,"The new Maverick HP/NVIDIA Interactive Visualization and Data Analytics System is configured with 132 HP ProLiant SL250s Gen8 compute nodes and 132 NVIDIA Tesla K40 GPU accelerators. In addition, with 256 GB of memory and 500 GB of storage per node, users have access to an aggregate of 33.7 TB of memory and 66 TB of local storage.. Compute nodes have access to a 20 PB Lustre Parallel file system, Stockyard.  An FDR InfiniBand switch fabric interconnects the nodes facilitating high-speed internode communication and I/O traffic.",HP/NVIDIA Interactive Visualization and Data Analytics System (Maverick),"{'coming_soon_begin_date': '0013-01-01', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '0013-12-14', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2014-03-03', 'decommissioned_end_date': None, 'pre_production_begin_date': '2013-12-15', 'pre_production_end_date': '2014-03-02'}",2016-04-18 20:42:48.667
193,,production,jetstream.tacc.xsede.org,tacc.xsede.org,production,2016-01-20,2019-11-30,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}, {'organization_abbreviation': 'UT Austin', 'organization_name': 'University of Texas at Austin', 'organization_code': '0036582'}, {'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,68,resource,,Jetstream is a user-friendly cloud environment designed to give researchers and students access to computing and data analysis resources on demand.,IU/TACC (Jetstream),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2019-11-30', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-11-30', 'friendly_end_date': None, 'production_begin_date': '2016-01-20', 'decommissioned_end_date': None, 'pre_production_begin_date': '2015-11-01', 'pre_production_end_date': '2016-01-19'}",2016-05-17 18:54:59.989
194,,,arc-ts.umich.xsede.org,umich.xsede.org,,,,"{'latitude': None, 'sensitive_data_support_description': 'FISMA Moderate\r\nHIPAA/PHI\r\nFERPA\r\nSSN', 'is_accounted': False, 'is_authenticated': False, 'xsedenet_participant': None, 'is_authorized': False, 'supports_sensitive_data': True, 'is_publicly_available': False, 'user_guide_url': 'http://arc-ts.umich.edu/systems-and-services/yottabyte/', 'longitude': None}",129.0,XSEDE,XSEDE Level 3,144594,other,"Virtual Desktop
Databases
Sensitive Data Enclaves","The Yottabyte Research Cloud is a partnership between ARC and Yottabyte that will provide U-M researchers with high performance, secure and flexible computing environments that enable the analysis of sensitive data sets restricted by federal privacy laws, proprietary access agreements, or confidentiality requirements.

The system is built on Yottabyte’s software-defined infrastructure, known as yCenter and YottaBlox, and represents U-M’s first use of software-defined infrastructure for research, allowing the on-the-fly personalized configuration of any-scale computing resources. Yottabyte donated $5.5 million worth of hardware and software to U-M to setup the system, and U-M contributed another $2 million to support delivery of services to researchers and for general operations.

yCenter SDI software translates the physical CPU, RAM and storage components of YottaBlox appliances into definable and configurable virtual resource groups that may be used to build multi-tenant, multi-site cloud infrastructures. Each yCenter instance manages clusters of physical hyper-converged, compute, storage and network fabric YottaBlox. These resources are organized and represented virtually into one or many virtual nodes, tenants, virtual datacenters (VDC) and virtual machine (VM) resource containers.",Yottabyte Research Cloud,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-17 20:48:52.817
195,,decommissioned,spur.tacc.teragrid.org,tacc.teragrid.org,decommissioned,2013-02-04,,"{'latitude': None, 'node_count': 8.0, 'manufacturer': 'Sun', 'rmax': None, 'interconnect': 'InfiniBand', 'is_publicly_available': True, 'peak_teraflops': 1.13, 'advance_max_reservable_su': 64.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530135, 'conversion_factors': [{'begin_date': '2008-10-13', 'factor': 1.8519999999999999, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'TACC Sun Visualization System (Spur)', 'allocations_description': None, 'allocable_id': 126624}, 'job_manager': '', 'batch_system': 'SGE', 'platform_name': 'Visualization Cluster', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '1.73 PB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'tg-support@tacc.utexas.edu', 'login_hostname': 'tg-login.spur.tacc.teragrid.org', 'storage_network': 'InfiniBand', 'graphics_card': '', 'is_visualization': True, 'memory_per_cpu_gb': 8.0, 'cpu_type': 'AMD Opteron', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 64.0, 'nickname': 'Spur', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'Lustre Parallel File System', 'user_guide_url': 'https://www.xsede.org/web/guest/tacc-spur', 'disk_size_tb': 1730.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.3, 'cpu_count_per_node': 16, 'operating_system': 'Linux (CentOS)', 'community_software_area': True}",42.0,XSEDE,XSEDE Level 1,126624,compute,"Spur is intended for serial and parallel visualization applications that take advantage of large per-node memory, multiple computing cores, and multiple graphics processors.  Spur is also an ideal visualization resource for researchers that use Ranger since data produced on Ranger can be visualized directly on Spur with no data migration.","Spur, the TACC Sun Visualization Cluster, consists of 8 nodes, each with significant computing and graphics resources.  Total system resources include 128 compute cores, 1 TB distributed memory and 32 NVIDIA FX5600 GPUs.

The login node is a Sun Fire X4600 server with 8 dual-core AMD Opteron processors, 256 GB memory, and 2 NVIDIA Quadro Plex model 4 graphics cards.  Compute nodes include: 1 Sun Fire X4400 server with 4 quad-core AMD Opteron processors, 128 GB memory, and 2 NVIDIA Quadro Plex model 4 graphics cards  and 6 Sun Fire x4400 servers, each with 4 quad-core AMD Opteron processors, 128 GB memory, and an NVIDIA Quadro Plex S4 graphics card.

Spur shares the InfiniBand interconnect and Lustre Parallel file systems of the TACC Sun Constellation Cluster, Ranger.  Thus, Spur acts not only as a powerful, stand-alone visualization system: it also enables researchers to perform visualization tasks on Ranger-generated data without migrating to another file system and to integrate simulation and rendering tasks on a single network fabric.",TACC Sun Visualization System (Spur),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-02-04', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2013-02-03', 'friendly_end_date': None, 'production_begin_date': '2008-10-13', 'decommissioned_end_date': None, 'pre_production_begin_date': '2008-01-01', 'pre_production_end_date': '2008-10-12'}",
196,"GPFS-WAN is currently mounted on the following machines:
IU: tg-login.iu.teragrid.org (Big Red PPC Linux Cluster),SDSC: tg-login.sdsc.te",decommissioned,gpfs-wan.ncar.teragrid.org,ncar.teragrid.org,decommissioned,2011-07-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'user_guide_url': None, 'is_accounted': True, 'xsedenet_participant': None, 'databases': 'Not applicable', 'file_space_tb': 700.0, 'supports_sensitive_data': None}",79.0,XSEDE,XSEDE Level 1,141292,storage,"TeraGrid GPFS-WAN (Global Parallel File System-Wide Area Network) is a large-scale storage system mounted on several TeraGrid platforms. Although the system is physically located at SDSC, it appears to the user as local.",,GPFS-WAN,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-06-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
197,,decommissioned,datastar-p655.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2008-10-01,,"{'latitude': None, 'node_count': 96.0, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': 'IBM Federation', 'is_publicly_available': True, 'peak_teraflops': 14.3, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530099, 'conversion_factors': [{'begin_date': '2006-03-31', 'factor': 0.914, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-SDSC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}], 'display_resource_name': 'SDSC DataStar IBM p655 (8-way)', 'allocations_description': None, 'allocable_id': 125568}, 'job_manager': 'jobmanager-fork\n\njobmanager-loadleveler', 'batch_system': 'LoadLeveler', 'platform_name': 'pSeries 655', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '2 TB', 'rpeak': None, 'machine_type': 'MPP', 'gpu_description': None, 'community_software_area_email': None, 'login_hostname': 'dslogin.sdsc.edu', 'storage_network': 'Fibre Channel', 'graphics_card': None, 'is_visualization': False, 'memory_per_cpu_gb': 4.0, 'cpu_type': 'IBM Power4+', 'alternate_login_hostname': None, 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'DataStar', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': None, 'parallel_file_system': 'GPFS: 130 TB', 'user_guide_url': 'http://www.sdsc.edu/us/resources/datastar/', 'disk_size_tb': 115.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.7000000000000002, 'cpu_count_per_node': 8, 'operating_system': 'AIX 5.2L', 'community_software_area': False}",1.0,XSEDE,XSEDE Level 1,125568,compute,"[DECOMM]The DataStar p655 partition is primarily intended to run applications of very high levels of parallelism or concurrency, especially those with high parallel I/O requirements. The queuing policies favor jobs with a higher processor count.
<br /><strong>Status</strong>

Decommissioned 10/1/2008","DataStar has 272 8-way P655+ compute nodes -- 176 nodes with 1.5-GHz Power4+ CPUs and 16 GB of memory, and 96 with 1.7 GHz Power4+ CPUs and 32 GB of memory. The nodes are connected by an IBM high-speed Federation switch and have access to 130 TB of GPFS.","SDSC IBM p655 (DataStar, 8-way)","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2008-10-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
198,,production,xwfs.tacc.xsede.org,tacc.xsede.org,production,2013-07-01,,"{'latitude': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'specifications': '', 'is_publicly_available': False, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'GigaBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530167, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'Transfers Only', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'Transfers Only', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': None}], 'display_resource_name': 'XSEDE-Wide File System (XWFS)', 'allocations_description': None, 'allocable_id': 142301}, 'is_accounted': False, 'user_guide_url': 'https://portal.xsede.org/xwfs', 'databases': '', 'supports_sensitive_data': None, 'file_space_tb': 720.0, 'xsedenet_participant': None}",91.0,XSEDE,XSEDE Level 1,142301,storage,"shared, distributed data storage and access",,XSEDE-Wide File System (XWFS),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2013-07-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
199,,decommissioned,abe.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2011-04-15,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,19,resource,,"This Dell blade system has 1,200 PowerEdge 1955 dual socket, quad core compute blades, an InfiniBand interconnect and 100 TB of storage in a Lustre filesystem.",Abe,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-04-15', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-04-14', 'friendly_end_date': None, 'production_begin_date': '2007-07-09', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
200,,decommissioned,kraken.nics.teragrid.org,nics.teragrid.org,decommissioned,2014-05-01,,"{'latitude': None, 'node_count': 9408.0, 'manufacturer': 'Cray', 'rmax': None, 'interconnect': 'Cray SeaStar - 3D Torus', 'is_publicly_available': True, 'peak_teraflops': 1174.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530131, 'conversion_factors': [{'begin_date': '2011-02-14', 'factor': 2.04, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': 0.017022613000000002}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'Transfers Only', 'maximum_amount': 200000.0, 'allocation_type': 'Startup', 'dollar_value': 0.017022613000000002}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': 0.017022613000000002}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-Kraken', 'dollar_value': 0.017022613000000002}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.017022613000000002}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': 0.017022613000000002}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 0.017022613000000002}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.017022613000000002}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'Transfers Only', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.017022613000000002}], 'display_resource_name': 'NICS Cray XT5 (Kraken)', 'allocations_description': None, 'allocable_id': 126758}, 'job_manager': '', 'batch_system': 'Torque/Moab', 'platform_name': 'XT5', 'nfs_network': 'GigE', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '2400 Tbytes', 'rpeak': None, 'machine_type': 'MPP', 'gpu_description': None, 'community_software_area_email': 'help@teragrid.org', 'login_hostname': 'kraken.nics.xsede.org', 'storage_network': 'lustre file system on DDN disk', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 1.3333333333000001, 'cpu_type': 'AMD', 'alternate_login_hostname': 'kraken-gsi.nics.xsede.org', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Kraken-XT5', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://www.xsede.org/web/guest/nics-kraken', 'disk_size_tb': 2400.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.6, 'cpu_count_per_node': 12, 'operating_system': 'Compute Node Linux (CNL) 2.2', 'community_software_area': True}",29.0,XSEDE,XSEDE Level 1,126758,compute,Kraken is intended for highly scalable parallel applications.,"The Kraken system is a Cray XT5 with 9,408
compute nodes interconnected with SeaStar, a 3D torus. Each compute node has two hex-core AMD Opterons for a total of 112,896 cores. All nodes have 16 Gbytes of memory: 4/3 Gbyte of memory per core.",NICS Cray XT5 (Kraken),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2014-05-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2014-04-30', 'friendly_end_date': None, 'production_begin_date': '2009-10-05', 'decommissioned_end_date': None, 'pre_production_begin_date': '2009-01-01', 'pre_production_end_date': '2009-10-04'}",
201,,decommissioned,dtf.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2010-03-31,,"{'latitude': None, 'node_count': 631.0, 'manufacturer': 'Intel', 'rmax': None, 'interconnect': 'Myrinet', 'is_publicly_available': True, 'peak_teraflops': 10.23, 'advance_max_reservable_su': 1774.0, 'is_accounted': True, 'sensitive_data_support_description': None, 'job_manager': 'jobmanager (default)jobmanager-pbs', 'batch_system': '', 'platform_name': 'IA-64 Cluster', 'nfs_network': 'Gigabit Ethernet', 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '1 TB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'mikep@ncsa.uiuc.edu', 'login_hostname': 'login-hg.ncsa.teragrid.org', 'storage_network': 'Fibre Channel', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Intel Itanium2 - Madison', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 1774.0, 'nickname': 'Mercury', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'GPFS: 170 TB', 'user_guide_url': 'http://www.ncsa.uiuc.edu/UserInfo/Resources/Hardware/TGIA64LinuxCluster/', 'disk_size_tb': 60.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.5, 'cpu_count_per_node': 2, 'operating_system': 'Linux 2.4.21-SMP', 'community_software_area': True}",43.0,XSEDE,XSEDE Level 1,125808,compute,"The NCSA IA-64 Linux Cluster (mercury) is primarily intended to run applications of moderate to high levels of parallelism, particularly those needing a 64-bit environment and codes that perform well in a distributed cluster environment. 
<br />
Note: mercury is co-allocated with the SDSC and ANL IA-64 Clusters.","NCSA       's IA-64 TeraGrid Linux Cluster consists of 887 IBM nodes: 256 nodes with dual 1.3 GHz Intel Itanium 2 processors (half with 4 GB of memory per node, and the other half with 12 GB of memory per node), and 631 nodes with dual 1.5 GHz Intel Itanium 2 processors (4 GB of memory per node). The cluster is running SuSE Linux and is using Myricom       's Myrinet cluster interconnect network, and the GPFS parallel filesystem.",NCSA DTF,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-03-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-02-28', 'friendly_end_date': None, 'production_begin_date': '2008-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
202,,friendly,lonestar4.tacc.teragrid.org,tacc.teragrid.org,friendly,2015-01-01,,"{'organizations': [{'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,59,resource,,"The new Lonestar Dell PowerEdge Westmere Linux Cluster is configured with 1,888 Dell M610 blades, each with two 3.3 GHz Westmere processors.  With 24 GB of memory and 146 GB of storage per node, users have access to an aggregate of 44 TB of memory and 276 TB of local storage.  Lonestar also provides access to five large memory (1TB) nodes, and eight nodes containing two NVIDIA GPU's, giving users access to high-throughput computing and remote visualization capabilities respectively.  Compute nodes have access to a PB Lustre Parallel file system.   A QDR InfiniBand switch fabric interconnects the nodes through a fat-tree topology with a point-to-point bandwidth of 40GB/sec (unidirectional speed).",TACC Dell PowerEdge Westmere Linux Cluster (Lonestar),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2014-12-31', 'friendly_begin_date': '2015-01-01', 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2014-06-29', 'friendly_end_date': None, 'production_begin_date': '2011-02-01', 'decommissioned_end_date': '2014-12-31', 'pre_production_begin_date': None, 'pre_production_end_date': None}",
203,,decommissioned,albedo.psc.xsede.org,psc.xsede.org,decommissioned,2012-11-14,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': 'Distributed storage under one namespace served by IU, NCSA, NICS, PSC, SDSC, TACC', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'user_guide_url': None, 'is_accounted': True, 'xsedenet_participant': None, 'databases': '', 'file_space_tb': 1000.0, 'supports_sensitive_data': None}",84.0,XSEDE,XSEDE Level 1,142295,storage,Teragrid wide filesystem,,"TeraGrid Albedo, Wide Area File System","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2012-11-14', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2012-11-13', 'friendly_end_date': None, 'production_begin_date': '2011-02-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
204,HSI,decommissioned,tape.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2011-07-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': 'HPSS', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'TeraBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530118, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-SDSC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}], 'display_resource_name': 'SDSC Tape Storage', 'allocations_description': None, 'allocable_id': 140025}, 'is_accounted': True, 'user_guide_url': None, 'databases': '', 'supports_sensitive_data': None, 'file_space_tb': 25000.0, 'xsedenet_participant': None}",70.0,XSEDE,XSEDE Level 1,140025,storage,"Development (up to 25 TB), medium 25-200 TB), or large (greater than 100 TG)
          allocations are available for long-term archival storage independent of SDSC computation resources
          <b",,SDSC Tape Storage,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-07-01', 'friendly_begin_date': None, 'post_production_end_date': '2011-06-30', 'coming_soon_end_date': None, 'post_production_begin_date': '2010-09-30', 'retired_end_date': None, 'production_end_date': '2010-09-29', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
205,"NCAR MSS DCS Commands (see frost man pages on msrcp command for more information, or check NCAR DCS Informat",decommissioned,mss.ncar.teragrid.org,ncar.teragrid.org,decommissioned,2011-03-21,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': 'NCAR MSS,\r\n1 Terabyte Tapes,\r\nNumber of tape drives, r/w speed, striping, etc. is irrelevant because this system is for archival rather than file server purposes', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'user_guide_url': None, 'is_accounted': True, 'xsedenet_participant': None, 'databases': 'NA', 'file_space_tb': 10.0, 'supports_sensitive_data': None}",71.0,XSEDE,XSEDE Level 1,140203,storage,"Controls archival storage on the NCAR Mass Storage System (MSS) by way of the MSS software. Data is stored on 1 Terabyte tapes. NCAR MSS is located at NCAR.
<h4>Recommended Use</h4>
NCAR MSS is reco",,NCAR Mass Storage System (Single or Double Copy),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2011-03-21', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2011-03-20', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
206,"GLADE is accessible to XSEDE users who also have also been granted access, via NCAR processes, to the resources that CISL manages.",production,glade.ncar.xsede.org,ncar.xsede.org,production,2012-12-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': 'GLADE provides 10.9 PB of usable capacity. GLADE comprise 20 NSD servers, 76 disk controllers and drawers, and 4,560 3-TB NL-SAS drives. In 2014, GLADE will add 5.5 PB of usable capacity.', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'user_guide_url': '', 'is_accounted': False, 'xsedenet_participant': None, 'databases': '', 'file_space_tb': 10900.0, 'supports_sensitive_data': None}",92.0,XSEDE,XSEDE Level 2,142299,storage,NCAR's 11-PB Globally Accessible Data Environment (GLADE) file spaces are mounted on the Yellowstone HPC system and the Geyser and Caldera data analysis and visualization clusters. ,,NCAR GLADE central file systems and data storage,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2012-12-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
207,,production,wrangler.storage.tacc.xsede.org,storage.tacc.xsede.org,production,2015-02-28,2019-02-28,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': 'Wrangler features 10PB of replicated storage hosted at TACC and at Indiana University, providing users with high-availability storage for working with data, data services, and results.', 'is_publicly_available': False, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'GigaBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530175, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2015-02-11', 'required_resource_display_name': 'TACC Data Analytics System (Wrangler)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530174, 'allocable_id': 142689}], 'default_amount': 5000.0, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-09-14', 'required_resource_display_name': 'TACC Data Analytics System (Wrangler)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530174, 'allocable_id': 142689}], 'default_amount': 5000.0, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2016-09-01', 'required_resource_display_name': 'TACC Data Analytics System (Wrangler)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530174, 'allocable_id': 142689}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': None}], 'display_resource_name': 'TACC Long-term Storage (Wrangler Storage)', 'allocations_description': None, 'allocable_id': 142690}, 'is_accounted': True, 'user_guide_url': 'http://portal.xsede.org/tacc-wrangler', 'databases': '', 'supports_sensitive_data': None, 'file_space_tb': 10000.0, 'xsedenet_participant': None}",93.0,XSEDE,XSEDE Level 1,142690,storage,Mid-term and long-term preservation of results and interim results for projects using the Wrangler Data Analytics System capabilities.,,TACC Long-term Storage (Wrangler Storage),"{'coming_soon_begin_date': '2014-09-12', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': '2019-01-31', 'post_production_end_date': None, 'coming_soon_end_date': '2014-09-14', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-02-28', 'friendly_end_date': None, 'production_begin_date': '2015-02-28', 'decommissioned_end_date': None, 'pre_production_begin_date': '2014-09-15', 'pre_production_end_date': '2015-01-30'}",
208,,post-production,staff.tg.teragrid.org,tg.teragrid.org,post-production,,,"{'is_accounted': False, 'is_authenticated': False, 'user_guide_url': '', 'is_authorized': False, 'is_publicly_available': False, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'GridResource', 'allocable_resource_id': 530138, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Staff', 'dollar_value': None}], 'display_resource_name': 'XSEDE Staff Compute Grid', 'allocations_description': None, 'allocable_id': 142321}, 'resources_in_grid': []}",108.0,XSEDE,XSEDE Level 1,142321,grid,,XSEDE Staff Grid,XSEDE Staff Grid,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': '2007-01-01', 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
209,,,nautilus.nics.teragrid.org,nics.teragrid.org,,,,"{'organizations': [{'organization_abbreviation': 'NICS', 'organization_name': 'National Institute for Computational Sciences', 'organization_code': 'T103349'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,39,resource,,"Nautilus is an SGI Altix UV 1000 system.  It has 1024 cores (Intel Nehalem EX processors), 4 terabytes of global shared memory, and 8 GPUs in a single system image.

This machine was initially decommissioned on 9/30/2013. However, it is again available as an XSEDE production resource starting May 1, 2014. This machine will only be available for supplement requests.","NICS SGI/NVIDIA, Visualization and Data Analysis System (Nautilus)","{'coming_soon_begin_date': '2010-03-17', 'retired_begin_date': None, 'decommissioned_begin_date': '2015-04-30', 'friendly_begin_date': None, 'post_production_end_date': '2015-04-29', 'coming_soon_end_date': '2010-04-06', 'post_production_begin_date': '2014-08-11', 'retired_end_date': None, 'production_end_date': '2014-04-30', 'friendly_end_date': None, 'production_begin_date': '2010-10-01', 'decommissioned_end_date': '2015-04-30', 'pre_production_begin_date': '2010-04-07', 'pre_production_end_date': '2010-09-30'}",2016-07-27 17:15:41.451
210,OSG grid protocols (no login shell).,production,Red.unl.xsede.org,unl.xsede.org,production,2005-05-01,2017-12-31,"{'latitude': 40.8258, 'node_count': 300.0, 'manufacturer': '', 'rmax': 0.0, 'interconnect': 'gigabit ethernet', 'is_publicly_available': True, 'peak_teraflops': 100.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'sensitive_data_support_description': None, 'job_manager': 'HTCondor', 'batch_system': 'HTCondor', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '0', 'rpeak': 0.0, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'n/a', 'storage_network': '10 gigabit ethernet', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 4.0, 'cpu_type': 'x86', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Red', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': 96.6852, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '129.93.239.129', 'parallel_file_system': None, 'user_guide_url': 'https://www.opensciencegrid.org/about/what-kind-of-computational-problems-fit-well-on-osg/', 'disk_size_tb': 2000.0, 'local_storage_per_node_gb': 1000.0, 'cpu_speed_ghz': 2.2, 'cpu_count_per_node': 20, 'operating_system': 'CentOS 7', 'community_software_area': False}",114.0,XSEDE,XSEDE Level 3,144574,compute,Opportunistic distributed high throughput computing. ,U.S. CMS Tier2 Compute Element for computing via the Open Science Grid. ,U.S. CMS Tier2 Compute Element,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2017-12-31', 'friendly_end_date': None, 'production_begin_date': '2005-05-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-09-13 15:12:01.318
211,,,trestles.uark.xsede.org,uark.xsede.org,,,,"{'organizations': [{'organization_abbreviation': 'U Arkansas', 'organization_name': 'University of Arkansas', 'organization_code': '0011080'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 3,113,resource,,"Trestles consists of 256 compute nodes, with a total of 8,192 cores across the system providing a theoretical peak performance of 79 TFlop/s. Each node is powerful and memory-rich, with four 8-core 2.4 GHz AMD Magny-Cours processors, for a total of 32 cores per node, 64 GB of DDR3 RAM and a 120 GB SSD local disk drive. The compute nodes are connected via Mellanox QDR InfiniBand interconnect in a fat-tree topology, with each link capable of 8 GB/s (bidirectional), and is connected to shared Lustre file systems with 16TB of scratch space and 350TB of main storage.
Trestles, previously an XSEDE Level 1 resource, was acquired by the University of Arkansas from San Diego Supercomputer Center and from the National Science Foundation in May of 2015.",University of Arkansas/AHPCC Cluster (Trestles),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-05-20 17:05:03.459
212,via Lustre from the IU Data Capacitor.  IU is also in the process of creating a Web portal interface for GridFTP access.,decommissioned,data-capacitor.iu.teragrid.org,iu.teragrid.org,decommissioned,2014-01-07,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': False, 'sensitive_data_support_description': None, 'user_guide_url': '', 'is_accounted': True, 'xsedenet_participant': None, 'databases': 'Oracle,MySQL', 'file_space_tb': 535.0, 'supports_sensitive_data': None}",75.0,XSEDE,XSEDE Level 1,139497,storage,Dedicated (nonpurged) disk storage in support of data collections and data-centric computing  Lustre is offered as an experimental service for researchers who may have a particular interest in Lustre.,,Lustre file space (IU Data Capacitor),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2014-01-07', 'friendly_begin_date': None, 'post_production_end_date': '2014-01-06', 'coming_soon_end_date': None, 'post_production_begin_date': '2012-12-31', 'retired_end_date': None, 'production_end_date': '2012-12-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
213,Ranch is available to XSEDE users who have an allocation on TACC's resources,production,ranch.tacc.xsede.org,tacc.xsede.org,production,2013-03-15,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': ""TACC's long-term mass storage solution, Ranch, is an Oracle® StorageTek Modular Library System. Ranch utilizes Oracle's Sun Storage Archive Manager Filesystem (SAM-FS) for migrating files to/from a tape archival system with a current offline storage capacity of 60 PB.\r\nRanch's disk cache is built on Oracle's ZFS 7240 and Dell MD3600i disk arrays containing approximately 640 TB of usable spinning disk storage. These disk arrays are controlled by a Dell R720  SAM-FS Metadata server which has 16 CPUs and 72 GB of RAM.\r\nTwo Oracle StorageTek SL8500 Automated Tape Libraries house all of the offline archival storage. Each SL8500 library can house up to 10,000 tapes with 64 tape drive slots. One SL8500 is currently populated with 10,000 T-10000B media where each tape is capable of holding one TB of uncompressed data while the second SL8500 houses 6,000 of the latest T-10000C media which can hold five TB of uncompressed data. Each SL8500 library also contains eight handbots to manage tapes and move them to/from the tape drives with a pass-through port connecting the two SL8500 libraries. If necessary, up to four SL8500 libraries can be integrated into a single archival solution, allowing for an offline storage capacity of 200 PB with current tape media."", 'is_publicly_available': False, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'GigaBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530162, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}], 'display_resource_name': 'TACC Long-term tape Archival Storage (Ranch)', 'allocations_description': None, 'allocable_id': 142293}, 'is_accounted': False, 'user_guide_url': 'https://portal.xsede.org/tacc-ranch', 'databases': '', 'supports_sensitive_data': None, 'file_space_tb': 61440.0, 'xsedenet_participant': None}",90.0,XSEDE,XSEDE Level 1,142293,storage,"TACC's High Performance Computing systems are used primarily for scientific computing with users having access to WORK, SCRATCH, and HOME file systems that are limited in size. This is also true for TACC's visualization system, Longhorn. The Ranch system serves the HPC and Vis community systems by providing a massive, high-performance file system for archival purposes. Space on Ranch can also be requested independent of an accompanying allocation on an XSEDE compute or visualization resource.
Please note that Ranch is an archival system. The ranch system is not backed up or replicated. This means that Ranch contains a single copy, and only a single copy, of your file/s. While lost data due to tape damage is rare, please keep this fact in mind for your data management plans.
",,TACC Long-term tape Archival Storage (Ranch),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2013-03-15', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
214,,decommissioned,trestles.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2015-05-01,,"{'latitude': None, 'node_count': 324.0, 'manufacturer': 'Appro', 'rmax': None, 'interconnect': 'Quad Data Rate (QDR) InfiniBand technology with a 40-Gb/s point-to-point bandwidth.  ', 'is_publicly_available': True, 'peak_teraflops': 100.0, 'advance_max_reservable_su': 1024.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530149, 'conversion_factors': [{'begin_date': '2011-01-01', 'factor': 2.3, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': '1.5M SUs limit, except for Gateway requests', 'allocation_state': 'Transfers Only', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': 0.029642857}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '1.5M SUs limit, except for Gateway requests', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.029642857}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '1.5M SUs limit, except for Gateway requests', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.029642857}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '1.5M SUs limit, except for Gateway requests', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.029642857}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '1.5M SUs limit, except for Gateway requests', 'allocation_state': 'No Actions Allowed', 'maximum_amount': 50000.0, 'allocation_type': 'Startup', 'dollar_value': 0.029642857}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '1.5M SUs limit, except for Gateway requests', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 0.029642857}], 'display_resource_name': 'SDSC Appro Linux Cluster (Trestles)', 'allocations_description': '<span class=""important"">1.5M SUs limit, except for Gateway requests</span>', 'allocable_id': 142266}, 'job_manager': 'Catalina', 'batch_system': 'Torque', 'platform_name': '', 'nfs_network': 'QDR InfiniBand', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': ""SDSC's Lustre Data Oasis with /oasis/scratch/trestles: 407 TB, /oasis/projects/nsf: 1.36 PB; /home: 70TB on 4 NFS Servers"", 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'mahidhar@sdsc.edu', 'login_hostname': 'trestles.sdsc.edu', 'storage_network': 'QDR InfiniBand to 12x10Gb/s Ethernet bridge', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': '8-core, 2.4-GHz AMD Magny-Cours processors', 'alternate_login_hostname': 'trestles-login.sdsc.edu, trestles-login1.sdsc.edu, trestles.sdsc.teragrid.org', 'xsedenet_participant': None, 'max_reservable_su': 1024.0, 'nickname': 'Trestles', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '198.202.118.30', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://portal.xsede.org/sdsc-trestles', 'disk_size_tb': 407.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.4, 'cpu_count_per_node': 32, 'operating_system': 'Linux (CentOS)', 'community_software_area': False}",40.0,XSEDE,XSEDE Level 1,142266,compute,"Trestles is designed to enable modest-scale and gateway researchers to be as computationally productive as possible. The system is allocated and scheduled with the primary objective to maintain fast queue turnaround times and secondarily to optimize utilization. Individual jobs are restricted to a maximum of 1,024 cores. Annual awards are limited to a total of 1.5 million SUs per project so that the system can support a large number of users; gateway applications can request more than 1.5 M SUs since they support a large number of users. Trestles features long run times (up to 2 weeks), pre-emptive on-demand queues for applications that require urgent access (because of unpredictable natural or man-made events that have a societal impact), as well as user-settable reservations for researchers who need predictable access for their workflows. Trestles is particularly well-suited to I/O-intensive applications that can benefit from the flash memory available on each compute node (e.g. computational chemistry and other applications that use a large number of small, temporary files) and the high-bandwidth to the Lustre parallel file system.","Trestles consists of 324 compute nodes, with a total of 10,368 cores across the system. Each node is powerful and memory-rich, with four 8-core 2.4 GHz AMD Magny-Cours processors, for a total of 32 cores per node, 64 GB of DDR3 RAM and a 120 GB SSD local disk drive. The compute nodes are connected via QDR InfiniBand interconnect in a fat-tree topology, with each link capable of 8 GB/s (bidirectional). Trestles’ scratch Lustre parallel file system has 800 TB capacity and 20 GB/s bandwidth. In addition, Trestles users receive a default 500 GB persistent storage allocation on SDSC’s “Project Storage” resource, and can request up to 50 TB storage as a separate allocation.",SDSC Appro Linux Cluster (Trestles),"{'coming_soon_begin_date': '2010-10-01', 'retired_begin_date': None, 'decommissioned_begin_date': '2015-05-01', 'friendly_begin_date': None, 'post_production_end_date': '2015-04-30', 'coming_soon_end_date': '2010-12-31', 'post_production_begin_date': '2014-09-14', 'retired_end_date': None, 'production_end_date': '2014-09-13', 'friendly_end_date': None, 'production_begin_date': '2011-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
215,,decommissioned,data-collections.sdsc.teragrid.org,sdsc.teragrid.org,decommissioned,2009-01-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '', 'is_publicly_available': True, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'TeraBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530104, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'DAC-SDSC', 'dollar_value': None}], 'display_resource_name': 'SDSC Collections Disk Space', 'allocations_description': None, 'allocable_id': 141477}, 'is_accounted': True, 'user_guide_url': None, 'databases': 'Oracle,MySQL', 'supports_sensitive_data': None, 'file_space_tb': 0.0, 'xsedenet_participant': None}",73.0,XSEDE,XSEDE Level 1,141477,storage,"Development (up to 5 TB), medium 5-25 TB), or large (greater than 25 TB)
          allocations are available for housing data collection in a database or on disk",,SDSC Collections Disk Space,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-01-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
216,,production,wrangler.storage.tacc.xsede.org,storage.tacc.xsede.org,production,2015-02-28,2019-02-28,"{'organizations': [{'organization_abbreviation': 'UT Austin', 'organization_name': 'University of Texas at Austin', 'organization_code': '0036582'}, {'organization_abbreviation': 'TACC', 'organization_name': 'Texas Advanced Computing Center', 'organization_code': 'T103142'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,93,resource,,Storage allocations on this resource will be granted only in conjunction with compute allocations on the Wrangler Data Analytics System. Exceptions will be made for users needing only dedicated services like MYSQL.,TACC Long-term Storage (Wrangler Storage),"{'coming_soon_begin_date': '2014-09-12', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': '2019-01-31', 'post_production_end_date': None, 'coming_soon_end_date': '2014-09-14', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-02-28', 'friendly_end_date': None, 'production_begin_date': '2015-02-28', 'decommissioned_end_date': None, 'pre_production_begin_date': '2014-09-15', 'pre_production_end_date': '2015-01-30'}",2016-04-18 16:06:43.591
217,Secure Shell.,,schooner.ou.xsede.org,ou.xsede.org,,,,"{'latitude': 35.18395, 'node_count': 499.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'Infiniband (Mellanox FDR10)', 'is_publicly_available': False, 'peak_teraflops': 346.0, 'advance_max_reservable_su': 0.0, 'is_accounted': False, 'sensitive_data_support_description': None, 'job_manager': '', 'batch_system': 'SLURM', 'platform_name': 'Dell PowerEdge R430/R730', 'nfs_network': 'GigE (10GE core)', 'advance_reservation_support': False, 'model': 'PowerEdge R430, R730', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': 'NVIDIA K20M', 'community_software_area_email': '', 'login_hostname': 'schooner.oscer.ou.edu', 'storage_network': 'Infiniband (Mellanox FDR10)', 'graphics_card': 'NVIDIA K20M', 'is_visualization': False, 'memory_per_cpu_gb': 1.6, 'cpu_type': 'Intel Xeon E5-26xx v3 ""Haswell""', 'alternate_login_hostname': 'schooner[123].oscer.ou.edu', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Schooner', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': 97.437149, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '129.15.42.21', 'parallel_file_system': None, 'user_guide_url': 'wiki.oscer.ou.edu', 'disk_size_tb': 450.0, 'local_storage_per_node_gb': 1000.0, 'cpu_speed_ghz': 2.3, 'cpu_count_per_node': 20, 'operating_system': 'CentOS', 'community_software_area': False}",121.0,XSEDE,XSEDE Level 3,144581,compute,Non-interactive High Performance and High Throughput Computing.,"346 TFLOPs, has some condominium including older condominium with Sandy Bridge CPUs. Modest number of NVIDIA K20M accelerators and Intel 31S1P accelerators. Mellanox FDR10 Infiniband, Gigabit Ethernet to the nodes, 10GE core. DataDirect Networks SFA7700X Lustre disk system with 70 x 6 TB 7200 RPM drives. Batch scheduler: SLURM.",U Oklahoma Dell PowerEdge R430/R730 cluster with Intel Xeon Haswell,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-11-20 18:13:26.424
218,,decommissioned,dtf.uc.teragrid.org,uc.teragrid.org,decommissioned,2009-07-01,,"{'latitude': None, 'node_count': 64.0, 'manufacturer': 'Intel', 'rmax': None, 'interconnect': 'Myrinet', 'is_publicly_available': False, 'peak_teraflops': 0.61, 'advance_max_reservable_su': 0.0, 'is_accounted': False, 'sensitive_data_support_description': None, 'job_manager': 'Default/Interactive: jobmanager-fork,  Batch: jobmanager-pbs', 'batch_system': '', 'platform_name': 'IA-64 Cluster', 'nfs_network': 'Gigabit Ethernet', 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': 'tg-grid.uc.teragrid.org', 'storage_network': 'Fibre Channel', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Intel Itanium 2', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'IA-64 Cluster Test Node', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'GPFS: 11 TB scratch\r\n4 TB home/software\r\n\r\nPVFS: 5 TB scratch', 'user_guide_url': 'http://www.uc.teragrid.org/user-guide.html', 'disk_size_tb': 4.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 1.5, 'cpu_count_per_node': 2, 'operating_system': 'Red Hat Enterprise Linux 4', 'community_software_area': False}",25.0,XSEDE,XSEDE Level 1,142231,compute,,"The IA-64 TeraGrid Linux Cluster at UC/ANL consists of 62 nodes with dual Intel Itanium 2 processors, with 4 GB of memory per node. The cluster is running Red Hat Enterprise Linux and is using the Myricom Myrinet cluster interconnect network. There is a 16 TB local high-performance GPFS, and access to the TeraGrid-wide GPFS-WAN file-system.",IA-64 Cluster Test,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2009-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
219,,,hpc.tufts.xsede.org,tufts.xsede.org,,,,"{'latitude': None, 'node_count': None, 'manufacturer': 'Cisco', 'rmax': None, 'interconnect': '', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': 0.0, 'is_accounted': False, 'sensitive_data_support_description': None, 'job_manager': '', 'batch_system': '', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'xfer.cluster.tufts.edu', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': None, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'xfer.tufts.xsede.org', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': '', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': None, 'cpu_count_per_node': None, 'operating_system': 'Redhat', 'community_software_area': False}",122.0,XSEDE,XSEDE Level 3,144579,compute,Main file transfer node for compute cluster.,Main file transfer node for compute cluster.,Tufts University High Performance Compute (HPC) Cluster File Transfer Node,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-11-10 19:58:29.232
220,,decommissioned,forge.ncsa.teragrid.org,ncsa.teragrid.org,decommissioned,2012-09-28,,"{'organizations': [{'organization_abbreviation': 'NCSA', 'organization_name': 'National Center for Supercomputing Applications', 'organization_code': '6000632'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,34,resource,,Forge consists of 36 Dell PowerEdge C6145 quad-socket nodes with dual 8-core AMD Magny-Cours 6136 processors and 64 GB of memory. Each node supports 6 NVIDIA Fermi M2070 GPUs.,NCSA Dell with NVIDIA Fermi GPU Cluster (Forge),"{'coming_soon_begin_date': '2011-06-01', 'retired_begin_date': None, 'decommissioned_begin_date': '2012-09-28', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2011-09-18', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2012-09-27', 'friendly_end_date': None, 'production_begin_date': '2011-10-03', 'decommissioned_end_date': None, 'pre_production_begin_date': '2011-09-19', 'pre_production_end_date': '2011-10-02'}",
221,,decommissioned,lonestar.tacc.teragrid.org,tacc.teragrid.org,decommissioned,2010-12-13,,"{'latitude': None, 'node_count': 1460.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'InfiniBand Switch', 'is_publicly_available': False, 'peak_teraflops': 62.16, 'advance_max_reservable_su': None, 'is_accounted': False, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530111, 'conversion_factors': [{'begin_date': None, 'factor': 0.0, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}], 'display_resource_name': 'TACC Dell Linux Cluster (Lonestar)', 'allocations_description': None, 'allocable_id': 142280}, 'job_manager': 'LSF 6.2', 'batch_system': 'LSF 6.2', 'platform_name': 'PowerEdge 1955 Linux Cluster', 'nfs_network': 'Gigabit Ethernet', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '/home 420 GB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': '', 'storage_network': 'Fibre Channel', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Xeon, EM64T, dual CPU (dual-core)', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Lonestar', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'Lustre, Parallel File System 35 TB', 'user_guide_url': '', 'disk_size_tb': 106.5, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.6, 'cpu_count_per_node': 4, 'operating_system': 'Linux (CentOS)', 'community_software_area': True}",36.0,XSEDE,XSEDE Level 1,142280,compute,"Lonestar is intended primarily for parallel applications scalable up to 4096 processing cores. Normal batch queues enable users to run up to 24-hour simulations that utilize up to 512 cores. Simulations requiring longer run times and/or more cores are accommodated via a special queue after approval from TACC technical staff. Serial and development queues are available to users for code development, conversion of serial applications to parallel, and single-cpu performance analysis. ","The Lonestar Dell PowerEdge Linux Cluster is configured with 5,840 compute-node cores, 11.6 TB of total memory and 106TB of local disk space. The peak performance rated is 62 TFLOPS. The system supports a 70TB globally accessible, Lustre parallel file system. Nodes are interconnected with InfiniBand technology in a fat-tree topology with a 1GB/sec point-to-point bandwidth. Also, a 2.8 petabyte archive system and a 5TB SAN are available through the login/development nodes. ",TACC Dell PowerEdge Linux Cluster (Lonestar),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2010-12-13', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2010-12-12', 'friendly_end_date': None, 'production_begin_date': '2006-07-20', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
222,,decommissioned,quarry.iu.teragrid.org,iu.teragrid.org,decommissioned,2016-06-30,,"{'organizations': [{'organization_abbreviation': 'Indiana U', 'organization_name': 'Indiana University', 'organization_code': '0087312'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,41,resource,,"The system consists of multiple Intel-based Hewlett-Packard (HP) servers geographically distributed for failover at the IU Bloomington and IUPUI Data Centers. The host operating system is Ubuntu and virtual machines (VM) are provisioned using KVM that rely on Ceph to provide block storage mirrored between the two sites.

A standard VM consists of one virtual CPU, 4 GB of memory, and 10 GB of persistent local storage. Service owners get root access to their VMs. Supported VM operating systems are Red Hat Enterprise Linux (RHEL), CentOS, Debian Stable, and Ubuntu Linux. To request a VM, fill out and submit the VM Request Form (http://rt.uits.iu.edu/systems/hps/vm-request-form.php).",Indiana University Gateway/Web Service Hosting (Quarry),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2016-06-30', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2016-06-29', 'friendly_end_date': None, 'production_begin_date': '2008-08-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2008-01-01', 'pre_production_end_date': '2008-07-31'}",
223,,production,comet.sdsc.xsede.org,sdsc.xsede.org,production,2015-04-01,2019-01-30,"{'latitude': None, 'node_count': 1984.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'Hybrid Fat-Tree, FDR InfiniBand', 'is_publicly_available': True, 'peak_teraflops': 2000.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530173, 'conversion_factors': [{'begin_date': '2014-12-08', 'factor': 6.165, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': None, 'required_resource_display_name': 'SDSC Medium-term disk storage (Data Oasis)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530165, 'allocable_id': 142302}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.015}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-06-10', 'required_resource_display_name': 'SDSC Medium-term disk storage (Data Oasis)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530165, 'allocable_id': 142302}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.015}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.015}, {'relative_order': None, 'required_resources': [{'begin_date': '2015-06-10', 'required_resource_display_name': 'SDSC Medium-term disk storage (Data Oasis)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530165, 'allocable_id': 142302}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': 50000.0, 'allocation_type': 'Startup', 'dollar_value': 0.015}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-09-14', 'required_resource_display_name': 'SDSC Medium-term disk storage (Data Oasis)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530165, 'allocable_id': 142302}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': 'Maximum request(SU) limit of 10M SUs except for Gateway requests; maximum number of cores per job = 1,728', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.015}], 'display_resource_name': 'SDSC Dell Cluster with Intel Haswell Processors (Comet)', 'allocations_description': None, 'allocable_id': 142688}, 'job_manager': 'SLURM', 'batch_system': '', 'platform_name': '', 'nfs_network': 'FDR InfiniBand', 'sensitive_data_support_description': None, 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '7000 (Performance)6000 (Durable)', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': 'Two NVIDIA K80s (each is a dual GPU) each on 36 GPU-enabled nodes', 'community_software_area_email': '', 'login_hostname': 'comet.sdsc.xsede.org', 'storage_network': 'FDR InfiniBand/40GigE', 'graphics_card': 'NVIDIA K80', 'is_visualization': False, 'memory_per_cpu_gb': 5.33, 'cpu_type': 'Intel Xeon E5-2680v3 ', 'alternate_login_hostname': 'comet-login.sdsc.edu', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Comet', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://portal.xsede.org/sdsc-comet', 'disk_size_tb': 7000.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.5, 'cpu_count_per_node': 24, 'operating_system': 'Linux (CentOS)', 'community_software_area': True}",49.0,XSEDE,XSEDE Level 1,142688,compute,"Comet is designed to provide cyberinfrastructure for the long tail of science, covering a diverse application base with complex workflows. It features a rich collection of preinstalled applications including commercial software like Gaussian, Abaqus, Q-Chem, Matlab MDCS, and IDL. The system is geared towards supporting capacity computing, optimized for quick turnaround on small/modest scale jobs (up to 1728 cores). The local SSDs on each compute node will be beneficial to applications that exhibit random access data patterns or require fast access to significant amounts of compute node local scratch space. The large memory per node (128 GB) makes Comet ideal for shared-memory applications or MPI codes with large per-process memory footprints. The AVX2-capable Intel Haswell processors should provide excellent performance for applications with vectorizable loops or that make heavy use of optimized math libraries. Comet features a Gateway friendly environment including specialized nodes for Gateway hosting. Research groups/communities interested in enhanced control of software stack and compute nodes can take advantage of the high performance virtualization feature of Comet that will be enabled by June 2016.","Comet will be a 2.0 Petaflop (PF) Dell integrated compute cluster, with next-generation Intel Haswell processors (with AVX2), interconnected with Mellanox FDR InfiniBand in a hybrid fat-tree topology. Full bisection bandwidth will be available at rack level (72 nodes) and there will be 4:1 oversubscription cross-rack. Compute nodes will feature 320 GB of SSD storage and 128GB of DRAM per node. The system will also feature 7PB of performance storage (200GB/s aggregate), and 6PB of durable storage. A subset of the system will feature 4 NVIDIA GPUs per node. Additionally, 4 1.5TB large memory nodes and additional nodes for Gateway hosting and VM image repositories will be available. Comet will enable high performance virtualization using the single root I/O virtualization (SR-IOV) technology.",SDSC Dell Cluster with Intel Haswell Processors (Comet),"{'coming_soon_begin_date': '2014-09-12', 'retired_begin_date': None, 'decommissioned_begin_date': '2019-01-31', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2014-09-13', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-01-30', 'friendly_end_date': None, 'production_begin_date': '2015-04-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2014-09-14', 'pre_production_end_date': '2015-03-31'}",2016-05-19 20:18:02.880
224,,decommissioned,athena.nics.teragrid.org,nics.teragrid.org,decommissioned,2011-07-31,,"{'latitude': None, 'node_count': 4152.0, 'manufacturer': 'Cray', 'rmax': None, 'interconnect': 'Cray SeaStar - 3D Torus', 'is_publicly_available': True, 'peak_teraflops': 166.0, 'advance_max_reservable_su': None, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530140, 'conversion_factors': [{'begin_date': None, 'factor': 0.0, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': 0.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}], 'display_resource_name': 'NICS Cray XT4 (Athena)', 'allocations_description': None, 'allocable_id': 142268}, 'job_manager': '', 'batch_system': 'Torque/Moab', 'platform_name': 'XT4', 'nfs_network': 'GigE', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '100', 'rpeak': None, 'machine_type': 'MPP', 'gpu_description': None, 'community_software_area_email': 'help@teragrid.org', 'login_hostname': 'athena.nics.teragrid.org', 'storage_network': 'lustre file system on DDN disk', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 1.0, 'cpu_type': 'AMD', 'alternate_login_hostname': 'athena-gsi.nics.teragrid.org', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Athena', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': None, 'ip_address': '', 'parallel_file_system': 'Lustre', 'user_guide_url': 'http://www.nics.tennessee.edu/computing-resources/', 'disk_size_tb': 100.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.6, 'cpu_count_per_node': 4, 'operating_system': 'Cray Linux Environment (CLE) 2.2', 'community_software_area': True}",30.0,XSEDE,XSEDE Level 1,142268,compute,Athena is intended for moderate to highly scalable parallel applications with relatively small memory and disk storage requirements. (Available memory is about 1 Gbyte per core and total formatted disk storage for all users is 86 Tbytes.) GRAM is not supported on Athena.,"The Athena system is a Cray XT4 with 4512 compute nodes interconnected with SeaStar, a 3D torus. Each compute node has one four-core AMD Opteron for a total of 18048 cores. All nodes have 4 Gbytes of memory: 1 Gbyte of memory per core.

To use Athena, users must apply for time on Kraken and request a transfer to Athena. Based on job requirements, NICS staff will determine whether Athena is a good match.",NICS Cray XT4 (Athena),"{'coming_soon_begin_date': '2009-01-01', 'retired_begin_date': '2010-08-01', 'decommissioned_begin_date': '2011-07-31', 'friendly_begin_date': None, 'post_production_end_date': '2011-07-30', 'coming_soon_end_date': '2010-07-25', 'post_production_begin_date': '2011-07-01', 'retired_end_date': '2010-09-30', 'production_end_date': '2011-06-30', 'friendly_end_date': None, 'production_begin_date': '2010-10-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2010-07-26', 'pre_production_end_date': '2010-09-30'}",
225,,decommissioned,quarry.iu.teragrid.org,iu.teragrid.org,decommissioned,2016-06-30,,"{'latitude': None, 'node_count': 16.0, 'manufacturer': 'HP', 'rmax': None, 'interconnect': '10gb Ethernet', 'is_publicly_available': True, 'peak_teraflops': 0.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Number of virtual machines', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530126, 'conversion_factors': [{'begin_date': '2008-01-01', 'factor': 5.35, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 118.9824232}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': 118.9824232}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': 5.0, 'allocation_type': 'Startup', 'dollar_value': 118.9824232}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': 118.9824232}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': 118.9824232}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 118.9824232}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 118.9824232}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 118.9824232}], 'display_resource_name': 'Indiana University Gateway/Web Service Hosting (Quarry)', 'allocations_description': None, 'allocable_id': 126218}, 'job_manager': 'N/A', 'batch_system': 'N/A', 'platform_name': 'DL160, DL360, DL180, DL380', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': '264TB', 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': None, 'community_software_area_email': '', 'login_hostname': '(varies) gwXXX.iu.xsede.org', 'storage_network': '10gb Ethernet', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 8.0, 'cpu_type': '(varies) Intel Xeon E5603, E5606, E5-2660', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Quarry', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre (1.1PB)', 'user_guide_url': 'https://portal.xsede.org/iu-quarry', 'disk_size_tb': 264.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.2, 'cpu_count_per_node': 8, 'operating_system': 'Ubuntu Linux', 'community_software_area': False}",41.0,XSEDE,XSEDE Level 1,126218,compute,"The Quarry Gateway Hosting System at Indiana University is used solely for hosting Extreme Science and Engineering Discovery Environment (XSEDE) Science Gateway and Web Service allocations, and is restricted to members of approved projects that have a web service component. The system can also be used for XSEDE central service virtual machines.
","The system consists of multiple Intel-based Hewlett-Packard (HP) servers geographically distributed for failover at the IU Bloomington and IUPUI Data Centers. The host operating system is Ubuntu and virtual machines (VM) are provisioned using KVM that rely on Ceph to provide block storage mirrored between the two sites.

A standard VM consists of one virtual CPU, 4 GB of memory, and 10 GB of persistent local storage. Service owners get root access to their VMs. Supported VM operating systems are Red Hat Enterprise Linux (RHEL), CentOS, Debian Stable, and Ubuntu Linux. To request a VM, fill out and submit the VM Request Form (http://rt.uits.iu.edu/systems/hps/vm-request-form.php).",Indiana University Gateway/Web Service Hosting (Quarry),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2016-06-30', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2016-06-29', 'friendly_end_date': None, 'production_begin_date': '2008-08-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2008-01-01', 'pre_production_end_date': '2008-07-31'}",
226,,decommissioned,darter.nics.xsede.org,nics.xsede.org,decommissioned,2016-07-01,,"{'latitude': 35.930379, 'node_count': 724.0, 'manufacturer': 'Cray', 'rmax': None, 'interconnect': 'Aries', 'is_publicly_available': True, 'peak_teraflops': 248.9, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530172, 'conversion_factors': [{'begin_date': None, 'factor': 2.028, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'Transfers Only', 'maximum_amount': None, 'allocation_type': 'Staff', 'dollar_value': 0.02}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.02}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.02}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': 0.0, 'allocation_type': 'Startup', 'dollar_value': 0.02}], 'display_resource_name': 'NICS Cray XC30 (Darter)', 'allocations_description': None, 'allocable_id': 142324}, 'job_manager': '', 'batch_system': 'Torque/Moab', 'platform_name': 'XC30', 'nfs_network': '10 GigE', 'sensitive_data_support_description': None, 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '1634 TB', 'rpeak': None, 'machine_type': 'MPP', 'gpu_description': '', 'community_software_area_email': 'help@xsede.org', 'login_hostname': 'darter.nics.xsede.org', 'storage_network': 'Infiniband', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': 'Intel', 'alternate_login_hostname': 'gsissh.darter.nics.utk.edu', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Darter', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': -84.310984, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '192.249.6.188', 'parallel_file_system': 'Lustre', 'user_guide_url': 'https://portal.xsede.org/nics-darter', 'disk_size_tb': 1634.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.6, 'cpu_count_per_node': 32, 'operating_system': 'Cray Linux Environment 5 (SLES 11)', 'community_software_area': True}",55.0,XSEDE,XSEDE Level 1,142324,compute,Darter is intended for highly scalable parallel applications.,"Darter is a Cray XC30 (Cascade) supercomputer that runs the Cray Linux Environment (CLE) 5.0 upo 3 based on SLES 11.  It has 11,968 physical compute cores (23,936 logical cores with Hyper-Threading enabled), 24 TB of compute memory,    the interconnect is a Dragonfly network topology from Cray Aries technology.  Darter has 748 compute nodes with a peak performance of 250 Tflops. Darter has 334TB parallel Lustre file system for scratch on Cray Sonexion hardware,    2 login nodes with 10GigE uplinks and long term archival/storage available through HPSS
",NICS Cray XC30 - Darter,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2016-07-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2016-06-30', 'friendly_end_date': None, 'production_begin_date': '2014-05-15', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-09-12 19:07:56.951
227,"directly from tape or from a front-end cache.  
GridFTP.  HPSS Hierarchical Storage Interface (HSI).  
IU recommends use of GridFTP",decommissioned,archive.iu.teragrid.org,iu.teragrid.org,decommissioned,2013-01-01,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': 'HPSS,\r\n500 GB tapes,\r\n52 tape drives,\r\nR/W=100 MB/sec,\r\n1- and 4-way stripes', 'is_publicly_available': False, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'TeraBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530125, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'LRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'MRAC', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': None}], 'display_resource_name': 'Indiana University DC-WAN Lustre Filesystem', 'allocations_description': None, 'allocable_id': 139672}, 'is_accounted': True, 'user_guide_url': '', 'databases': 'NA', 'supports_sensitive_data': None, 'file_space_tb': 2800.0, 'xsedenet_participant': None}",87.0,XSEDE,XSEDE Level 1,139672,storage,"Archival storage under control of the HPSS (High Performance Software System) software, stored on 500 GB tapes. The HPSS installation is a geographically distributed data storage system. Data may be c",,Indiana University DC-WAN Lustre Filesystem,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2013-01-01', 'friendly_begin_date': None, 'post_production_end_date': '2012-12-31', 'coming_soon_end_date': None, 'post_production_begin_date': '2012-12-31', 'retired_end_date': None, 'production_end_date': '2012-12-30', 'friendly_end_date': None, 'production_begin_date': '2009-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
228,,,arc-ts.umich.xsede.org,umich.xsede.org,,,,"{'latitude': None, 'node_count': None, 'manufacturer': 'IBM', 'rmax': None, 'interconnect': '', 'is_publicly_available': True, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': False, 'sensitive_data_support_description': '', 'job_manager': '', 'batch_system': '', 'platform_name': 'ConFlux', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': 'Nvidia P100 Pascal NVLink', 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': None, 'cpu_type': 'Power 8', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'ConFlux', 'supports_sensitive_data': False, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': 'http://arc-ts.umich.edu/systems-and-services/conflux/', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': None, 'cpu_count_per_node': None, 'operating_system': '', 'community_software_area': False}",129.0,XSEDE,XSEDE Level 3,144592,compute,,"The ConFlux cluster will be built with ~43 IBM Power8 CPU two-socket “Firestone” S822LC compute nodes providing 20 cores in each, and fifteen Power8 CPU two-socket “Garrison” compute nodes providing an additional 20 cores each. Each of the Garrison nodes will also host four NVIDIA Pascal GPUs connected via NVIDIA’s NVLink technology to the Power8 system bus. Each node has a local high-speed flash memory for random access.",ConFlux - Data Driven Physics Platform,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-17 20:46:15.823
229,,production,oasis-dm.sdsc.xsede.org,sdsc.xsede.org,production,2012-09-11,,"{'latitude': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'specifications': '4PB of Lustre-based storage backed by 64 OSSs and a 10GigE interconnect', 'is_publicly_available': False, 'sensitive_data_support_description': None, 'allocations_info': {'unit_type': 'GigaBytes', 'allocable_type': 'StorageResource', 'allocable_resource_id': 530165, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'SDSC Appro with Intel Sandy Bridge Cluster (Gordon Compute Cluster)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530155, 'allocable_id': 142281}, {'begin_date': '2014-09-14', 'required_resource_display_name': 'SDSC Dell Cluster with Intel Haswell Processors (Comet)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530173, 'allocable_id': 142688}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.364}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'SDSC Appro with Intel Sandy Bridge Cluster (Gordon Compute Cluster)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530155, 'allocable_id': 142281}, {'begin_date': '2015-06-10', 'required_resource_display_name': 'SDSC Dell Cluster with Intel Haswell Processors (Comet)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530173, 'allocable_id': 142688}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.364}, {'relative_order': None, 'required_resources': [{'begin_date': '2016-09-01', 'required_resource_display_name': 'SDSC Dell Cluster with Intel Haswell Processors (Comet)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530173, 'allocable_id': 142688}, {'begin_date': '2016-09-01', 'required_resource_display_name': 'SDSC Appro with Intel Sandy Bridge Cluster (Gordon Compute Cluster)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530155, 'allocable_id': 142281}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.364}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'SDSC Appro with Intel Sandy Bridge Cluster (Gordon Compute Cluster)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530155, 'allocable_id': 142281}, {'begin_date': '2015-06-10', 'required_resource_display_name': 'SDSC Dell Cluster with Intel Haswell Processors (Comet)', 'end_date': None, 'allocable_type': 'ComputeResource', 'required_resource_allocable_resource_id': 530173, 'allocable_id': 142688}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Startup', 'dollar_value': 0.364}], 'display_resource_name': 'SDSC Medium-term disk storage (Data Oasis)', 'allocations_description': None, 'allocable_id': 142302}, 'is_accounted': False, 'user_guide_url': 'https://portal.xsede.org/sdsc-dataoasis', 'databases': '', 'supports_sensitive_data': None, 'file_space_tb': 4000.0, 'xsedenet_participant': None}",86.0,XSEDE,XSEDE Level 1,142302,storage,"High-capacity, high-performance, short- to medium-term storage and scratch space for data-intensive research.",,SDSC Medium-term disk storage (Data Oasis),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2012-09-11', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",
230,,production,trestles.uark.xsede.org,uark.xsede.org,production,2015-07-01,2018-07-01,"{'latitude': 36.0678324, 'node_count': 256.0, 'manufacturer': 'Appro', 'rmax': None, 'interconnect': 'Quad Data Rate (QDR) InfiniBand technology with a 40-Gb/s point-to-point bandwidth.', 'is_publicly_available': False, 'peak_teraflops': 79.0, 'advance_max_reservable_su': 0.0, 'is_accounted': False, 'sensitive_data_support_description': None, 'job_manager': 'Torque/Maui', 'batch_system': '', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '$SCRATCH: 16TB, $STORAGE: 350TB', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': 'trestles.uark.edu', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 2.0, 'cpu_type': '8-core, 2.4-GHz AMD Magny-Cours processors', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Trestles', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': -94.1758438, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': 'http://hpcwiki.uark.edu/', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.4, 'cpu_count_per_node': 32, 'operating_system': 'CentOS', 'community_software_area': False}",113.0,XSEDE,XSEDE Level 3,144571,compute,"Trestles is designed to enable modest-scale researchers to be as computationally productive as possible. The system is allocated and scheduled with the primary objective to maintain fast queue turnaround times and secondarily to optimize utilization. Trestles is particularly well-suited to I/O-intensive applications that can benefit from the flash memory available on each compute node (e.g. computational chemistry and other applications that use a large number of small, temporary files) and the high-bandwidth to the Lustre parallel file system.","Trestles consists of 256 compute nodes, with a total of 8,192 cores across the system providing a theoretical peak performance of 79 TFlop/s. Each node is powerful and memory-rich, with four 8-core 2.4 GHz AMD Magny-Cours processors, for a total of 32 cores per node, 64 GB of DDR3 RAM and a 120 GB SSD local disk drive. The compute nodes are connected via Mellanox QDR InfiniBand interconnect in a fat-tree topology, with each link capable of 8 GB/s (bidirectional), and is connected to shared Lustre file systems with 16TB of scratch space and 350TB of main storage.",University of Arkansas/AHPCC Cluster (Trestles),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2018-07-01', 'friendly_end_date': None, 'production_begin_date': '2015-07-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-09-13 10:48:41.839
231,,,arc-ts.umich.xsede.org,umich.xsede.org,,,,"{'latitude': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'specifications': '', 'is_publicly_available': False, 'sensitive_data_support_description': 'FISMA Moderate\r\nPHI/HIPAA\r\nFERPA\r\nSSN', 'user_guide_url': 'http://arc-ts.umich.edu/systems-and-services/yottabyte/', 'is_accounted': False, 'xsedenet_participant': None, 'databases': 'docker,postgesql,mysql,casandra,mongodb,elasticsearch,presto', 'file_space_tb': None, 'supports_sensitive_data': True}",129.0,XSEDE,XSEDE Level 3,144595,storage,Support for multiple online data ingest and database offerings in SQL and noSQL.,,YBRC - Data Services,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-17 20:50:19.697
232,,,arc-ts.umich.xsede.org,umich.xsede.org,,,,"{'latitude': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'specifications': '', 'is_publicly_available': True, 'sensitive_data_support_description': '', 'user_guide_url': 'http://arc-ts.umich.edu/systems-and-services/osiris/', 'is_accounted': False, 'xsedenet_participant': None, 'databases': '', 'file_space_tb': None, 'supports_sensitive_data': False}",129.0,XSEDE,XSEDE Level 3,144591,storage,"OSiRIS will use commercial off-the-shelf hardware coupled with CEPH software to build a high performance software-defined storage system. The system is composed of a number of building-block components: storage head- nodes with SSDs + 60-disk SAS shelf, RHEV cluster (2-hosts), Globus Connect servers, a PERFSONAR network monitoring node and reliable, OpenFlow capable switches.",,OSiRIS - Open Storage Research Infrastructure,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-17 20:44:28.741
233,,,arc-ts.umich.xsede.org,umich.xsede.org,,,,"{'latitude': None, 'sensitive_data_support_description': '', 'is_accounted': False, 'is_authenticated': False, 'xsedenet_participant': None, 'is_authorized': False, 'supports_sensitive_data': False, 'is_publicly_available': False, 'user_guide_url': '', 'longitude': None}",129.0,XSEDE,XSEDE Level 3,144593,other,Big Data workflows in the Hadoop/Spark ecosystem.,"The Data Science Platform is an upgraded Hadoop cluster currently available as a technology preview with no associated charges to U-M researchers. The ARC-TS Hadoop cluster is an on-campus resource that provides a different service level than most cloud-based Hadoop offerings, including:

    high-bandwidth data transfer to and from other campus data storage locations with no data transfer costs
    very high-speed inter-node connections using 40Gb/s Ethernet.

The cluster provides 112TB of total usable disk space, 40GbE inter-node networking, Hadoop version 2.6.0, and several additional data science tools.

Aside from Hadoop and its Distributed File System, the ARC-TS data science service includes:

    Pig, a high-level language that enables substantial parallelization, allowing the analysis of very large data sets.
    Hive, data warehouse software that facilitates querying and managing large datasets residing in distributed storage using a SQL-like language called HiveQL.
    Sqoop, a tool for transferring data between SQL databases and the Hadoop Distributed File System.
    Rmr, an extension of the R Statistical Language to support distributed processing of large datasets stored in the Hadoop Distributed File System.
    Spark, a general processing engine compatible with Hadoop data
    mrjob, allows MapReduce jobs in Python to run on Hadoop
",Flux Hadoop - Big Data Cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-17 20:47:27.749
234,,decommissioned,test1.xes.xsede.org,xes.xsede.org,decommissioned,2017-03-01,,"{'latitude': None, 'node_count': None, 'manufacturer': '', 'rmax': None, 'interconnect': '', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': False, 'sensitive_data_support_description': '', 'job_manager': '', 'batch_system': '', 'platform_name': 'XSEDE Test', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': None, 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': None, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'XSEDE Test', 'supports_sensitive_data': False, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': '', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': None, 'cpu_count_per_node': None, 'operating_system': '', 'community_software_area': False}",130.0,XSEDE,XSEDE Level 1,144596,compute,,XSEDE Test,Compute XSEDE Test Resource,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2017-03-01', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-22 21:10:59.924
235,,production,usdrcg.usd.xsede.org,usd.xsede.org,production,2006-01-01,,"{'latitude': None, 'node_count': None, 'manufacturer': 'Sun, HP', 'rmax': None, 'interconnect': '', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': False, 'sensitive_data_support_description': '', 'job_manager': 'SGE', 'batch_system': 'SGE', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': None, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'usdlegacy', 'supports_sensitive_data': False, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': '', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': None, 'cpu_count_per_node': None, 'operating_system': '', 'community_software_area': False}",125.0,XSEDE,XSEDE Level 3,144583,compute,,,University of South Dakota Legacy HPC cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2006-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2005-01-01', 'pre_production_end_date': '2005-12-31'}",2017-01-27 17:27:48.317
236,,,arc-ts.umich.xsede.org,umich.xsede.org,,,,"{'latitude': None, 'node_count': None, 'manufacturer': '', 'rmax': None, 'interconnect': '', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': False, 'sensitive_data_support_description': 'HIPAA/PHI\r\nFISMA\r\nSSN\r\nhttps://www.safecomputing.umich.edu/dataguide/', 'job_manager': '', 'batch_system': '', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': None, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'ARMIS', 'supports_sensitive_data': True, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': 'http://arc-ts.umich.edu/systems-and-services/armis/', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': None, 'cpu_count_per_node': None, 'operating_system': '', 'community_software_area': False}",129.0,XSEDE,XSEDE Level 3,144589,compute,,"The Armis HPC cluster, in conjunction with Turbo Research Storage, provides a secure, scalable, and distributed computing environment that aligns with HIPAA privacy standards. The HPC environment is composed of a task-managing administrative nodes, standard Linux-based 2 and 4 socket server class hardware in a secure data center, connected by both a high-speed ethernet (1 Gbps) and InfiniBand network (40Gbps), and a secure parallel filesystem for temporary data, provided by HIPAA-aligned Turbo Research Storage. Armis is currently provided as a pilot service.",ARMIS - Cluster for PHI/HIPAA Data,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-17 20:43:11.988
237,Limited to Tennessee researchers,production,beacon.nics.xsede.org,nics.xsede.org,production,2016-09-01,,"{'latitude': 35.929527, 'node_count': 48.0, 'manufacturer': 'Cray', 'rmax': None, 'interconnect': 'FDR Infiniband', 'is_publicly_available': False, 'peak_teraflops': 210.0, 'advance_max_reservable_su': 0.0, 'is_accounted': False, 'sensitive_data_support_description': None, 'job_manager': 'Torque/Moab', 'batch_system': 'Torque/Moab', 'platform_name': 'Beacon', 'nfs_network': '', 'advance_reservation_support': False, 'model': 'CS300', 'primary_storage_shared_gb': '1500000', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '4 Intel® Xeon Phi™ coprocessors 5110P with 8 GB of memory each', 'community_software_area_email': '', 'login_hostname': 'login.beacon.nics.utk.edu', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 256.0, 'cpu_type': 'Intel Xeon E5-2670', 'alternate_login_hostname': 'gsissh.beacon.nics.utk.edu', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Beacon', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': -84.310738, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '192.249.6.185', 'parallel_file_system': None, 'user_guide_url': 'https://www.nics.utk.edu/beacon', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': 960.0, 'cpu_speed_ghz': 2.6, 'cpu_count_per_node': 16, 'operating_system': 'Linux', 'community_software_area': False}",117.0,XSEDE,XSEDE Level 2,144575,compute,Beacon system can be used to port and optimize scientific codes to the coprocessors based on Intel's Many Integrated Core (MIC) architecture.,"Beacon is an energy efficient cluster that utilizes Intel® Xeon Phi™ coprocessors. Beacon provides 768 conventional cores and 11,520 accelerator cores that provide over 210 TFLOP/s of combined computational performance, 12 TB of system memory, 1.5 TB of coprocessor memory, and over 73 TB of SSD storage, in aggregate.",Beacon cluster,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2016-09-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-10-19 21:03:31.721
238,,production,rmacc-summit.colorado.xsede.org,colorado.xsede.org,production,2017-02-06,,"{'latitude': None, 'node_count': 478.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': 'Omni-Path', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': None, 'is_accounted': False, 'sensitive_data_support_description': '', 'job_manager': '', 'batch_system': 'Slurm', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': False, 'model': 'C6320', 'primary_storage_shared_gb': '1200000', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '2x NVIDIA K80 per GPU node', 'community_software_area_email': '', 'login_hostname': 'login.rc.colorado.edu', 'storage_network': 'Omni-Path', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 5.0, 'cpu_type': 'Intel(R) Xeon(R) CPU E5-2680 v3', 'alternate_login_hostname': '', 'xsedenet_participant': False, 'max_reservable_su': None, 'nickname': '', 'supports_sensitive_data': False, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': 'https://www.rc.colorado.edu/support/user-guide', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': 180.0, 'cpu_speed_ghz': 2.5, 'cpu_count_per_node': 24, 'operating_system': 'RHEL 7.2', 'community_software_area': False}",131.0,XSEDE,XSEDE Level 3,144597,compute,Parallel and high-throughput serial HPC jobs.,,RMACC-Summit,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': '2016-11-21', 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': '2017-02-03', 'production_begin_date': '2017-02-06', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-28 20:19:51.007
239,,post-production,gordonio.sdsc.xsede.org,sdsc.xsede.org,post-production,,,"{'organizations': [{'organization_abbreviation': 'SDSC', 'organization_name': 'San Diego Supercomputer Center', 'organization_code': '6000830'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 1,47,resource,,"There are 64 I/O nodes in the Gordon compute cluster, and some are available as a dedicated resource outside the batch scheduler. You must request allocations for these separately from your Gordon Compute Cluster allocation. This I/O node resource is referred to as Gordon ION.

Gordon ION is a 64-node, flash-based I/O resource that is an integral part of Gordon Compute Cluster, as well as a distinct and individually allocated resource. Each node is composed of two hex-core, 2.66GHz Westmere processors, 48GB of DRAM memory, and 4 TB of high-performance Intel flash memory capable of delivering over 560k I/O IOPS.  Each I/O node can access Data Oasis, SDSC’s 4 PB Lustre-based file system, via two 10 GbE connections. This results in a sustained aggregate bandwidth into Gordon ION through all 64 nodes of 100 GB/s.",SDSC Appro with Intel Flash I/O Nodes (Gordon ION),"{'coming_soon_begin_date': '2011-09-01', 'retired_begin_date': None, 'decommissioned_begin_date': '2017-04-01', 'friendly_begin_date': None, 'post_production_end_date': '2017-03-31', 'coming_soon_end_date': '2011-12-31', 'post_production_begin_date': '2014-09-14', 'retired_end_date': None, 'production_end_date': '2014-09-13', 'friendly_end_date': None, 'production_begin_date': '2012-01-01', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-02-15 01:43:12.277
240,,decommissioned,blacklight.psc.teragrid.org,psc.teragrid.org,decommissioned,2015-08-16,,"{'latitude': None, 'node_count': 2.0, 'manufacturer': 'SGI', 'rmax': None, 'interconnect': 'SGI NUMALink', 'is_publicly_available': True, 'peak_teraflops': 36.0, 'advance_max_reservable_su': 0.0, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530150, 'conversion_factors': [{'begin_date': None, 'factor': 1.796, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': 30000.0, 'allocation_type': 'Startup', 'dollar_value': 0.10718796100000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': False, 'site_provider_comments': None, 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'TRAC', 'dollar_value': 0.10718796100000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': 0.10718796100000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': 0.10718796100000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': 0.10718796100000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': 0.10718796100000001}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'No Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': 0.10718796100000001}], 'display_resource_name': 'PSC SGI Altix UV (Blacklight)', 'allocations_description': '', 'allocable_id': 142267}, 'job_manager': 'PBS', 'batch_system': 'PBS', 'platform_name': 'UV 1000 cc-NUMA', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '/usr/users', 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': '', 'community_software_area_email': 'raymond@psc.edu', 'login_hostname': 'blacklight.psc.xsede.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 64.0, 'cpu_type': 'Intel Xeon X7560 (Nehalem) 8 core', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'Blacklight', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': True, 'ip_address': '', 'parallel_file_system': '$SCRATCH', 'user_guide_url': 'https://portal.xsede.org/psc-blacklight', 'disk_size_tb': 150.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.27, 'cpu_count_per_node': 2048, 'operating_system': 'SuSE Linux', 'community_software_area': True}",60.0,XSEDE,XSEDE Level 1,142267,compute,Blacklight is intended for applications that require a large shared memory for computational tasks.,"Blacklight is an SGI UV 1000cc-NUMA shared-memory system comprising 256 blades.
Each blade holds 2 Intel Xeon X7560 (Nehalem) eight-core processors, for a total of 4096 cores
across the whole machine. Each core has a clock rate of 2.27 GHz, supports two hardware threads
and can perform 9 Gflops. The sixteen cores on each blade share 128 Gbytes of local memory.
Users can run shared memory jobs of up to 16 Tbytes.

Blacklight jobs have access to scratch storage. Blacklight users should also apply for between 100 GB
and 40 TB of persistent storage, as needed,  on the Data SuperCell (DSC). They may also request storage on the XSEDE-wide file system (XWFS).
",PSC SGI Altix UV (Blacklight),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': '2015-08-16', 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2015-08-15', 'friendly_end_date': None, 'production_begin_date': '2011-01-18', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-05-27 18:16:21.524
241,,production,maverick.tacc.xsede.org,tacc.xsede.org,production,2014-03-03,,"{'latitude': None, 'node_count': 132.0, 'manufacturer': 'HP', 'rmax': None, 'interconnect': 'FDR InfiniBand', 'is_publicly_available': True, 'peak_teraflops': 59.1, 'advance_max_reservable_su': None, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530171, 'conversion_factors': [{'begin_date': None, 'factor': 4.8375, 'conversion': 'Teragrid', 'end_date': None}], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Educational', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2014-04-01', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': 3000.0, 'allocation_type': 'Startup', 'dollar_value': None}, {'relative_order': None, 'required_resources': [{'begin_date': '2016-09-01', 'required_resource_display_name': 'TACC Long-term tape Archival Storage (Ranch)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530162, 'allocable_id': 142293}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'XSEDE2 Staff Allocations', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Campus Champions', 'dollar_value': None}, {'relative_order': None, 'required_resources': [], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': None, 'allocation_state': 'All Actions Allowed', 'maximum_amount': None, 'allocation_type': 'Discretionary', 'dollar_value': None}], 'display_resource_name': 'TACC HP/NVIDIA Interactive Visualization and Data Analytics System (Maverick)', 'allocations_description': None, 'allocable_id': 142303}, 'job_manager': 'SLURM', 'batch_system': 'SLURM', 'platform_name': 'HP/NVIDIA Interactive, Visualization and Data Analytics System', 'nfs_network': '', 'sensitive_data_support_description': None, 'advance_reservation_support': None, 'model': None, 'primary_storage_shared_gb': 'Lustre file system mounted from Stockyard', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': None, 'community_software_area_email': 'help@xsede.org', 'login_hostname': 'maverick.tacc.xsede.org', 'storage_network': 'Mellanox FDR InfiniBand', 'graphics_card': 'NVIDIA Tesla K40', 'is_visualization': True, 'memory_per_cpu_gb': 12.0, 'cpu_type': 'Intel Xeon E5-2680 V2', 'alternate_login_hostname': 'maverick.tacc.utexas.edu', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Maverick', 'supports_sensitive_data': None, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': 'Lustre file system mounted from Stockyard', 'user_guide_url': 'https://portal.xsede.org/tacc-maverick', 'disk_size_tb': 20480.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': 2.8, 'cpu_count_per_node': 20, 'operating_system': 'Linux (CentOS 6.4)', 'community_software_area': False}",56.0,XSEDE,XSEDE Level 1,142303,compute,Maverick is intended primarily for interactive visualization and data analysis jobs to allow for interactive query of large-scale data sets.  Normal batch queues will enable users to run simulations utilizing up to 32 nodes for up to 4 hours for interactive visualization jobs and up to 32 nodes for 12 hours for GPGPU and HPC jobs (at a lower priority).  Jobs requiring run times and more cores than allowed by the normal queues will be run in a special queue after approval of TACC staff.  Development queues are configured and available for use.,"The new Maverick HP/NVIDIA Interactive Visualization and Data Analytics System is configured with 132 HP ProLiant SL250s Gen8 compute nodes and 132 NVIDIA Tesla K40 GPU accelerators. In addition, with 256 GB of memory and 500 GB of storage per node, users have access to an aggregate of 33.7 TB of memory and 66 TB of local storage.. Compute nodes have access to a 20 PB Lustre Parallel file system, Stockyard.  An FDR InfiniBand switch fabric interconnects the nodes facilitating high-speed internode communication and I/O traffic.",HP/NVIDIA Interactive Visualization and Data Analytics System (Maverick),"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': '2014-03-03', 'decommissioned_end_date': None, 'pre_production_begin_date': '2013-12-15', 'pre_production_end_date': '2014-03-02'}",
242,,,hpc.tufts.xsede.org,tufts.xsede.org,,,,"{'latitude': None, 'node_count': 4.0, 'manufacturer': '', 'rmax': None, 'interconnect': '', 'is_publicly_available': False, 'peak_teraflops': None, 'advance_max_reservable_su': 0.0, 'is_accounted': False, 'sensitive_data_support_description': None, 'job_manager': '', 'batch_system': '', 'platform_name': '', 'nfs_network': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'SMP', 'gpu_description': '', 'community_software_area_email': '', 'login_hostname': '', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 384.0, 'cpu_type': '', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': 0.0, 'nickname': 'largemem000.tufts.xsede.org', 'supports_sensitive_data': None, 'is_authenticated': False, 'longitude': None, 'is_authorized': False, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': '', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': None, 'cpu_speed_ghz': None, 'cpu_count_per_node': None, 'operating_system': 'Redhat', 'community_software_area': False}",122.0,XSEDE,XSEDE Level 3,144580,compute,Large memory interactive and batch computes.,Large memory node for compute cluster.,Tufts University High Performance Compute (HPC) Large Memory Nodes,"{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': None, 'production_begin_date': None, 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2016-11-10 20:04:41.994
243,,production,rmacc-summit.colorado.xsede.org,colorado.xsede.org,production,2017-02-06,,"{'organizations': [{'organization_abbreviation': 'CU Boulder', 'organization_name': 'University of Colorado, Boulder', 'organization_code': '0087171'}], 'xsede_services_only': False}",,XSEDE,XSEDE Level 3,131,resource,,"RMACC-Summit is a heterogeneous HPC cluster that provides computing resources to participating members of the Rocky Mountain Advanced Computing Consortium (www.rmacc.org).  It consists of 485 nodes: 452 dual-socket general compute nodes, 11 GPU nodes, 5 high-memory nodes, and 20 Phi (""Knights Landing"") nodes.  It is supported by NSF grants ACI-1532235 and ACI-1532236.","RMACC Heterogeneous Compute Cluster (""Summit"")","{'coming_soon_begin_date': None, 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': '2016-11-21', 'post_production_end_date': None, 'coming_soon_end_date': None, 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': None, 'friendly_end_date': '2017-02-03', 'production_begin_date': '2017-02-06', 'decommissioned_end_date': None, 'pre_production_begin_date': None, 'pre_production_end_date': None}",2017-03-28 20:08:09.017
244,,production,comet.gpu.sdsc.xsede.org,gpu.sdsc.xsede.org,production,2015-04-01,2019-01-30,"{'latitude': None, 'node_count': 36.0, 'manufacturer': 'Dell', 'rmax': None, 'interconnect': '', 'is_publicly_available': True, 'peak_teraflops': 104.0, 'advance_max_reservable_su': None, 'is_accounted': True, 'allocations_info': {'unit_type': 'Service Units', 'allocable_type': 'ComputeResource', 'allocable_resource_id': 530201, 'conversion_factors': [], 'allocation_type_specifics': [{'relative_order': None, 'required_resources': [{'begin_date': '2014-09-14', 'required_resource_display_name': 'SDSC Medium-term disk storage (Data Oasis)', 'end_date': None, 'allocable_type': 'StorageResource', 'required_resource_allocable_resource_id': 530165, 'allocable_id': 142302}], 'default_amount': None, 'minimum_amount': None, 'allocation_type_active': True, 'site_provider_comments': '', 'allocation_state': 'New and Renewal Only', 'maximum_amount': None, 'allocation_type': 'Research', 'dollar_value': None}], 'display_resource_name': 'SDSC Comet GPU Nodes (Comet GPU)', 'allocations_description': '', 'allocable_id': 144586}, 'job_manager': 'SLURM', 'batch_system': '', 'platform_name': '', 'nfs_network': '', 'sensitive_data_support_description': '', 'advance_reservation_support': False, 'model': '', 'primary_storage_shared_gb': '', 'rpeak': None, 'machine_type': 'Cluster', 'gpu_description': '2 NVIDIA Tesla K80 GPU graphics cards per node, each with 2 GK210 GPUs (144 GPUs in total)', 'community_software_area_email': '', 'login_hostname': 'comet.sdsc.xsede.org', 'storage_network': '', 'graphics_card': '', 'is_visualization': False, 'memory_per_cpu_gb': 5.0, 'cpu_type': 'Intel Xeon E5-2680v3\xa0', 'alternate_login_hostname': '', 'xsedenet_participant': None, 'max_reservable_su': None, 'nickname': 'Comet GPU', 'supports_sensitive_data': False, 'is_authenticated': True, 'longitude': None, 'is_authorized': True, 'display_in_xsede_su_calculator': False, 'ip_address': '', 'parallel_file_system': None, 'user_guide_url': 'https://portal.xsede.org/sdsc-comet', 'disk_size_tb': 0.0, 'local_storage_per_node_gb': 286.0, 'cpu_speed_ghz': 2.5, 'cpu_count_per_node': 24, 'operating_system': 'Linux (CentOS)', 'community_software_area': False}",128.0,XSEDE,XSEDE Level 1,144586,compute,"GPUs are a specialized resource that performs well for certain classes of algorithms and applications. There is a large and growing base of community codes that have been optimized for GPUs including those in molecular dynamics, and machine learning. GPU-enabled applications on Comet include: Amber, Gromacs, BEAST, OpenMM, TensorFlow, and NAMD.","Comet has 36 general purpose GPU nodes, with 2 Tesla K80 GPU graphics cards per node, each with 2 GK210 GPUs (144 GPUs in total). Each GPU node also features 2 Intel Haswell processors of the same design and performance as the standard compute nodes (described separately). The GPU nodes are integrated into the Comet resource and available through the Slurm scheduler for either dedicated or shared node jobs (i.e., a user can run on 1 or more GPUs/node and will be charged accordingly). Like the Comet standard compute nodes, the GPU nodes feature a local SSD which can be specified as a scratch resource during job execution – in many cases using SSD’s can alleviate I/O bottlenecks associated with using the shared Lustre parallel file system.",SDSC Comet GPU Nodes (Comet GPU),"{'coming_soon_begin_date': '2014-09-12', 'retired_begin_date': None, 'decommissioned_begin_date': None, 'friendly_begin_date': None, 'post_production_end_date': None, 'coming_soon_end_date': '2014-09-13', 'post_production_begin_date': None, 'retired_end_date': None, 'production_end_date': '2019-01-30', 'friendly_end_date': None, 'production_begin_date': '2015-04-01', 'decommissioned_end_date': None, 'pre_production_begin_date': '2014-09-14', 'pre_production_end_date': '2015-03-31'}",2017-03-15 20:36:57.488
